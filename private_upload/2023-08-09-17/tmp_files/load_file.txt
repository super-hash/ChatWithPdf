filepath=private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf,len=66
page_content='Controllable Synthesis of Hierarchical Porous Fe 3O4Particles\nMediated by Poly(diallyldimethylammonium chloride) and Their\nApplication in Arsenic Removal\nTing Wang,†Liyuan Zhang,†Haiying Wang,†,‡Weichun Yang,†,‡Yingchun Fu,§Wenli Zhou,§\nWanting Yu,†Kaisong Xiang,†Zhen Su,†Shuo Dai,†and Liyuan Chai *,†,‡\n†Department of Environmental Engineering, School of Metallurgy and Environment, Central South University, Changsha 410017, P.\nR. China\n‡Chinese National Engineering Research Center for Control & Treatment of Heavy Metal Pollution, Changsha 410017, P. R. China\n§Key Laboratory of Chemical Biology and Traditional Chinese Medicine Research (Ministry of Education), College of Chemistry and\nChemical Engineering, Hunan Normal University, Changsha 410081, P. R. China\n*SSupporting Information\nABSTRACT: Hierarchical porous Fe 3O4particles with' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 0}
page_content='§Key Laboratory of Chemical Biology and Traditional Chinese Medicine Research (Ministry of Education), College of Chemistry and\nChemical Engineering, Hunan Normal University, Changsha 410081, P. R. China\n*SSupporting Information\nABSTRACT: Hierarchical porous Fe 3O4particles with\ntunable grain size were synthesized based on a facile poly(diallyldimethylammonium chloride) (PDDA)-modulated sol-vothermal method. The products were characterized withscanning electron microscopy (SEM) and transmissionelectron microscopy (TEM), X-ray photoelectron spectrosco-py (XPS), Fourier transform infrared spectroscopy (FT-IR),X-ray di ﬀraction (XRD), N\n2adsorption −desorption technique,\nvibrating sample magnetometer (VSM), and dynamic lightscattering (DLS). The results show that increasing the PDDAdosage decrease the grain size and particle size, which increased the particle porosity and enhanced the surface area from 7.05 to32.75 m' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 0}
page_content='2adsorption −desorption technique,\nvibrating sample magnetometer (VSM), and dynamic lightscattering (DLS). The results show that increasing the PDDAdosage decrease the grain size and particle size, which increased the particle porosity and enhanced the surface area from 7.05 to32.75 m\n2g−1. Possible mechanism can be ascribed to the PDDA function on capping the crystal surface and promoting the\nviscosity of reaction medium to mediate the growth and assembly of grain. Furthermore, the arsenic adsorption application of theas-obtained Fe\n3O4samples was investigated and the adsorption mechanism was proposed. High magnetic Fe 3O4particles with\nincreased surface area display improved arsenic adsorption performance, superior e ﬃciency in low-level arsenic removal, high\ndesorption e ﬃciency, and satisfactory magnetic recyclability, which are very promising compared with commercial Fe 3O4\nparticles.\nKEYWORDS: Fe3O4, poly (diallyldimethylammonium chloride), solvothermal, adsorption' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 0}
page_content='desorption e ﬃciency, and satisfactory magnetic recyclability, which are very promising compared with commercial Fe 3O4\nparticles.\nKEYWORDS: Fe3O4, poly (diallyldimethylammonium chloride), solvothermal, adsorption\n1. INTRODUCTION\nArsenic, one of the top 20 hazardous substances, greatly\nthreatens the health of human body, ecological balance, and\nindustrial development.1Thus, the remediation of arsenic\npollution has attracted worldwide attention.2−4So far,\ntechnologies involving oxidation,5coagulation,6adsorption,7−9\nion-exchange,10and reverse osmosis11have been developed to\ndetoxicate arsenic pollution. Among them, adsorption is one of\nthe most promising technologies for arsenic removal, because\nof its easy operation and low-cost production.4,12,13However,\nthe separation of traditional adsorbent (e.g., ﬁltration,\ncentrifugation or gravitational sedimentation) is time-consum-\ning and cost-ine ﬀective and therefore limits the practical' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 0}
page_content='of its easy operation and low-cost production.4,12,13However,\nthe separation of traditional adsorbent (e.g., ﬁltration,\ncentrifugation or gravitational sedimentation) is time-consum-\ning and cost-ine ﬀective and therefore limits the practical\napplication.14Hence, magnetic adsorbents such as Fe 3O4\nexhibit unique advantages due to their quick and e ﬀective\nmagnetic separation.15−17\nIn parallel, the rapid growth of nanotechnology has attracted\na great deal of interest in environmental application.18−25In\nterms of the application of Fe3O4as an adsorbent, decreasingthe Fe3O4particle size from micrometers to nanometers would\nincrease the available adsorptive areas by 100 to 1000\ntimes.26−28However, as the Fe3O4particle size decreases to\nnanometers, its response to an external magnetic ﬁeld\nundesirably decreases, which will not be large enough to\novercome Brownian motion and no e ﬃcient magnetic\nseparation will occur.26,29,30To tackle this problem, one' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 0}
page_content='times.26−28However, as the Fe3O4particle size decreases to\nnanometers, its response to an external magnetic ﬁeld\nundesirably decreases, which will not be large enough to\novercome Brownian motion and no e ﬃcient magnetic\nseparation will occur.26,29,30To tackle this problem, one\npractical strategy is to prepare magnetic hierarchical structures,\nwhich are constructed with building blocks of nanounits. Thehierarchical nanostructures not only exhibit high speci ﬁc surface\narea because of the abundant interparticle spaces or intra-\nparticle pores, but also possess satisfactory magnetic response\nbecause of their larger size and weaker Brownian motion, which\ntherefore show great superiority to individual nanometer- and\nmicrometer-sized materials.\n31−36To date, two conventional\nReceived: August 22, 2013\nAccepted: November 19, 2013Research Article\nwww.acsami.org\n© XXXX American Chemical Society A dx.doi.org/10.1021/am403533v |ACS Appl. Mater. Interfaces XXXX, XXX, XXX −XXX' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 0}
page_content='template methods were reported to synthesize hierarchical\nnanoarchitectures, including hard templates such as silica,37\npolymer spheres,38and metal oxides,39as well as soft templates\nsuch as emulsion droplets/micelles40,41and even gas bubbles.42\nThese synthetic routes seem to be inconvenient because\ncomplicated template presynthesis or time-consuming pre-\ncursor calcination at elevated temperature is needed.37,43−46\nMoreover, the removal of templates by erosion or calcination\nbrings adverse e ﬀect on the product morphology.47,48\nConsequently, it is preferable to develop one-step template-\nfree methods for the preparation of hierarchical particles with\nwell-de ﬁned morphology.\nIt is generally believed that the grain acts as the building\nblock and its oriental assembly constitutes hierarchical particles.Consequently, it is predictable that the grain property and its\nassembly behavior synchronously in ﬂuence product morphol-\nogy.49−51To date, the reported advanced template-free' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 1}
page_content='block and its oriental assembly constitutes hierarchical particles.Consequently, it is predictable that the grain property and its\nassembly behavior synchronously in ﬂuence product morphol-\nogy.49−51To date, the reported advanced template-free\nmethods have mainly focused on the modulation of grain\nassembly by Kirkendall e ﬀect,52,53Ostwald ripening ef-\nfect45,54−57or self-attachment e ﬀect.46,58,59For instance, Yong\net al. reported that the assembly of grain evolved into porous\nFe3O4hollow submicrospheres based on Ostwald ripening\nprocess through one-pot solvothermal method.60Though the\npattern and mechanism of grain assembly were comprehen-sively investigated, rare researches were devoted to studying the\neﬀect of grain property on the performance of yielded Fe\n3O4\nhierarchical particles.\nHerein, our research aimed to controllably prepare Fe3O4\nhierarchical particles via modulating the grain property though' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 1}
page_content='eﬀect of grain property on the performance of yielded Fe\n3O4\nhierarchical particles.\nHerein, our research aimed to controllably prepare Fe3O4\nhierarchical particles via modulating the grain property though\none-pot solvothermal method. To the best of our knowledge,reports have seldom demonstrated the availability of this\nstrategy on controlling the morphology and application\nperformance of hierarchical particles. Poly-(diallyldimethylammonium chloride) (PDDA), as an environ-mentally friendly and low cost polyelectrolyte, has been widely\nused in the preparation of composites via electrostatic or π−π\nstack interaction for biosensor and catalysis.\n61−65Although the\npotential modulation usage for the synthesis of magnetic\nparticles has not been investigated. In this research, a facile\nPDDA-mediated solvothermal method was proposed tocontrollably synthesize Fe\n3O4hierarchical particles. Increasing\nthe PDDA dosage declines the grain and particle size, which' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 1}
page_content='potential modulation usage for the synthesis of magnetic\nparticles has not been investigated. In this research, a facile\nPDDA-mediated solvothermal method was proposed tocontrollably synthesize Fe\n3O4hierarchical particles. Increasing\nthe PDDA dosage declines the grain and particle size, which\nleads to the increment of speci ﬁc area and porosity, eventually\nenhancing the adsorption performance. The mechanism for\nPDDA-induced grain size tunable strategy was also discussed.\nThe prepared Fe 3O4particles show higher adsorption capacity\nthan commercial Fe3O4particles and pose great potential in the\nlow-level arsenic removal, such as the remediation of\ngroundwater (with arsenic concentration of hundreds ofmicrograms per liter66−68).\n2. EXPERIMENTAL SECTION\n2.1. Materials. Ferric chloride hexahydrate (FeCl3·6H2O),\nanhydrous sodium acetate (CH3COONa, NaAc), and ethylene glycol' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 1}
page_content='low-level arsenic removal, such as the remediation of\ngroundwater (with arsenic concentration of hundreds ofmicrograms per liter66−68).\n2. EXPERIMENTAL SECTION\n2.1. Materials. Ferric chloride hexahydrate (FeCl3·6H2O),\nanhydrous sodium acetate (CH3COONa, NaAc), and ethylene glycol\n(EG) were obtained from the Sinopharm Group Chemical ReagentCo., Ltd.. A 35.0 wt % aqueous solution of high molar mass ( M\nw100\n000−200 000) PDDA was obtained from Sigma-Aldrich. Na3AsO4·\n12H2O and NaAsO2were used as the sources of As(V) and As(III),\nrespectively. Commercial Fe3O4with the diameter of 200 nm was\npurchased from Beijing Dk Nano technology Co., Ltd.. All reagentswere used without further treatment. Ultrapure water with a resistivityof 18.2 M Ωcm\n−1, produced with a Milli-Q apparatus (Millipore), was\nused throughout all of the experiments.2.2. Preparation of Porous Fe3O4.Porous Fe3O4particles were' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 1}
page_content='−1, produced with a Milli-Q apparatus (Millipore), was\nused throughout all of the experiments.2.2. Preparation of Porous Fe3O4.Porous Fe3O4particles were\nsynthesized exploiting a facile solvothermal method via morphology-mediated by PDDA solution. In a typical procedure, 1.35 g of FeCl\n3·\n6H2O was dissolved in a mixture containing 36 mL of EG and an\nappropriate amount of PDDA solution, then 3.6 g of NaAc was added.After vigorous stirring for 30 min, a transparent solution was obtained\nand transferred to a 50 mL Te ﬂon-lined autoclave, which was then\nplaced in an oven at 200 °C for 6 h, followed by naturally cooling to\nroom temperature. The black precipitate was collected and ultrasonicwashed by water and ethanol for three times, respectively, throughmagnetic separation. The yielded product was vacuum-dried at 60 °C\nfor 12 h. By modulating the dosage of PDDA solution as 1, 2, 3, 4, 5,and 6 g, the resultant series products were separately named as Fe\n3O4-i' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 1}
page_content='for 12 h. By modulating the dosage of PDDA solution as 1, 2, 3, 4, 5,and 6 g, the resultant series products were separately named as Fe\n3O4-i\n(i=1−6). To understand the formation mechanism of Fe3O4,F e3O4-2\nand Fe3O4-4 samples were collected at various reaction time (1.5, 2.5,\n4, 5, 6, and 8 h), followed by washing and drying procedures. Theobtained series products were denoted as Fe\n3O4-2-xho rF e3O4-4-xh(x\nrefers to the reaction time).\n2.3. Characterization. Scanning electron microscopy (SEM, JSM-\n6360) and transmission electron microscopy (TEM, TECNAI G2)\nwere used to characterize the morphology of the nanoparticles. The X-ray di ﬀraction (XRD) patterns of the Fe\n3O4were obtained using\nRigaku D/Max-RB di ﬀractometer with Cu −Kαradiation ( λ= 0.15406\nnm, 35 kV, 40 mA). X-ray photoelectron spectroscopy (XPS)measurements were carried out on a Thermo Fisher Scienti ﬁcK -\nAlpha 1063 using Al K αX-ray as the excitation source. Fourier 40' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 1}
page_content='3O4were obtained using\nRigaku D/Max-RB di ﬀractometer with Cu −Kαradiation ( λ= 0.15406\nnm, 35 kV, 40 mA). X-ray photoelectron spectroscopy (XPS)measurements were carried out on a Thermo Fisher Scienti ﬁcK -\nAlpha 1063 using Al K αX-ray as the excitation source. Fourier 40\ntransformed infrared spectroscopy (FT-IR, Nicolet IS10) wasemployed to analyze the molecular structure of the yielded productat a resolution of 4 cm\n−1. The size of the pinhole and the integration\ntime were set as 100 μm and 30 s, respectively. Magnetic properties of\nthe product were investigated using a vibrating sample magnetometer(VSM, EV7, ADE) with an applied ﬁeld between −7000 and 7000 Oe\nat room temperature. Speci ﬁc surface areas of the yielded products\nwere measured by adsorption −desorption of ultrapure N\n2on a\nQuantachrome Instruments system via Brunauer −Emmett −Teller\n(BET) method. Pore size distribution was determined by N2\ndesorption isotherm using Barret −Joymer −Halender (BJH) method.' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 1}
page_content='were measured by adsorption −desorption of ultrapure N\n2on a\nQuantachrome Instruments system via Brunauer −Emmett −Teller\n(BET) method. Pore size distribution was determined by N2\ndesorption isotherm using Barret −Joymer −Halender (BJH) method.\nThe size of the Fe 3O4particles was determined by dynamic light\nscattering (DLS) on a Malvern zetasizer instrument (type Nano-ZS,\nMalvern Instruments Ltd., Britain) using Fe3O4suspension with the\nconcentration of 0.01 g L−1.\n2.4. Batch Adsorption Experiment. Solutions containing\ndiﬀerent concentrations of As(V) or As(III) were prepared and\nadjusted to pH 5 ±0.2 using HCl. Then, 5 mg of the adsorbent\nsample was added to 10 mL arsenic aqueous solution under stirring.\nAfter a speci ﬁed time, the solid and liquid were magnetic separated' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 1}
page_content='diﬀerent concentrations of As(V) or As(III) were prepared and\nadjusted to pH 5 ±0.2 using HCl. Then, 5 mg of the adsorbent\nsample was added to 10 mL arsenic aqueous solution under stirring.\nAfter a speci ﬁed time, the solid and liquid were magnetic separated\nand the initial and residual concentrations of arsenic were measured byinductively coupled plasma-optical emission spectroscopy (ICP-OES)(Optima 5300DV). The adsorption isotherm was obtained by varyingthe initial arsenic concentrations and stirring for 4 h at 25 °C\n(concentration range: 0.1 −17 mg L\n−1for As(III) and 0.1 −7.5 mg L−1\nfor As(V), respectively). For comparison, commercial Fe3O4with the\ndiameter of 200 nm synthesized by coprecipitation was also exploited.The equilibrium adsorption capacity ( q\ne) (mg g−1) for arsenic was\ncalculated according to the following equation\n=−qcc V\nm()\ne0e\n(1)\nwhere c0and ce(mg L−1) are the initial and equilibrium arsenic\naqueous concentrations, respectively; Vis the volume (mL) of arsenic' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 1}
page_content='e) (mg g−1) for arsenic was\ncalculated according to the following equation\n=−qcc V\nm()\ne0e\n(1)\nwhere c0and ce(mg L−1) are the initial and equilibrium arsenic\naqueous concentrations, respectively; Vis the volume (mL) of arsenic\naqueous solution; mis the mass (mg) of adsorbents used in the\nexperiment.\nTo test the low-level arsenic removal feasibility of adsorbents, initial\narsenic solution with As(V) concentration in the range of 50 −1400 μg\nL−1and As(III) in the range of 50 −600μgL−1were prepared. Other\nadsorption experiment procedures were the same as above.\nThe adsorption kinetics was investigated with the initial As(V)\nconcentration of 3.5 mg L−1and As(III) concentration of 3 mg L−1at\npH = 5.0 ±0.2 and adsorbents dose = 0.5 g L−1. The solution wasACS Applied Materials & Interfaces Research Article\ndx.doi.org/10.1021/am403533v |ACS Appl. Mater. Interfaces XXXX, XXX, XXX −XXX B' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 1}
page_content='allowed to react with the adsorbent for a ﬁxed period (between 10 and\n240 min).\nThe regeneration of the absorbent was conducted by using 0.1 M\nNaOH solution as eluent with adsorbents dose = 1 g L−1at 25 °C.\nBrieﬂy, the absorbent was ﬁrst ultrasoni ﬁcated in NaOH solution for\n30 min and then shaken for 2 h, followed by magnetic separation and\nwashing by water three times. Then the adsorbent was applied into\nrecycle adsorption study. The recycle adsorption experimental\nprocedure and detection method are in accordance with the ﬁrst\nadsorption experiment, including the mixing of the adsorbent witharsenic solution under stirring for 4 h, the solid and liquid separation\nby external magnetism, and the determination of the residual arsenic\nsolution by ICP-OES (Optima 5300DV).\nAll the experimental data we re the average of triplicate\ndeterminations with relative errors under 5%.\n3. RESULTS AND DISCUSSION\n3.1. Morphology, Structure, and Property of Fe 3O4.' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 2}
page_content='by external magnetism, and the determination of the residual arsenic\nsolution by ICP-OES (Optima 5300DV).\nAll the experimental data we re the average of triplicate\ndeterminations with relative errors under 5%.\n3. RESULTS AND DISCUSSION\n3.1. Morphology, Structure, and Property of Fe 3O4.\nThe SEM images and size distribution of the Fe 3O4-i(i=1−6)\nparticles are presented in Figure 1. As seen, Figure 1A −F shows\nthat the size of monodispersed hierarchical particles monoto-\nnously decreases from (A) 420 nm to (F) 100 nm, as increasingthe PDDA dosage from 1 to 6 g. Correspondingly, themorphology of hierarchical particles gradually becomes coarseand porous, with the increase of PDDA dosage. As can beconﬁrmed by TEM images in Figure S1 in the SupportingInformation, increasing the PDDA dosage concurrently\ndecreases hierarchical particle size and increases porosity.HRTEM images were also conducted to give further insightinto the grain assembly, as shown in Figure 2. Taking Fe\n3O4-4' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 2}
page_content='decreases hierarchical particle size and increases porosity.HRTEM images were also conducted to give further insightinto the grain assembly, as shown in Figure 2. Taking Fe\n3O4-4\nas an example, the particle shows pineal-like morphology withfringe spacing of 0.48 nm, corresponding to the (111) latticeplanes of Fe\n3O4. The result indicates the possible oriented\nassembly of grain along (111) plane, which is the crystallo-\ngraphic plane with the highest energy and preferential fororiented attachment.51The structures and grain size of Fe3O4\nwere further measured by XRD, as shown in Figure 3. All thediﬀraction peaks at 18.32 ±0.03, 30.10 ±0.05, 35.48 ±0.03,\n43.10 ±0.02, 53.40 ±0.04, 57.02 ±0.05, and 62.58 ±0.08 °\ncan be indexed to the indices (111), (220), (311), (400),(422), (511), and (440) of Fe\n3O4. According to the Scherrer\nformula, the grain size gradually decreased from 34.4 nm(Fe\n3O4-2) to 13.4 nm (Fe 3O4-6) with the increase of PDDA' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 2}
page_content='43.10 ±0.02, 53.40 ±0.04, 57.02 ±0.05, and 62.58 ±0.08 °\ncan be indexed to the indices (111), (220), (311), (400),(422), (511), and (440) of Fe\n3O4. According to the Scherrer\nformula, the grain size gradually decreased from 34.4 nm(Fe\n3O4-2) to 13.4 nm (Fe 3O4-6) with the increase of PDDA\ndosage (as listed in Table 1), which indicates the feasibility ofthe PDDA-induced grain size tunable strategy. Brie ﬂy speaking,\nthe SEM, TEM, and XRD results show that PDDA modulated\nsolvothermal method successfully modulate products morphol-ogy, particle size, grain size, and facilitate the oriented grainassembly.\nOn the other hand, the surface area and pore size distribution\nof as-synthesized Fe\n3O4-i(i= 2, 4, 5, 6) were determined by\nnitrogen adsorption −desorption measurements, as shown in\nFigure 4. Fe3O4-6 (Figure 4D) synthesized with the highest\nPDDA dosage possesses surface area and pore volume of 32.75m\n2g−1and 0.12 cm3g−1, respectively, both of which are higher' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 2}
page_content='of as-synthesized Fe\n3O4-i(i= 2, 4, 5, 6) were determined by\nnitrogen adsorption −desorption measurements, as shown in\nFigure 4. Fe3O4-6 (Figure 4D) synthesized with the highest\nPDDA dosage possesses surface area and pore volume of 32.75m\n2g−1and 0.12 cm3g−1, respectively, both of which are higher\nthan that of the Fe 3O4-5 (31.16 cm2g−1and 0.117 cm3g−1,\nFigure 4C), Fe 3O4-4 (19.13 m2g−1and 0.07 cm3g−1, Figure\n4B) and Fe 3O4-2 (7.05 m2g−1and 0.015 cm3g−1, Figure 4A).\nAll the samples pose pore size in the range of 7 −12 nm. The\nresults above can be ascribed to the fact that smaller grainassembly possesses more channels, leading to the increasedsurface area and pore amount. Hence, increasing the PDDAdosage yields Fe\n3O4hierarchical particles composing of smaller\ngrain, which exhibit higher surface area and porosity.\nThe magnetic property of Fe 3O4hierarchical particles was' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 2}
page_content='3O4hierarchical particles composing of smaller\ngrain, which exhibit higher surface area and porosity.\nThe magnetic property of Fe 3O4hierarchical particles was\nevaluated by examining the magnetic hysteresis loops at roomtemperature, as shown in Figure S2 in the SupportingInformation. The M\nsfor the Fe 3O4-i(i=2−6) is in the\nrange of 50 −80 emu g−1, which is comparable with many\nreported high magnetic particles.31\nBrieﬂy speaking, hierarchical porous Fe 3O4particles with\nhigh magnetism were synthesized by facile PDDA-modulatedsolvothermal method, which is achieved in one-pot solutionreaction and avoids the time/energy consuming precursorcalcination process. Furthermore, PDDA-induced grain sizetunable strategy has been proved to be an e ﬃcient way to\nenhance the surface area and porosity of particles.\n3.2. Mechanism for the Formation of Fe\n3O4Hier-\narchical Particles Mediated by PDDA. The morphology' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 2}
page_content='enhance the surface area and porosity of particles.\n3.2. Mechanism for the Formation of Fe\n3O4Hier-\narchical Particles Mediated by PDDA. The morphology\nand structure of the products with initial PDDA dosage of 4 gat various reaction time were examined by TEM, FT-IR andXRD to preliminarily understand the morphology and structureevolution of Fe\n3O4hierarchical particles, as shown in Figure 5.\nTEM results give insight into the morphology evolution of\nmesoporous Fe 3O4. As shown in Figure 5A −F), three typical\nstages were observed for the formation of Fe 3O4, namely, the\nformation of spindle precursor with length of 5 −10 nm (0 −1.5\nh), the formation and assembly of grain to sphere particles(1.5−4 h), and the oriented assembly/Ostwald ripening\nFigure 1. SEM images and the corresponding hierarchical particle size\ndistribution of (A) Fe 3O4-1, (B) Fe 3O4-2, (C) Fe 3O4-3, (D) Fe 3O4-4,\n(E) Fe3O4-5, and (F) Fe3O4-6 at initial PDDA dosage varying from 1' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 2}
page_content='Figure 1. SEM images and the corresponding hierarchical particle size\ndistribution of (A) Fe 3O4-1, (B) Fe 3O4-2, (C) Fe 3O4-3, (D) Fe 3O4-4,\n(E) Fe3O4-5, and (F) Fe3O4-6 at initial PDDA dosage varying from 1\nto 6 g.ACS Applied Materials & Interfaces Research Article\ndx.doi.org/10.1021/am403533v |ACS Appl. Mater. Interfaces XXXX, XXX, XXX −XXX C' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 2}
page_content='process of preformed sphere into porous particles (4 −8 h). The\nXRD patterns (Figure 5G) of Fe 3O4-4 at 1.5h depict a strong\npeak at 7.55 °along with a broad weak one at 25 °probably\noriginated from (001) and (013) planes of an iron oxide acetatehydroxide hydrate with a formula of Fe\n2O(CH 3COO)(OH) 3·\nH2O according to JCPDS.69Then XRD patterns of the samples\nobtained at the time from 2.5 to 4 h show gradually enhancedpeaks at 30.00, 35.48, 43.14, 53.44, 57.04, and 62.58 °, marked\nby the indices (220), (311), (400), (422), (511), and (440) ofFe\n3O4phases. When the reaction time was 6 −8 h, the produced\naggregates were pure Fe3O4. The FT-IR spectra (Figure 5H) of\nFe3O4-1.5h and Fe3O4-2.5h show reduced absorption peaks at\n1578 and 1445 cm−1due to the asymmetric and symmetric\nstretching of COO−group, band at 1090 cm−1owing to C −O\nstretching of the COO−group, band at 887 cm−1due to the\nOH bending.69The spectra of the samples within 4 −8 h show' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 3}
page_content='Fe3O4-1.5h and Fe3O4-2.5h show reduced absorption peaks at\n1578 and 1445 cm−1due to the asymmetric and symmetric\nstretching of COO−group, band at 1090 cm−1owing to C −O\nstretching of the COO−group, band at 887 cm−1due to the\nOH bending.69The spectra of the samples within 4 −8 h show\nbroad strong band at 591 cm−1due to the Fe −O lattice mode\nof Fe3O4.34Except for the anticipated typical peak for ironcomposite (Fe 2O(CH 3COO)(OH) 3·H2Oo rF e 3O4), the peak\nat 1125 cm−1was ascribed to the C −N symmetric stretching\nvibration of PDDA. The PDDA also exhibits weak CH 2\nbending vibrations (around 1474, 1326, and 960 cm−1), C−\nH asymmetric, and C −H symmetric stretching frequencies\n(2918 and 2867 cm−1).70,71Thus, both the XRD and FT-IR\nindicate the gradual formation of Fe 3O4phase at the expense of\npreformed Fe 2O(CH 3COO)(OH) 3·H2O phase.\nBrieﬂy speaking, typical three stages were observed for the\nformation of Fe 3O4, namely, the formation of spindle precursor' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 3}
page_content='(2918 and 2867 cm−1).70,71Thus, both the XRD and FT-IR\nindicate the gradual formation of Fe 3O4phase at the expense of\npreformed Fe 2O(CH 3COO)(OH) 3·H2O phase.\nBrieﬂy speaking, typical three stages were observed for the\nformation of Fe 3O4, namely, the formation of spindle precursor\nwith length of 5 −10 nm (mainly composed of Fe 2O-\n(CH 3COO)(OH) 3·H2Oa t0 −1.5 h), the formation and\nassembly of grain to sphere Fe3O4particles (1.5 −4 h), and\nthe oriented assembly/Ostwald ripening process of preformedsphere into porous Fe\n3O4particles (4 −8 h), which are in\nagreement with the reported literatures.48,51,58,69,72FT-IR\nanalysis indicates that the hierarchical particles exhibit the\nvibration of PDDA which suggests the existence of PDDA on\nthe particle surface and therefore the potential capping functionof PDDA.\nFurthermore, to reveal the e ﬀect of PDDA dosage on the\nmorphology evolution, the amount was decreased from 4 to 2 gwith otherwise the same conditions above, as shown in Figure' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 3}
page_content='the particle surface and therefore the potential capping functionof PDDA.\nFurthermore, to reveal the e ﬀect of PDDA dosage on the\nmorphology evolution, the amount was decreased from 4 to 2 gwith otherwise the same conditions above, as shown in Figure\nS3 in the Supporting Information. The grains collected at 2.5, 4,\n5, 6, and 8 h for the synthesis adopting 4 g of PDDA increase\nfrom 20.1 to 23.6 nm, indicating grain size increase by 3.5 nm;\nwhile the increment of grain size for the synthesis with 2 g ofPDDA is 9.3 nm (increase from 21.5 to 30.8 nm). The results\nreﬂect that the increase in grain size was depressed as increasing\nthe PDDA dosage, which might be partially ascribed to the\ncapping e ﬀect of PDDA. On the other hand, it is discovered\nthat the viscosity of the reaction medium was enhanced withthe increase in PDDA dosage, as shown in Table 1. It is well-\naccepted that the increment of viscosity results in the promoted\nmass transfer resistance, which is not conductive to crystal' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 3}
page_content='that the viscosity of the reaction medium was enhanced withthe increase in PDDA dosage, as shown in Table 1. It is well-\naccepted that the increment of viscosity results in the promoted\nmass transfer resistance, which is not conductive to crystal\ngrowth, thus leading to the decreased grain size.\n54,73\nFigure 2. HRTEM images (A −C) of Fe3O4-4; B and C represent the magni ﬁcation of the red area in A.\nFigure 3. XRD patterns of Fe3O4particles obtained at di ﬀerent PDDA\ndosage: (a) 2 g, (b) 3 g, (c) 4 g, (d) 5 g, (e) 6 g, with other\nexperimental parameters keeping constant.\nTable 1. Viscosity of Reaction Medium, Particle Size, Grain Size, Magnetic Properties, And Absorption Performance of Fe 3O4-i\n(i=2−6) with di ﬀerent initial PDDA dosage\nsamplesPDDA dosage\n(g)viscosity of reaction medium\n(Pa S)particle size\n(nm)grain size\n(nm)magnetic property (emu\ng−1)qm(As(V))\n(mg g−1)qe(As(III))\n(mg g−1)\nFe3O4-2 2 0.062 ±0.002 350 ±15 34.4 ±0.8 74.42 ±1.93 1.93 1.57' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 3}
page_content='(i=2−6) with di ﬀerent initial PDDA dosage\nsamplesPDDA dosage\n(g)viscosity of reaction medium\n(Pa S)particle size\n(nm)grain size\n(nm)magnetic property (emu\ng−1)qm(As(V))\n(mg g−1)qe(As(III))\n(mg g−1)\nFe3O4-2 2 0.062 ±0.002 350 ±15 34.4 ±0.8 74.42 ±1.93 1.93 1.57\nFe3O4-3 3 0.082 ±0.004 215 ±12 28.1 ±0.6 72.11 ±4.15 2.31 1.99\nFe3O4-4 4 0.106 ±0.005 195 ±10 20.2 ±0.5 57.96 ±3.47 4.07 3.29\nFe3O4-5 5 0.127 ±0.004 185 ±10 14.4 ±0.3 53.96 ±2.86 6.35 6.06\nFe3O4-6 6 0.145 ±0.006 95 ±10 13.3 ±0.5 49.21 ±3.13 7.23 6.77ACS Applied Materials & Interfaces Research Article\ndx.doi.org/10.1021/am403533v |ACS Appl. Mater. Interfaces XXXX, XXX, XXX −XXX D' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 3}
page_content='Thus, based on the discussion above, a possible mechanism\nwas proposed to elucidate the PDDA-induced grain size tunablestrategy for the controllable synthesis of porous Fe\n3O4\nhierarchical particles. As shown in Scheme 1, a mixturecomposed of FeCl\n3, EG, NaAc, and PDDA was ﬁrst obtained\nand the viscosity of mixture was greatly enhanced by PDDA.Spindle particles were then obtained with PDDA as capping\nagents, which improved the particle dispersibility. As time goes\non, hierarchical Fe\n3O4particles were eventually produced, and\nmeanwhile PDDA function on capping e ﬀect and increasing\nviscosity declines particle and grain size, facilitates oriented\nassembly, thus synchronously enhancing surface area andporosity. Brie ﬂy speaking, a PDDA-modulated solvothermal\nmethod can controllably prepare porous Fe\n3O4hierarchical\nparticles.\n3.3. Arsenic Adsorption Performance of Fe3O4.As\ndiscussed above, as-obtained Fe 3O4samples exhibit high' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 4}
page_content='assembly, thus synchronously enhancing surface area andporosity. Brie ﬂy speaking, a PDDA-modulated solvothermal\nmethod can controllably prepare porous Fe\n3O4hierarchical\nparticles.\n3.3. Arsenic Adsorption Performance of Fe3O4.As\ndiscussed above, as-obtained Fe 3O4samples exhibit high\nspeciﬁc area, high porosity, and excellent magnetic property,\nall of which are generally regarded as desirable properties of\nadsorbent for the pollutants removal. Before arsenic adsorption,the dispersibility of aqueous Fe\n3O4samples was evaluated by\ndetermining the particle size via DLS. As shown in Figure S4 in\nthe Supporting Information, the size of Fe3O4shows a\nmonotonous decrease from 520 nm (A) to 180 nm (F) withPDDA dosage from 1 to 6 g. Considering the possible plus bias\nfor DLS compared with TEM/SEM,\n74,75the results indicate\ngood dispersibility for Fe 3O4suspension. Then, the adsorption\ncapacity of Fe 3O4-i(i=2−6) for As(V) and As(III) was' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 4}
page_content='monotonous decrease from 520 nm (A) to 180 nm (F) withPDDA dosage from 1 to 6 g. Considering the possible plus bias\nfor DLS compared with TEM/SEM,\n74,75the results indicate\ngood dispersibility for Fe 3O4suspension. Then, the adsorption\ncapacity of Fe 3O4-i(i=2−6) for As(V) and As(III) was\nevaluated using the equilibrium adsorption isotherm by varyingthe initial As(V) and As(III) concentrations. As shown in\nFigure 6A and C, the adsorption capacity of As(V) and As(III)\nmonotonously increased from 1.93 and 1.57 mg g−1for Fe 3O4-\n2 to 7.23 and 6.77 mg g−1for Fe 3O4-6, indicating that the\nmorphology mediated by PDDA greatly facilitates theabsorption performance of particles. Fe\n3O4-5 and Fe3O4-6 in\nour research exhibit higher adsorption capacity than commer-\ncial Fe 3O4particles (1.35 mg g−1for As(V) and 0.76 mg g−1for\nAs(III)) and some other reported metal oxide spheres, as listed\nin Table 2.28,31,76Moreover, in terms of the low contaminated' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 4}
page_content='3O4-5 and Fe3O4-6 in\nour research exhibit higher adsorption capacity than commer-\ncial Fe 3O4particles (1.35 mg g−1for As(V) and 0.76 mg g−1for\nAs(III)) and some other reported metal oxide spheres, as listed\nin Table 2.28,31,76Moreover, in terms of the low contaminated\ndrinking water (with hundreds of microgram per liter),66the\nprepared Fe 3O4exhibits excellent removal e ﬃciency. Exploiting\nFe3O4-5 as adsorbent, arsenic solution with initial As(V)\nconcentration ≤800 μgL−1or initial As(III) concentration\n≤300μgL−1can be detoxicated to drinking water standard of\nUSA (with arsenic concentration less than 10 μgL−1) (see\nFigure S5 in the Supporting Information), indicating the\npotential application in the low-level arsenic removal.\nTwo equations, Langmuir and Freundlich isotherms models,\nw e r eu s e dt oa n a l y z et h ee x p e r i m e n t a ld a t aa n dt h e\nmathematical expressions are depicted in eq 2 and 3,\nrespectively\n=+C\nqq KC\nq1 e\nem Le\nm (2)\n=+qncK log1log loge eF(3)' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 4}
page_content='Two equations, Langmuir and Freundlich isotherms models,\nw e r eu s e dt oa n a l y z et h ee x p e r i m e n t a ld a t aa n dt h e\nmathematical expressions are depicted in eq 2 and 3,\nrespectively\n=+C\nqq KC\nq1 e\nem Le\nm (2)\n=+qncK log1log loge eF(3)\nFigure 4. Nitrogen adsorption −desorption isotherms and pore-size distribution curves (the corresponding insert) of (A) as-obtained Fe 3O4-2, (B)\nFe3O4-4, (C) Fe3O4-5, and (D) Fe3O4-6, respectively.ACS Applied Materials & Interfaces Research Article\ndx.doi.org/10.1021/am403533v |ACS Appl. Mater. Interfaces XXXX, XXX, XXX −XXX E' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 4}
page_content='where qmand KLare Langmuir constants and represent the\nmaximum adsorption capacity of adsorbents (mg g−1) and the\nenergy of adsorption, respectively. KFand nare Freundlich\nconstants related to adsorption capacity and adsorptionintensity, respectively.\nFor the Langmuir isotherm model, the values of q\nmandKL\ncan be calculated from the slope and intercept of plots of ce/qe\nversus ce. For the Freundlich isotherm model, the values of n\nandKFcan be obtained by a plot of log qeagainst log ce. The\nadsorption of As(V) onto Fe 3O4ﬁts the Langmuir isotherm\nmodel (Figure 6B), which interprets the adsorption process as amonolayer adsorption on a homogeneous surface. In contrast,the adsorption of As(III) onto Fe\n3O4ﬁts well with the' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 5}
page_content='andKFcan be obtained by a plot of log qeagainst log ce. The\nadsorption of As(V) onto Fe 3O4ﬁts the Langmuir isotherm\nmodel (Figure 6B), which interprets the adsorption process as amonolayer adsorption on a homogeneous surface. In contrast,the adsorption of As(III) onto Fe\n3O4ﬁts well with the\nFreundlich isotherm model (Figure 6D), indicating that theadsorption process is a multilayer adsorption on a homoge-neous surface. The parameters of the Langmuir and Freundlichmodels were calculated and listed in Table S1 in the SupportingInformation. The two di ﬀerent adsorption isotherm models\nmay be attributed to the di ﬀerent surface charge e ﬀects of\nAs(V) and As(III) species under the environment of pH 5.\n77\nFor As(V), there exists an electrostatic attraction betweenpositively charged Fe\n3O4samples and negatively charged As(V)\nspecies and the adsorbed As(V) species have a repulsive e ﬀect\non As(V) species in the solution. However, As(III) existspredominantly as noncharged H' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 5}
page_content='77\nFor As(V), there exists an electrostatic attraction betweenpositively charged Fe\n3O4samples and negatively charged As(V)\nspecies and the adsorbed As(V) species have a repulsive e ﬀect\non As(V) species in the solution. However, As(III) existspredominantly as noncharged H\n3AsO 3.7The interaction\nbetween Fe 3O4samples and noncharged As(III) species is\nlittle so that the adsorption of As(III) should continue toincrease with the increase of As(III) concentration.\n77Moreover,\nsmaller 1/ nimplies stronger adsorption intensity.31Thus, the\ndecrease of 1/ nfrom Fe 3O4-2 to Fe 3O4-6 indicate that the\nadsorption process gradually becomes easier.\nThe kinetics of adsorption is one of the most important\ncharacteristics that de ﬁne the adsorption e ﬃciency, as shown in\nFigure 7. The adsorption of As(V) or As(III) is rapid at ﬁrst\nFigure 5. (A−F) TEM, (G) XRD, and (H) FT-IR spectra of Fe3O4-4-xh samples ( x= 1.5, 2.5, 4, 5, 6, 8).ACS Applied Materials & Interfaces Research Article' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 5}
page_content='characteristics that de ﬁne the adsorption e ﬃciency, as shown in\nFigure 7. The adsorption of As(V) or As(III) is rapid at ﬁrst\nFigure 5. (A−F) TEM, (G) XRD, and (H) FT-IR spectra of Fe3O4-4-xh samples ( x= 1.5, 2.5, 4, 5, 6, 8).ACS Applied Materials & Interfaces Research Article\ndx.doi.org/10.1021/am403533v |ACS Appl. Mater. Interfaces XXXX, XXX, XXX −XXX F' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 5}
page_content='and then slows considerably. The rapid adsorption at ﬁrst is\nascribed to the process of arsenic adsorption on the exterior\nsurface of the Fe 3O4particles. The slower adsorption rate\nfollowed might be partially due to the higher di ﬀusionresistance as the arsenic begins to enter and move into the\ninterior of the Fe 3O4particles via the nanopores.77Moreover,\nFe3O4-5 exhibits the highest removal e ﬃciency of 90.7% As(V)\nand 88.3% As(III), which are higher than Fe 3O4-4 (57.3%Scheme 1. Scheme of the Formation of Hierarchical Fe 3O4Particles Mediated by PDDA\nFigure 6. Adsorption isotherms of (A) As(V) and (C) As(III) onto (a) Fe 3O4-2, (b) Fe 3O4-3, (c) Fe 3O4-4, (d) Fe 3O4-5, and (e) Fe 3O4-6 samples\nwith the initial PDDA dosage of 2 g, 3 g, 4 g, 5 g, and 6 g, respectively. (B) Linearized Langmuir isotherm for As(V) adsorption and (D) linearizedFreundlich isotherm for As(III) adsorption by (a) Fe\n3O4-2, (b) Fe3O4-3, (c) Fe3O4-4, (d) Fe3O4-5, and (e) Fe3O4-6 samples ( T=2 5 °C; adsorbent' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 6}
page_content='with the initial PDDA dosage of 2 g, 3 g, 4 g, 5 g, and 6 g, respectively. (B) Linearized Langmuir isotherm for As(V) adsorption and (D) linearizedFreundlich isotherm for As(III) adsorption by (a) Fe\n3O4-2, (b) Fe3O4-3, (c) Fe3O4-4, (d) Fe3O4-5, and (e) Fe3O4-6 samples ( T=2 5 °C; adsorbent\ndoses = 0.5 g L−1;p H=5 ±0.2).ACS Applied Materials & Interfaces Research Article\ndx.doi.org/10.1021/am403533v |ACS Appl. Mater. Interfaces XXXX, XXX, XXX −XXX G' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 6}
page_content='As(V) and 37.5% As(III)) and Fe 3O4-3 (28.6% As(V) and\n19.2% As(III)). The better removal performances could beattributed to PDDA-induced high porous structure andincreased surface area of the prepared Fe\n3O4particles. All the\nabove adsorption kinetic experimental data can be best ﬁtted\ninto a pseudo-second-order rate kinetic model, which ispresented as follows\n=+t\nq kq qt11\ne t 2e2(4)\nwhere qeandqtare the amount of As(III) and As(V) adsorbed\nat equilibrium and at time t, respectively; k2is the rate constant\nof the pseudo-second-order model of adsorption (g mg−1\nmin−1). The values of k2andqecan be obtained by a plot of\n(t)/(qt) against t.\nThe conditions of As(V) and As(III) ions on the\nnanospheres were then characterized by XPS, as shown inFigure 8. Fe\n3O4with arsenic adsorbed was collected by\nmagnetic separation after the adsorption process (the initialarsenic concentration (As(V) and As(III): 5 mg L\n−1). As shown' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 7}
page_content='The conditions of As(V) and As(III) ions on the\nnanospheres were then characterized by XPS, as shown inFigure 8. Fe\n3O4with arsenic adsorbed was collected by\nmagnetic separation after the adsorption process (the initialarsenic concentration (As(V) and As(III): 5 mg L\n−1). As shown\nin the full-range XPS spectra (Figure 8A), the appearance of Asspecies and the increase of O intensity after arsenic adsorptionboth validate the arsenic adsorption on the Fe\n3O4surface. XPS\nof Fe2p of all samples (Figure 8B) show the binding energies ofFe2p\n1/2at 724.4 eV and Fe2p3/2at 710.5 eV, which are closed\nto that of Fe3O4reported.47,73XPS of As3d (Figure 8C) in\nFe3O4adsorbed As(V) shows a peak located at 45.1 eV,\nattributing to As(V) −O bonding, and that of As3d in Fe3O4Table 2. Comparison of the Adsorption Capacity of Arsenic\non Fe 3O4with Reported Inorganic Oxide\nadsorbent sample pHremoval\ncapacity for\nAs(V)\n(mg g−1)removal\ncapacity for\nAs(III)\n(mg g−1) ref\nFe3O4-5 5 6.35 6.06 this\nwork' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 7}
page_content='attributing to As(V) −O bonding, and that of As3d in Fe3O4Table 2. Comparison of the Adsorption Capacity of Arsenic\non Fe 3O4with Reported Inorganic Oxide\nadsorbent sample pHremoval\ncapacity for\nAs(V)\n(mg g−1)removal\ncapacity for\nAs(III)\n(mg g−1) ref\nFe3O4-5 5 6.35 6.06 this\nwork\nFe3O4-6 5 7.23 6.77 this\nwork\nCommercial Fe 3O45 1.35 0.76 this\nwork\nporous α-Fe2O3 4 5.3147\nporous γ-F e 2O3 4 4.7547\nfollow-like porous\nFe3O44 4.6547\nchestnutlike Fe 3O4\nhierarchicalnanstructure4 6.0731\ncubic nickel frames 7.1585\ndoughnut-like\nCuO4 4.776\nmultilayer spherical\nCuO4 0.576\nCommercial α-\nFe2O34 0.4647\ncommercial Fe 3O4\n(300 nm)4.8−8.0 1.08 1.5628\ncommercial CuO\nnanoparticles4 1.476\ncommercial TiO 2 4 4.1147\nFigure 7. (A, C) Adsorption rate of As(V) and As(III) by Fe3O4-3 (1), Fe3O4-4 (2), and Fe3O4-5 (3), samples. (B, D) Pseudo-second-order kinetic' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 7}
page_content='CuO4 0.576\nCommercial α-\nFe2O34 0.4647\ncommercial Fe 3O4\n(300 nm)4.8−8.0 1.08 1.5628\ncommercial CuO\nnanoparticles4 1.476\ncommercial TiO 2 4 4.1147\nFigure 7. (A, C) Adsorption rate of As(V) and As(III) by Fe3O4-3 (1), Fe3O4-4 (2), and Fe3O4-5 (3), samples. (B, D) Pseudo-second-order kinetic\nplots for the adsorption of As(V) and As(III). ( T=2 5 °C; adsorbent doses = 0.5 g L−1;p H=5 ±0.2).ACS Applied Materials & Interfaces Research Article\ndx.doi.org/10.1021/am403533v |ACS Appl. Mater. Interfaces XXXX, XXX, XXX −XXX H' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 7}
page_content='adsorbed As(III) shows ﬁtted peak located at 43.9 eV,\ncorresponding to As(III)-O, respectively.14,78−81The results\nconﬁrmed no major di ﬀerences in the valence state of the Fe\nand As species in arsenic adsorption. O1s XPS spectrum(Figure 8D) can be deconvoluted into peaks located at 530.0,531.5, and 533.0 eV, which are attributed to oxygen in the\nlattice (e.g., Fe −Oo rA s −O), oxygen atoms in the surface\nhydroxyl groups (H −O), and oxygen in the outermost layer of\nH\n2Oo rC O2adsorbed.14,79,82 −84The high peak intensity of\nH−O species of Fe 3O4conﬁrms the existence of many hydroxyl\ngroups on the surface of Fe 3O4spheres, which plays a vital\nimportant role in the arsenic removal.14Moreover, after arsenic\nadsorption, the shift of the O 1s binding energy to low energy,\nthe proportion decrease of the H −O (531.5 eV) and the\nproportion increase of O in the lattice (530.0 eV) suggest thatthe adsorption mechanism w a sm a i n l ya s c r i b e dt ot h e' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 8}
page_content='adsorption, the shift of the O 1s binding energy to low energy,\nthe proportion decrease of the H −O (531.5 eV) and the\nproportion increase of O in the lattice (530.0 eV) suggest thatthe adsorption mechanism w a sm a i n l ya s c r i b e dt ot h e\nsubstitution of Fe −OH groups by arsenic species.Moreover, the quick magnetic separation, high desorption\neﬃciency, and satisfactory recyclability of Fe\n3O4have been\ninvestigated, as shown in Figure 9. Taking Fe 3O4-5 as an\nexample, the Fe 3O4suspension possesses merits of not only\nquick magnetic separation (within 5 s) but also una ﬀected\nredispersion property (the size of Fe3O4-5 measured via DLS\nbefore and after magnetic separation are in the range of 240 −\n260 nm), which would greatly facilitate the application of Fe 3O4\nparticles in low-cost and high e ﬃcient water remediation. After\nthe recovery by magnetic separation, the Fe 3O4adsorbed\narsenic compounds could be treated by untrasoni ﬁcation and' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 8}
page_content='260 nm), which would greatly facilitate the application of Fe 3O4\nparticles in low-cost and high e ﬃcient water remediation. After\nthe recovery by magnetic separation, the Fe 3O4adsorbed\narsenic compounds could be treated by untrasoni ﬁcation and\nthen stirring in aqueous NaOH solution at pH 13 for 2 h,\nwhere upon they could be reused. It was found that the\ndesorption e ﬃciency was higher than 80% (86% for As(V) and\n92% for As(III)), and the removal e ﬃciency remained 85%\nafterﬁve cycles, which indicates the feasibility of regenerating\nthe Fe 3O4adsorbent.\nFigure 8. (A) Full-range, (B) Fe 2p, (C) As 3d, and (D) O 1s XPS spectra of several samples of interests including the Fe3O4,F e3O4adsorbed\nAs(V), and Fe 3O4adsorbed As(III) particles.\nFigure 9. (A) Separation/redispersion property of Fe3O4-5 under external magnetic ﬁeld (EM); (B) arsenic removal e ﬃciency of Fe3O4-5 particles\nin diﬀerent cycling numbers.ACS Applied Materials & Interfaces Research Article' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 8}
page_content='As(V), and Fe 3O4adsorbed As(III) particles.\nFigure 9. (A) Separation/redispersion property of Fe3O4-5 under external magnetic ﬁeld (EM); (B) arsenic removal e ﬃciency of Fe3O4-5 particles\nin diﬀerent cycling numbers.ACS Applied Materials & Interfaces Research Article\ndx.doi.org/10.1021/am403533v |ACS Appl. Mater. Interfaces XXXX, XXX, XXX −XXX I' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 8}
page_content='4. CONCLUSION\nA facile PDDA-modulated solvothermal method was proposed\nto synthesize porous hierarchical Fe 3O4particles with tunable\ngrain size. As the PDDA dosage increases, grain size andparticle size decrease, which yielded Fe\n3O4hierarchical particles\nwith enhanced surface area (from 7.05 to 32.75 cm3g−1) and\npromoted porosity (from 0.015 to 0.12 cm3g−1). Possible\nmechanism for PDDA-induced grain size tunable strategy canbe ascribed to capping e ﬀect and high reaction medium\nviscosity which mediate the growth and assembly of grain. Dueto the enhancement of surface area and high magnetismproperty, the prepared Fe\n3O4display improved arsenic\nadsorption performance, superior e ﬃciency in low-level arsenic\nremoval, high desorption e ﬃciency and satisfactory magnetic\nrecyclability, which are very promising compared withcommercial Fe\n3O4particles. The porous Fe3O4particles also\nprocess promising applications in other research ﬁeld such as' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 9}
page_content='adsorption performance, superior e ﬃciency in low-level arsenic\nremoval, high desorption e ﬃciency and satisfactory magnetic\nrecyclability, which are very promising compared withcommercial Fe\n3O4particles. The porous Fe3O4particles also\nprocess promising applications in other research ﬁeld such as\nbioseparation, targeted drug delivery, and catalysis. Moreover,as generally believed that building blocks assemble intohierarchical materials, this methodology, modulating theproperty of building blocks, is facile and potentially generalfor controllably synthesizing hierarchical materials with highapplication performance.\n■ASSOCIATED CONTENT\n*SSupporting Information\nAdditional information as noted in text. This material isavailable free of charge via the Internet at http://pubs.acs.org.\n■AUTHOR INFORMATION\nCorresponding Author\n*E-mail: Lychai@csu.edu.cn. Tel./Fax: +86 0731 88710171.\nAuthor Contributions\nThe authors declare no competing ﬁnancial interest. Ting' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 9}
page_content='Additional information as noted in text. This material isavailable free of charge via the Internet at http://pubs.acs.org.\n■AUTHOR INFORMATION\nCorresponding Author\n*E-mail: Lychai@csu.edu.cn. Tel./Fax: +86 0731 88710171.\nAuthor Contributions\nThe authors declare no competing ﬁnancial interest. Ting\nWang and Liyuan Zhang contributed equally to this work.\nNotes\nThe authors declare no competing ﬁnancial interest.■ACKNOWLEDGMENTS\nWe are thankful for the ﬁnancial support by National Science\nFound for Distinguished Young Scholars of China (50925417),Chang Jiang Scholars Program (T2011116), National PublicWelfare Research Project of Environmental ProtectionIndustrial (2011467062), and Key Technology for theRemediation of Arsenic Pollution in Xiangjiang River Basin(K1201010-61).\n■REFERENCES\n(1) Stone, R. Science 2008 ,321(5886), 184 −185.\n(2) Yu, X.-Y.; Xu, R.-X.; Gao, C.; Luo, T.; Jia, Y.; Liu, J.-H.; Huang,\nX.-J. ACS Appl. Mater. Interf. 2012 ,4(4), 1954 −1962.' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 9}
page_content='■REFERENCES\n(1) Stone, R. Science 2008 ,321(5886), 184 −185.\n(2) Yu, X.-Y.; Xu, R.-X.; Gao, C.; Luo, T.; Jia, Y.; Liu, J.-H.; Huang,\nX.-J. ACS Appl. Mater. Interf. 2012 ,4(4), 1954 −1962.\n(3) Manning, B. A.; Hunt, M. L.; Amrhein, C.; Yarmoff, J. A. Environ.\nSci. Technol. 2002 ,36(24), 5455 −5461.\n(4) Gupta, A.; Yunus, M.; Sankararamakrishnan, N. Ind. Eng. Chem.\nRes.2013 ,52(5), 2066 −2072.\n(5) Gihring, T. M.; Druschel, G. K.; McCleskey, R. B.; Hamers, R. J.;\nBanfield, J. F. Environ. Sci. Technol. 2001 ,35(19), 3857 −3862.\n(6) van Genuchten, C. M.; Addy, S. E.; Pen ̃a, J.; Gadgil, A. J. Environ.\nSci. Technol. 2012 ,46(2), 986 −994.\n(7) Hang, C.; Li, Q.; Gao, S.; Shang, J. K. Ind. Eng. Chem. Res. 2011 ,\n51(1), 353 −361.\n(8) Lafferty, B. J.; Ginder-Vogel, M.; Sparks, D. L. Environ. Sci.\nTechnol. 2011 ,45(21), 9218 −9223.(9) Shipley, H. J.; Yean, S.; Kan, A. T.; Tomson, M. B. Environ.\nToxicol. Chem. 2009 ,28(3), 509 −515.\n(10) Kim, J.; Benjamin, M. M. Water Res. 2004 ,38(8), 2053 −2062.' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 9}
page_content='51(1), 353 −361.\n(8) Lafferty, B. J.; Ginder-Vogel, M.; Sparks, D. L. Environ. Sci.\nTechnol. 2011 ,45(21), 9218 −9223.(9) Shipley, H. J.; Yean, S.; Kan, A. T.; Tomson, M. B. Environ.\nToxicol. Chem. 2009 ,28(3), 509 −515.\n(10) Kim, J.; Benjamin, M. M. Water Res. 2004 ,38(8), 2053 −2062.\n(11) Coronell, O.; Mi, B.; Marin ̃as, B. J.; Cahill, D. G. Environ. Sci.\nTechnol. 2012 ,47(1), 420 −428.\n(12) Hristovski, K. D.; Westerhoff, P. K.; Crittenden, J. C.; Olson, L.\nW.Environ. Sci. Technol. 2008 ,42(10), 3786 −3790.\n(13) Xu, W.; Wang, J.; Wang, L.; Sheng, G.; Liu, J.; Yu, H.; Huang,\nX.-J. J. Hazard. Mater. 2013 ,260(0), 498 −507.\n(14) Cao, C.-Y.; Qu, J.; Yan, W.-S.; Zhu, J.-F.; Wu, Z.-Y.; Song, W.-G.\nLangmuir 2012 ,28(9), 4573 −4579.\n(15) Yang, W.; Kan, A. T.; Chen, W.; Tomson, M. B. Water Res.\n2010 ,44(19), 5693 −5701.\n(16) Ai, Z.; Gao, Z.; Zhang, L.; He, W.; Yin, J. J. Environ. Sci. Technol.\n2013 ,47(10), 5344 −5352.\n(17) Wang, Y.; Zou, B.; Gao, T.; Wu, X.; Lou, S.; Zhou, S. J. Mater.' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 9}
page_content='Langmuir 2012 ,28(9), 4573 −4579.\n(15) Yang, W.; Kan, A. T.; Chen, W.; Tomson, M. B. Water Res.\n2010 ,44(19), 5693 −5701.\n(16) Ai, Z.; Gao, Z.; Zhang, L.; He, W.; Yin, J. J. Environ. Sci. Technol.\n2013 ,47(10), 5344 −5352.\n(17) Wang, Y.; Zou, B.; Gao, T.; Wu, X.; Lou, S.; Zhou, S. J. Mater.\nChem. 2012 ,22(18), 9034 −9040.\n(18) Khin, M. M.; Nair, S.; babu Veluru, J.; Rajendiran, M.;\nRamakrishna, S. Energy Environ. Sci 2012 ,5(8), 8075 −8109.\n(19) Wang, Z.; Wu, L.; Zhou, J.; Cai, W.; Shen, B.; Jiang, Z. J. Phys.\nChem. C 2013 ,117(10), 5446 −5452.\n(20) Valtchev, V.; Tosheva, L. Chem. Rev. 2013 .\n(21) Pang, X.; Zhao, L.; Han, W.; Xin, X.; Lin, Z. Nat. Nanotechnol.\n2013 ,8(6), 426 −431.\n(22) Liu, J.; Xu, J.; Che, R.; Chen, H.; Liu, M.; Liu, Z. Chem.\ue0d5Eur. J.\n2013 ,19(21), 6746 −6752.\n(23) Wang, L.; Huang, Y.; Kan, A. T.; Tomson, M. B.; Chen, W.\nEnviron. Sci. Technol. 2012 ,46(10), 5422 −5429.\n(24) Yang, L.; Luo, S.; Li, Y.; Xiao, Y.; Kang, Q.; Cai, Q. Environ. Sci.' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 9}
page_content='(22) Liu, J.; Xu, J.; Che, R.; Chen, H.; Liu, M.; Liu, Z. Chem.\ue0d5Eur. J.\n2013 ,19(21), 6746 −6752.\n(23) Wang, L.; Huang, Y.; Kan, A. T.; Tomson, M. B.; Chen, W.\nEnviron. Sci. Technol. 2012 ,46(10), 5422 −5429.\n(24) Yang, L.; Luo, S.; Li, Y.; Xiao, Y.; Kang, Q.; Cai, Q. Environ. Sci.\nTechnol. 2010 ,44(19), 7641 −7646.\n(25) Luo, X.-B.; Deng, F.; Min, L.; Luo, S.-L.; Guo, B.; Zeng, G.; Au,\nC.Environ. Sci. Technol. 2013 .\n(26) Yavuz, C. T.; Mayo, J.; William, W. Y.; Prakash, A.; Falkner, J.\nC.; Yean, S.; Cong, L.; Shipley, H. J.; Kan, A.; Tomson, M. Science\n2006 ,314(5801), 964 −967.\n(27) Zeng, H.; Singh, A.; Basak, S.; Ulrich, K.-U.; Sahu, M.; Biswas,\nP.; Catalano, J. G.; Giammar, D. E. Environ. Sci. Technol. 2009 ,43(5),\n1373−1378.\n(28) Yean, S.; Cong, L.; Yavuz, C.; Mayo, J.; Yu, W.; Kan, A.; Colvin,\nV.; Tomson, M. J. Mater. Res. 2005 ,20(12), 3255 −3264.\n(29) Cotten, G. B.; Eldredge, H. B. Sep. Sci. Technol. 2002 ,37(16),\n3755−3779.' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 9}
page_content='P.; Catalano, J. G.; Giammar, D. E. Environ. Sci. Technol. 2009 ,43(5),\n1373−1378.\n(28) Yean, S.; Cong, L.; Yavuz, C.; Mayo, J.; Yu, W.; Kan, A.; Colvin,\nV.; Tomson, M. J. Mater. Res. 2005 ,20(12), 3255 −3264.\n(29) Cotten, G. B.; Eldredge, H. B. Sep. Sci. Technol. 2002 ,37(16),\n3755−3779.\n(30) Kelland, D. R. IEEE Trans. Magn. 1998 ,34(4), 2123 −2125.\n(31) Mou, F.; Guan, J.; Ma, H.; Xu, L.; Shi, W. ACS Appl. Mater.\nInterfaces 2012 ,4(8), 3987 −3993.\n(32) Ge, J.; Huynh, T.; Hu, Y.; Yin, Y. Nano Lett. 2008 ,8(3), 931 −\n934.\n(33) Wei, Z.; Xing, R.; Zhang, X.; Liu, S.; Yu, H.; Li, P. ACS Appl.\nMater. Interfaces 2012 ,5(3), 598 −604.\n(34) Liu, G.; Deng, Q.; Wang, H.; Kang, S.; Yang, Y.; Ng, D. H.; Cai,\nW.; Wang, G. Chem.\ue0d5Eur. J. 2012 ,18(42), 13418 −13426.\n(35) Mou, F.; Guan, J.; Xiao, Z.; Sun, Z.; Shi, W.; Fan, X.-a. J. Mater.\nChem. 2011 ,21(14), 5414 −5421.\n(36) Wang, P.; Lo, I. Water Res. 2009 ,43(15), 3727 −3734.\n(37) Xuan, S.; Wang, F.; Lai, J. M.; Sham, K. W.; Wang, Y.-X. J.; Lee,' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 9}
page_content='W.; Wang, G. Chem.\ue0d5Eur. J. 2012 ,18(42), 13418 −13426.\n(35) Mou, F.; Guan, J.; Xiao, Z.; Sun, Z.; Shi, W.; Fan, X.-a. J. Mater.\nChem. 2011 ,21(14), 5414 −5421.\n(36) Wang, P.; Lo, I. Water Res. 2009 ,43(15), 3727 −3734.\n(37) Xuan, S.; Wang, F.; Lai, J. M.; Sham, K. W.; Wang, Y.-X. J.; Lee,\nS.-F.; Yu, J. C.; Cheng, C. H.; Leung, K. C.-F. ACS Appl. Mater.\nInterfaces 2011 ,3(2), 237 −244.\n(38) Ren, H.; Zhang, L.; Wang, T. T.; Li, L.; Wang, C. Chem.\nCommun. 2013 .\n(39) Lou, X. W.; Archer, L. A. Adv. Mater. 2008 ,20(10), 1853 −\n1858.\n(40) Liu, Y.; Wang, Y.; Zhou, S.; Lou, S.; Yuan, L.; Gao, T.; Wu, X.;\nShi, X.; Wang, K. ACS Appl. Mater. Interfaces 2012 ,4(9), 4913 −4920.\n(41) Wang, B.; Chen, J. S.; Wu, H. B.; Wang, Z.; Lou, X. W. J. Am.\nChem. Soc. 2011 ,133(43), 17146 −17148.\n(42) Peng, Q.; Dong, Y.; Li, Y. Angew. Chem., Int. Ed. 2003 ,42(26),\n3027−3030.\n(43) Li, X.; Si, Z.; Lei, Y.; Li, X.; Tang, J.; Song, S.; Zhang, H.' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 9}
page_content='(41) Wang, B.; Chen, J. S.; Wu, H. B.; Wang, Z.; Lou, X. W. J. Am.\nChem. Soc. 2011 ,133(43), 17146 −17148.\n(42) Peng, Q.; Dong, Y.; Li, Y. Angew. Chem., Int. Ed. 2003 ,42(26),\n3027−3030.\n(43) Li, X.; Si, Z.; Lei, Y.; Li, X.; Tang, J.; Song, S.; Zhang, H.\nCrystEngComm 2011 ,13(2), 642 −648.ACS Applied Materials & Interfaces Research Article\ndx.doi.org/10.1021/am403533v |ACS Appl. Mater. Interfaces XXXX, XXX, XXX −XXX J' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 9}
page_content='(44) Li, S.; Zhang, H.; Wu, J.; Ma, X.; Yang, D. Cryst. Growth Des.\n2006 ,6(2), 351 −353.\n(45) Liu, S.; Xing, R.; Lu, F.; Rana, R. K.; Zhu, J.-J. J. Phys. Chem. C\n2009 ,113(50), 21042 −21047.\n(46) Gao, Q.; Zhao, A.; Gan, Z.; Tao, W.; Li, D.; Zhang, M.; Guo, H.;\nWang, D.; Sun, H.; Mao, R. CrystEngComm 2012 ,14(14), 4834 −\n4842.\n(47) Zhong, L. S.; Hu, J. S.; Liang, H. P.; Cao, A. M.; Song, W. G.;\nWan, L. J. Adv. Mater. 2006 ,18(18), 2426 −2431.\n(48) Lian, J.; Duan, X.; Ma, J.; Peng, P.; Kim, T.; Zheng, W. ACS\nNano 2009 ,3(11), 3749 −3761.\n(49) Xuan, S.; Wang, Y.-X. J.; Yu, J. C.; Cham-Fai Leung, K. Chem.\nMater. 2009 ,21(21), 5079 −5087.\n(50) Zhu, Y.; Zhao, W.; Chen, H.; Shi, J. J. Phys. Chem. C 2007 ,111\n(14), 5281 −5285.\n(51) Jia, B.; Gao, L. J. Phys. Chem. C 2008 ,112(3), 666 −671.\n(52) Peng, S.; Sun, S. Angew. Chem. 2007 ,119(22), 4233 −4236.\n(53) Yin, Y.; Rioux, R. M.; Erdonmez, C. K.; Hughes, S.; Somorjai, G.\nA.; Alivisatos, A. P. Science 2004 ,304(5671), 711 −714.' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 10}
page_content='(14), 5281 −5285.\n(51) Jia, B.; Gao, L. J. Phys. Chem. C 2008 ,112(3), 666 −671.\n(52) Peng, S.; Sun, S. Angew. Chem. 2007 ,119(22), 4233 −4236.\n(53) Yin, Y.; Rioux, R. M.; Erdonmez, C. K.; Hughes, S.; Somorjai, G.\nA.; Alivisatos, A. P. Science 2004 ,304(5671), 711 −714.\n(54) Cheng, W.; Tang, K.; Qi, Y.; Sheng, J.; Liu, Z. J. Mater. Chem.\n2010 ,20(9), 1799 −1805.\n(55) Yang, H. G.; Zeng, H. C. J. Phys. Chem. B 2004 ,108(11),\n3492−3495.\n(56) Hu, P.; Yu, L.; Zuo, A.; Guo, C.; Yuan, F. J. Phys. Chem. C 2008 ,\n113(3), 900 −906.\n(57) Zhu, L.-P.; Xiao, H.-M.; Zhang, W.-D.; Yang, G.; Fu, S.-Y. Cryst.\nGrowth Des. 2008 ,8(3), 957 −963.\n(58) Chen, Y.; Xia, H.; Lu, L.; Xue, J. J. Mater. Chem. 2012 ,22(11),\n5006−5012.\n(59) Yang, H. G.; Zeng, H. C. Angew. Chem. 2004 ,116(44), 6056 −\n6059.\n(60) Wang, Y.; Zhu, Q.; Tao, L. CrystEngComm 2011 ,13(14),\n4652−4657.\n(61) Zhang, S.; Shao, Y.; Liao, H.; Engelhard, M. H.; Yin, G.; Lin, Y.\nACS Nano 2011 ,5(3), 1785 −1791.' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 10}
page_content='5006−5012.\n(59) Yang, H. G.; Zeng, H. C. Angew. Chem. 2004 ,116(44), 6056 −\n6059.\n(60) Wang, Y.; Zhu, Q.; Tao, L. CrystEngComm 2011 ,13(14),\n4652−4657.\n(61) Zhang, S.; Shao, Y.; Liao, H.; Engelhard, M. H.; Yin, G.; Lin, Y.\nACS Nano 2011 ,5(3), 1785 −1791.\n(62) Ren, W.; Fang, Y.; Wang, E. ACS Nano 2011 ,5(8), 6425 −6433.\n(63) Qin, C.; Chen, C.; Xie, Q.; Wang, L.; He, X.; Huang, Y.; Zhou,\nY.; Xie, F.; Yang, D.; Yao, S. Anal. Chim. Acta 2012 ,720,4 9−56.\n(64) Wang, S.; Yu, D.; Dai, L. J. Am. Chem. Soc. 2011 ,133(14),\n5182−5185.\n(65) Wang, S.; Yu, D.; Dai, L.; Chang, D. W.; Baek, J.-B. ACS Nano\n2011 ,5(8), 6202 −6209.\n(66) Mohan, D.; Pittman, C. U., Jr. J. Hazard. Mater. 2007 ,142(1),\n1−53.\n(67) Addo Ntim, S.; Mitra, S. J. Chem. Eng. Data 2011 ,56(5), 2077 −\n2083.\n(68) Katsoyiannis, I. A.; Zouboulis, A. I. Water Res. 2002 ,36(20),\n5141−5155.\n(69) Fan, T.; Pan, D.; Zhang, H. Ind. Eng. Chem. Res. 2011 ,50(15),\n9009−9018.\n(70) Liu, Z.-h.; Yang, X.; Makita, Y.; Ooi, K. Chem. Mater. 2002 ,14' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 10}
page_content='2083.\n(68) Katsoyiannis, I. A.; Zouboulis, A. I. Water Res. 2002 ,36(20),\n5141−5155.\n(69) Fan, T.; Pan, D.; Zhang, H. Ind. Eng. Chem. Res. 2011 ,50(15),\n9009−9018.\n(70) Liu, Z.-h.; Yang, X.; Makita, Y.; Ooi, K. Chem. Mater. 2002 ,14\n(11), 4800 −4806.\n(71) Liu, K.; Zhang, J.; Yang, G.; Wang, C.; Zhu, J.-J. Electrochem.\nCommun. 2010 ,12(3), 402 −405.\n(72) Zhu, M.; Diao, G. J. Phys. Chem. C 2011 ,115(39), 18923 −\n18934.\n(73) Zhu, H.; Hou, C.; Li, Y.; Zhao, G.; Liu, X.; Hou, K.; Li, Y. Chem.\nAsian J. 2013 ,8(7), 1447 −1454.\n(74) Lesniak, W.; Bielinska, A. U.; Sun, K.; Janczak, K. W.; Shi, X.;\nBaker, J. R.; Balogh, L. P. Nano Lett. 2005 ,5(11), 2123 −2130.\n(75) Cumberland, S. A.; Lead, J. R. J. Chromatogr. A 2009 ,1216 (52),\n9099−9105.\n(76) Cao, A.-m.; Monnell, J. D.; Matranga, C.; Wu, J.-m.; Cao, L.-l.;\nGao, D. J. Phys. Chem. C 2007 ,111(50), 18624 −18628.\n(77) Yu, X.-Y.; Luo, T.; Jia, Y.; Zhang, Y.-X.; Liu, J.-H.; Huang, X.-J. J.\nPhys. Chem. C 2011 ,115(45), 22242 −22250.' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 10}
page_content='9099−9105.\n(76) Cao, A.-m.; Monnell, J. D.; Matranga, C.; Wu, J.-m.; Cao, L.-l.;\nGao, D. J. Phys. Chem. C 2007 ,111(50), 18624 −18628.\n(77) Yu, X.-Y.; Luo, T.; Jia, Y.; Zhang, Y.-X.; Liu, J.-H.; Huang, X.-J. J.\nPhys. Chem. C 2011 ,115(45), 22242 −22250.\n(78) Kanel, S. R.; Greneche, J.-M.; Choi, H. Environ. Sci. Technol.\n2006 ,40(6), 2045 −2050.\n(79) Nesbitt, H.; Muir, I. Mineral. Petrol. 1998 ,62(1−2), 123 −144.(80) Gomes, J. A.; Daida, P.; Kesmez, M.; Weir, M.; Moreno, H.;\nParga, J. R.; Irwin, G.; McWhinney, H.; Grady, T.; Peterson, E. J.\nHazard. Mater. 2007 ,139(2), 220 −231.\n(81) Chen, B.; Zhu, Z.-L.; Ma, J.; Qiu, Y.-L.; Chen, J. J. Mater. Chem.\nA2013 ,1(37), 11355 −11367.\n(82) Wielant, J.; Hauffman, T.; Blajiev, O.; Hausbrand, R.; Terryn, H.\nJ. Phys. Chem. C 2007 ,111(35), 13177 −13184.\n(83) Ramos, M. A.; Yan, W.; Li, X.-q.; Koel, B. E.; Zhang, W.-x. J.\nPhys. Chem. C 2009 ,113(33), 14591 −14594.\n(84) Lim, S.-F.; Zheng, Y.-M.; Chen, J. P. Langmuir 2009 ,25(9),\n4973−4978.' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 10}
page_content='J. Phys. Chem. C 2007 ,111(35), 13177 −13184.\n(83) Ramos, M. A.; Yan, W.; Li, X.-q.; Koel, B. E.; Zhang, W.-x. J.\nPhys. Chem. C 2009 ,113(33), 14591 −14594.\n(84) Lim, S.-F.; Zheng, Y.-M.; Chen, J. P. Langmuir 2009 ,25(9),\n4973−4978.\n(85) Zheng, J. Y.; Wang, X.; Li, W.; Cao, Z.; Wang, H.; Zhang, C.;\nSong, W.-G.; Ma, Y.; Yao, J. CrystEngComm 2012 ,14(22), 7616 −\n7620.ACS Applied Materials & Interfaces Research Article\ndx.doi.org/10.1021/am403533v |ACS Appl. Mater. Interfaces XXXX, XXX, XXX −XXX K' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 10}
filepath=private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf,len=66
page_content='Controllable Synthesis of Hierarchical Porous Fe 3O4Particles\nMediated by Poly(diallyldimethylammonium chloride) and Their\nApplication in Arsenic Removal\nTing Wang,†Liyuan Zhang,†Haiying Wang,†,‡Weichun Yang,†,‡Yingchun Fu,§Wenli Zhou,§\nWanting Yu,†Kaisong Xiang,†Zhen Su,†Shuo Dai,†and Liyuan Chai *,†,‡\n†Department of Environmental Engineering, School of Metallurgy and Environment, Central South University, Changsha 410017, P.\nR. China\n‡Chinese National Engineering Research Center for Control & Treatment of Heavy Metal Pollution, Changsha 410017, P. R. China\n§Key Laboratory of Chemical Biology and Traditional Chinese Medicine Research (Ministry of Education), College of Chemistry and\nChemical Engineering, Hunan Normal University, Changsha 410081, P. R. China\n*SSupporting Information\nABSTRACT: Hierarchical porous Fe 3O4particles with' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 0}
page_content='§Key Laboratory of Chemical Biology and Traditional Chinese Medicine Research (Ministry of Education), College of Chemistry and\nChemical Engineering, Hunan Normal University, Changsha 410081, P. R. China\n*SSupporting Information\nABSTRACT: Hierarchical porous Fe 3O4particles with\ntunable grain size were synthesized based on a facile poly(diallyldimethylammonium chloride) (PDDA)-modulated sol-vothermal method. The products were characterized withscanning electron microscopy (SEM) and transmissionelectron microscopy (TEM), X-ray photoelectron spectrosco-py (XPS), Fourier transform infrared spectroscopy (FT-IR),X-ray di ﬀraction (XRD), N\n2adsorption −desorption technique,\nvibrating sample magnetometer (VSM), and dynamic lightscattering (DLS). The results show that increasing the PDDAdosage decrease the grain size and particle size, which increased the particle porosity and enhanced the surface area from 7.05 to32.75 m' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 0}
page_content='2adsorption −desorption technique,\nvibrating sample magnetometer (VSM), and dynamic lightscattering (DLS). The results show that increasing the PDDAdosage decrease the grain size and particle size, which increased the particle porosity and enhanced the surface area from 7.05 to32.75 m\n2g−1. Possible mechanism can be ascribed to the PDDA function on capping the crystal surface and promoting the\nviscosity of reaction medium to mediate the growth and assembly of grain. Furthermore, the arsenic adsorption application of theas-obtained Fe\n3O4samples was investigated and the adsorption mechanism was proposed. High magnetic Fe 3O4particles with\nincreased surface area display improved arsenic adsorption performance, superior e ﬃciency in low-level arsenic removal, high\ndesorption e ﬃciency, and satisfactory magnetic recyclability, which are very promising compared with commercial Fe 3O4\nparticles.\nKEYWORDS: Fe3O4, poly (diallyldimethylammonium chloride), solvothermal, adsorption' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 0}
page_content='desorption e ﬃciency, and satisfactory magnetic recyclability, which are very promising compared with commercial Fe 3O4\nparticles.\nKEYWORDS: Fe3O4, poly (diallyldimethylammonium chloride), solvothermal, adsorption\n1. INTRODUCTION\nArsenic, one of the top 20 hazardous substances, greatly\nthreatens the health of human body, ecological balance, and\nindustrial development.1Thus, the remediation of arsenic\npollution has attracted worldwide attention.2−4So far,\ntechnologies involving oxidation,5coagulation,6adsorption,7−9\nion-exchange,10and reverse osmosis11have been developed to\ndetoxicate arsenic pollution. Among them, adsorption is one of\nthe most promising technologies for arsenic removal, because\nof its easy operation and low-cost production.4,12,13However,\nthe separation of traditional adsorbent (e.g., ﬁltration,\ncentrifugation or gravitational sedimentation) is time-consum-\ning and cost-ine ﬀective and therefore limits the practical' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 0}
page_content='of its easy operation and low-cost production.4,12,13However,\nthe separation of traditional adsorbent (e.g., ﬁltration,\ncentrifugation or gravitational sedimentation) is time-consum-\ning and cost-ine ﬀective and therefore limits the practical\napplication.14Hence, magnetic adsorbents such as Fe 3O4\nexhibit unique advantages due to their quick and e ﬀective\nmagnetic separation.15−17\nIn parallel, the rapid growth of nanotechnology has attracted\na great deal of interest in environmental application.18−25In\nterms of the application of Fe3O4as an adsorbent, decreasingthe Fe3O4particle size from micrometers to nanometers would\nincrease the available adsorptive areas by 100 to 1000\ntimes.26−28However, as the Fe3O4particle size decreases to\nnanometers, its response to an external magnetic ﬁeld\nundesirably decreases, which will not be large enough to\novercome Brownian motion and no e ﬃcient magnetic\nseparation will occur.26,29,30To tackle this problem, one' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 0}
page_content='times.26−28However, as the Fe3O4particle size decreases to\nnanometers, its response to an external magnetic ﬁeld\nundesirably decreases, which will not be large enough to\novercome Brownian motion and no e ﬃcient magnetic\nseparation will occur.26,29,30To tackle this problem, one\npractical strategy is to prepare magnetic hierarchical structures,\nwhich are constructed with building blocks of nanounits. Thehierarchical nanostructures not only exhibit high speci ﬁc surface\narea because of the abundant interparticle spaces or intra-\nparticle pores, but also possess satisfactory magnetic response\nbecause of their larger size and weaker Brownian motion, which\ntherefore show great superiority to individual nanometer- and\nmicrometer-sized materials.\n31−36To date, two conventional\nReceived: August 22, 2013\nAccepted: November 19, 2013Research Article\nwww.acsami.org\n© XXXX American Chemical Society A dx.doi.org/10.1021/am403533v |ACS Appl. Mater. Interfaces XXXX, XXX, XXX −XXX' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 0}
page_content='template methods were reported to synthesize hierarchical\nnanoarchitectures, including hard templates such as silica,37\npolymer spheres,38and metal oxides,39as well as soft templates\nsuch as emulsion droplets/micelles40,41and even gas bubbles.42\nThese synthetic routes seem to be inconvenient because\ncomplicated template presynthesis or time-consuming pre-\ncursor calcination at elevated temperature is needed.37,43−46\nMoreover, the removal of templates by erosion or calcination\nbrings adverse e ﬀect on the product morphology.47,48\nConsequently, it is preferable to develop one-step template-\nfree methods for the preparation of hierarchical particles with\nwell-de ﬁned morphology.\nIt is generally believed that the grain acts as the building\nblock and its oriental assembly constitutes hierarchical particles.Consequently, it is predictable that the grain property and its\nassembly behavior synchronously in ﬂuence product morphol-\nogy.49−51To date, the reported advanced template-free' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 1}
page_content='block and its oriental assembly constitutes hierarchical particles.Consequently, it is predictable that the grain property and its\nassembly behavior synchronously in ﬂuence product morphol-\nogy.49−51To date, the reported advanced template-free\nmethods have mainly focused on the modulation of grain\nassembly by Kirkendall e ﬀect,52,53Ostwald ripening ef-\nfect45,54−57or self-attachment e ﬀect.46,58,59For instance, Yong\net al. reported that the assembly of grain evolved into porous\nFe3O4hollow submicrospheres based on Ostwald ripening\nprocess through one-pot solvothermal method.60Though the\npattern and mechanism of grain assembly were comprehen-sively investigated, rare researches were devoted to studying the\neﬀect of grain property on the performance of yielded Fe\n3O4\nhierarchical particles.\nHerein, our research aimed to controllably prepare Fe3O4\nhierarchical particles via modulating the grain property though' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 1}
page_content='eﬀect of grain property on the performance of yielded Fe\n3O4\nhierarchical particles.\nHerein, our research aimed to controllably prepare Fe3O4\nhierarchical particles via modulating the grain property though\none-pot solvothermal method. To the best of our knowledge,reports have seldom demonstrated the availability of this\nstrategy on controlling the morphology and application\nperformance of hierarchical particles. Poly-(diallyldimethylammonium chloride) (PDDA), as an environ-mentally friendly and low cost polyelectrolyte, has been widely\nused in the preparation of composites via electrostatic or π−π\nstack interaction for biosensor and catalysis.\n61−65Although the\npotential modulation usage for the synthesis of magnetic\nparticles has not been investigated. In this research, a facile\nPDDA-mediated solvothermal method was proposed tocontrollably synthesize Fe\n3O4hierarchical particles. Increasing\nthe PDDA dosage declines the grain and particle size, which' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 1}
page_content='potential modulation usage for the synthesis of magnetic\nparticles has not been investigated. In this research, a facile\nPDDA-mediated solvothermal method was proposed tocontrollably synthesize Fe\n3O4hierarchical particles. Increasing\nthe PDDA dosage declines the grain and particle size, which\nleads to the increment of speci ﬁc area and porosity, eventually\nenhancing the adsorption performance. The mechanism for\nPDDA-induced grain size tunable strategy was also discussed.\nThe prepared Fe 3O4particles show higher adsorption capacity\nthan commercial Fe3O4particles and pose great potential in the\nlow-level arsenic removal, such as the remediation of\ngroundwater (with arsenic concentration of hundreds ofmicrograms per liter66−68).\n2. EXPERIMENTAL SECTION\n2.1. Materials. Ferric chloride hexahydrate (FeCl3·6H2O),\nanhydrous sodium acetate (CH3COONa, NaAc), and ethylene glycol' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 1}
page_content='low-level arsenic removal, such as the remediation of\ngroundwater (with arsenic concentration of hundreds ofmicrograms per liter66−68).\n2. EXPERIMENTAL SECTION\n2.1. Materials. Ferric chloride hexahydrate (FeCl3·6H2O),\nanhydrous sodium acetate (CH3COONa, NaAc), and ethylene glycol\n(EG) were obtained from the Sinopharm Group Chemical ReagentCo., Ltd.. A 35.0 wt % aqueous solution of high molar mass ( M\nw100\n000−200 000) PDDA was obtained from Sigma-Aldrich. Na3AsO4·\n12H2O and NaAsO2were used as the sources of As(V) and As(III),\nrespectively. Commercial Fe3O4with the diameter of 200 nm was\npurchased from Beijing Dk Nano technology Co., Ltd.. All reagentswere used without further treatment. Ultrapure water with a resistivityof 18.2 M Ωcm\n−1, produced with a Milli-Q apparatus (Millipore), was\nused throughout all of the experiments.2.2. Preparation of Porous Fe3O4.Porous Fe3O4particles were' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 1}
page_content='−1, produced with a Milli-Q apparatus (Millipore), was\nused throughout all of the experiments.2.2. Preparation of Porous Fe3O4.Porous Fe3O4particles were\nsynthesized exploiting a facile solvothermal method via morphology-mediated by PDDA solution. In a typical procedure, 1.35 g of FeCl\n3·\n6H2O was dissolved in a mixture containing 36 mL of EG and an\nappropriate amount of PDDA solution, then 3.6 g of NaAc was added.After vigorous stirring for 30 min, a transparent solution was obtained\nand transferred to a 50 mL Te ﬂon-lined autoclave, which was then\nplaced in an oven at 200 °C for 6 h, followed by naturally cooling to\nroom temperature. The black precipitate was collected and ultrasonicwashed by water and ethanol for three times, respectively, throughmagnetic separation. The yielded product was vacuum-dried at 60 °C\nfor 12 h. By modulating the dosage of PDDA solution as 1, 2, 3, 4, 5,and 6 g, the resultant series products were separately named as Fe\n3O4-i' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 1}
page_content='for 12 h. By modulating the dosage of PDDA solution as 1, 2, 3, 4, 5,and 6 g, the resultant series products were separately named as Fe\n3O4-i\n(i=1−6). To understand the formation mechanism of Fe3O4,F e3O4-2\nand Fe3O4-4 samples were collected at various reaction time (1.5, 2.5,\n4, 5, 6, and 8 h), followed by washing and drying procedures. Theobtained series products were denoted as Fe\n3O4-2-xho rF e3O4-4-xh(x\nrefers to the reaction time).\n2.3. Characterization. Scanning electron microscopy (SEM, JSM-\n6360) and transmission electron microscopy (TEM, TECNAI G2)\nwere used to characterize the morphology of the nanoparticles. The X-ray di ﬀraction (XRD) patterns of the Fe\n3O4were obtained using\nRigaku D/Max-RB di ﬀractometer with Cu −Kαradiation ( λ= 0.15406\nnm, 35 kV, 40 mA). X-ray photoelectron spectroscopy (XPS)measurements were carried out on a Thermo Fisher Scienti ﬁcK -\nAlpha 1063 using Al K αX-ray as the excitation source. Fourier 40' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 1}
page_content='3O4were obtained using\nRigaku D/Max-RB di ﬀractometer with Cu −Kαradiation ( λ= 0.15406\nnm, 35 kV, 40 mA). X-ray photoelectron spectroscopy (XPS)measurements were carried out on a Thermo Fisher Scienti ﬁcK -\nAlpha 1063 using Al K αX-ray as the excitation source. Fourier 40\ntransformed infrared spectroscopy (FT-IR, Nicolet IS10) wasemployed to analyze the molecular structure of the yielded productat a resolution of 4 cm\n−1. The size of the pinhole and the integration\ntime were set as 100 μm and 30 s, respectively. Magnetic properties of\nthe product were investigated using a vibrating sample magnetometer(VSM, EV7, ADE) with an applied ﬁeld between −7000 and 7000 Oe\nat room temperature. Speci ﬁc surface areas of the yielded products\nwere measured by adsorption −desorption of ultrapure N\n2on a\nQuantachrome Instruments system via Brunauer −Emmett −Teller\n(BET) method. Pore size distribution was determined by N2\ndesorption isotherm using Barret −Joymer −Halender (BJH) method.' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 1}
page_content='were measured by adsorption −desorption of ultrapure N\n2on a\nQuantachrome Instruments system via Brunauer −Emmett −Teller\n(BET) method. Pore size distribution was determined by N2\ndesorption isotherm using Barret −Joymer −Halender (BJH) method.\nThe size of the Fe 3O4particles was determined by dynamic light\nscattering (DLS) on a Malvern zetasizer instrument (type Nano-ZS,\nMalvern Instruments Ltd., Britain) using Fe3O4suspension with the\nconcentration of 0.01 g L−1.\n2.4. Batch Adsorption Experiment. Solutions containing\ndiﬀerent concentrations of As(V) or As(III) were prepared and\nadjusted to pH 5 ±0.2 using HCl. Then, 5 mg of the adsorbent\nsample was added to 10 mL arsenic aqueous solution under stirring.\nAfter a speci ﬁed time, the solid and liquid were magnetic separated' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 1}
page_content='diﬀerent concentrations of As(V) or As(III) were prepared and\nadjusted to pH 5 ±0.2 using HCl. Then, 5 mg of the adsorbent\nsample was added to 10 mL arsenic aqueous solution under stirring.\nAfter a speci ﬁed time, the solid and liquid were magnetic separated\nand the initial and residual concentrations of arsenic were measured byinductively coupled plasma-optical emission spectroscopy (ICP-OES)(Optima 5300DV). The adsorption isotherm was obtained by varyingthe initial arsenic concentrations and stirring for 4 h at 25 °C\n(concentration range: 0.1 −17 mg L\n−1for As(III) and 0.1 −7.5 mg L−1\nfor As(V), respectively). For comparison, commercial Fe3O4with the\ndiameter of 200 nm synthesized by coprecipitation was also exploited.The equilibrium adsorption capacity ( q\ne) (mg g−1) for arsenic was\ncalculated according to the following equation\n=−qcc V\nm()\ne0e\n(1)\nwhere c0and ce(mg L−1) are the initial and equilibrium arsenic\naqueous concentrations, respectively; Vis the volume (mL) of arsenic' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 1}
page_content='e) (mg g−1) for arsenic was\ncalculated according to the following equation\n=−qcc V\nm()\ne0e\n(1)\nwhere c0and ce(mg L−1) are the initial and equilibrium arsenic\naqueous concentrations, respectively; Vis the volume (mL) of arsenic\naqueous solution; mis the mass (mg) of adsorbents used in the\nexperiment.\nTo test the low-level arsenic removal feasibility of adsorbents, initial\narsenic solution with As(V) concentration in the range of 50 −1400 μg\nL−1and As(III) in the range of 50 −600μgL−1were prepared. Other\nadsorption experiment procedures were the same as above.\nThe adsorption kinetics was investigated with the initial As(V)\nconcentration of 3.5 mg L−1and As(III) concentration of 3 mg L−1at\npH = 5.0 ±0.2 and adsorbents dose = 0.5 g L−1. The solution wasACS Applied Materials & Interfaces Research Article\ndx.doi.org/10.1021/am403533v |ACS Appl. Mater. Interfaces XXXX, XXX, XXX −XXX B' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 1}
page_content='allowed to react with the adsorbent for a ﬁxed period (between 10 and\n240 min).\nThe regeneration of the absorbent was conducted by using 0.1 M\nNaOH solution as eluent with adsorbents dose = 1 g L−1at 25 °C.\nBrieﬂy, the absorbent was ﬁrst ultrasoni ﬁcated in NaOH solution for\n30 min and then shaken for 2 h, followed by magnetic separation and\nwashing by water three times. Then the adsorbent was applied into\nrecycle adsorption study. The recycle adsorption experimental\nprocedure and detection method are in accordance with the ﬁrst\nadsorption experiment, including the mixing of the adsorbent witharsenic solution under stirring for 4 h, the solid and liquid separation\nby external magnetism, and the determination of the residual arsenic\nsolution by ICP-OES (Optima 5300DV).\nAll the experimental data we re the average of triplicate\ndeterminations with relative errors under 5%.\n3. RESULTS AND DISCUSSION\n3.1. Morphology, Structure, and Property of Fe 3O4.' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 2}
page_content='by external magnetism, and the determination of the residual arsenic\nsolution by ICP-OES (Optima 5300DV).\nAll the experimental data we re the average of triplicate\ndeterminations with relative errors under 5%.\n3. RESULTS AND DISCUSSION\n3.1. Morphology, Structure, and Property of Fe 3O4.\nThe SEM images and size distribution of the Fe 3O4-i(i=1−6)\nparticles are presented in Figure 1. As seen, Figure 1A −F shows\nthat the size of monodispersed hierarchical particles monoto-\nnously decreases from (A) 420 nm to (F) 100 nm, as increasingthe PDDA dosage from 1 to 6 g. Correspondingly, themorphology of hierarchical particles gradually becomes coarseand porous, with the increase of PDDA dosage. As can beconﬁrmed by TEM images in Figure S1 in the SupportingInformation, increasing the PDDA dosage concurrently\ndecreases hierarchical particle size and increases porosity.HRTEM images were also conducted to give further insightinto the grain assembly, as shown in Figure 2. Taking Fe\n3O4-4' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 2}
page_content='decreases hierarchical particle size and increases porosity.HRTEM images were also conducted to give further insightinto the grain assembly, as shown in Figure 2. Taking Fe\n3O4-4\nas an example, the particle shows pineal-like morphology withfringe spacing of 0.48 nm, corresponding to the (111) latticeplanes of Fe\n3O4. The result indicates the possible oriented\nassembly of grain along (111) plane, which is the crystallo-\ngraphic plane with the highest energy and preferential fororiented attachment.51The structures and grain size of Fe3O4\nwere further measured by XRD, as shown in Figure 3. All thediﬀraction peaks at 18.32 ±0.03, 30.10 ±0.05, 35.48 ±0.03,\n43.10 ±0.02, 53.40 ±0.04, 57.02 ±0.05, and 62.58 ±0.08 °\ncan be indexed to the indices (111), (220), (311), (400),(422), (511), and (440) of Fe\n3O4. According to the Scherrer\nformula, the grain size gradually decreased from 34.4 nm(Fe\n3O4-2) to 13.4 nm (Fe 3O4-6) with the increase of PDDA' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 2}
page_content='43.10 ±0.02, 53.40 ±0.04, 57.02 ±0.05, and 62.58 ±0.08 °\ncan be indexed to the indices (111), (220), (311), (400),(422), (511), and (440) of Fe\n3O4. According to the Scherrer\nformula, the grain size gradually decreased from 34.4 nm(Fe\n3O4-2) to 13.4 nm (Fe 3O4-6) with the increase of PDDA\ndosage (as listed in Table 1), which indicates the feasibility ofthe PDDA-induced grain size tunable strategy. Brie ﬂy speaking,\nthe SEM, TEM, and XRD results show that PDDA modulated\nsolvothermal method successfully modulate products morphol-ogy, particle size, grain size, and facilitate the oriented grainassembly.\nOn the other hand, the surface area and pore size distribution\nof as-synthesized Fe\n3O4-i(i= 2, 4, 5, 6) were determined by\nnitrogen adsorption −desorption measurements, as shown in\nFigure 4. Fe3O4-6 (Figure 4D) synthesized with the highest\nPDDA dosage possesses surface area and pore volume of 32.75m\n2g−1and 0.12 cm3g−1, respectively, both of which are higher' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 2}
page_content='of as-synthesized Fe\n3O4-i(i= 2, 4, 5, 6) were determined by\nnitrogen adsorption −desorption measurements, as shown in\nFigure 4. Fe3O4-6 (Figure 4D) synthesized with the highest\nPDDA dosage possesses surface area and pore volume of 32.75m\n2g−1and 0.12 cm3g−1, respectively, both of which are higher\nthan that of the Fe 3O4-5 (31.16 cm2g−1and 0.117 cm3g−1,\nFigure 4C), Fe 3O4-4 (19.13 m2g−1and 0.07 cm3g−1, Figure\n4B) and Fe 3O4-2 (7.05 m2g−1and 0.015 cm3g−1, Figure 4A).\nAll the samples pose pore size in the range of 7 −12 nm. The\nresults above can be ascribed to the fact that smaller grainassembly possesses more channels, leading to the increasedsurface area and pore amount. Hence, increasing the PDDAdosage yields Fe\n3O4hierarchical particles composing of smaller\ngrain, which exhibit higher surface area and porosity.\nThe magnetic property of Fe 3O4hierarchical particles was' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 2}
page_content='3O4hierarchical particles composing of smaller\ngrain, which exhibit higher surface area and porosity.\nThe magnetic property of Fe 3O4hierarchical particles was\nevaluated by examining the magnetic hysteresis loops at roomtemperature, as shown in Figure S2 in the SupportingInformation. The M\nsfor the Fe 3O4-i(i=2−6) is in the\nrange of 50 −80 emu g−1, which is comparable with many\nreported high magnetic particles.31\nBrieﬂy speaking, hierarchical porous Fe 3O4particles with\nhigh magnetism were synthesized by facile PDDA-modulatedsolvothermal method, which is achieved in one-pot solutionreaction and avoids the time/energy consuming precursorcalcination process. Furthermore, PDDA-induced grain sizetunable strategy has been proved to be an e ﬃcient way to\nenhance the surface area and porosity of particles.\n3.2. Mechanism for the Formation of Fe\n3O4Hier-\narchical Particles Mediated by PDDA. The morphology' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 2}
page_content='enhance the surface area and porosity of particles.\n3.2. Mechanism for the Formation of Fe\n3O4Hier-\narchical Particles Mediated by PDDA. The morphology\nand structure of the products with initial PDDA dosage of 4 gat various reaction time were examined by TEM, FT-IR andXRD to preliminarily understand the morphology and structureevolution of Fe\n3O4hierarchical particles, as shown in Figure 5.\nTEM results give insight into the morphology evolution of\nmesoporous Fe 3O4. As shown in Figure 5A −F), three typical\nstages were observed for the formation of Fe 3O4, namely, the\nformation of spindle precursor with length of 5 −10 nm (0 −1.5\nh), the formation and assembly of grain to sphere particles(1.5−4 h), and the oriented assembly/Ostwald ripening\nFigure 1. SEM images and the corresponding hierarchical particle size\ndistribution of (A) Fe 3O4-1, (B) Fe 3O4-2, (C) Fe 3O4-3, (D) Fe 3O4-4,\n(E) Fe3O4-5, and (F) Fe3O4-6 at initial PDDA dosage varying from 1' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 2}
page_content='Figure 1. SEM images and the corresponding hierarchical particle size\ndistribution of (A) Fe 3O4-1, (B) Fe 3O4-2, (C) Fe 3O4-3, (D) Fe 3O4-4,\n(E) Fe3O4-5, and (F) Fe3O4-6 at initial PDDA dosage varying from 1\nto 6 g.ACS Applied Materials & Interfaces Research Article\ndx.doi.org/10.1021/am403533v |ACS Appl. Mater. Interfaces XXXX, XXX, XXX −XXX C' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 2}
page_content='process of preformed sphere into porous particles (4 −8 h). The\nXRD patterns (Figure 5G) of Fe 3O4-4 at 1.5h depict a strong\npeak at 7.55 °along with a broad weak one at 25 °probably\noriginated from (001) and (013) planes of an iron oxide acetatehydroxide hydrate with a formula of Fe\n2O(CH 3COO)(OH) 3·\nH2O according to JCPDS.69Then XRD patterns of the samples\nobtained at the time from 2.5 to 4 h show gradually enhancedpeaks at 30.00, 35.48, 43.14, 53.44, 57.04, and 62.58 °, marked\nby the indices (220), (311), (400), (422), (511), and (440) ofFe\n3O4phases. When the reaction time was 6 −8 h, the produced\naggregates were pure Fe3O4. The FT-IR spectra (Figure 5H) of\nFe3O4-1.5h and Fe3O4-2.5h show reduced absorption peaks at\n1578 and 1445 cm−1due to the asymmetric and symmetric\nstretching of COO−group, band at 1090 cm−1owing to C −O\nstretching of the COO−group, band at 887 cm−1due to the\nOH bending.69The spectra of the samples within 4 −8 h show' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 3}
page_content='Fe3O4-1.5h and Fe3O4-2.5h show reduced absorption peaks at\n1578 and 1445 cm−1due to the asymmetric and symmetric\nstretching of COO−group, band at 1090 cm−1owing to C −O\nstretching of the COO−group, band at 887 cm−1due to the\nOH bending.69The spectra of the samples within 4 −8 h show\nbroad strong band at 591 cm−1due to the Fe −O lattice mode\nof Fe3O4.34Except for the anticipated typical peak for ironcomposite (Fe 2O(CH 3COO)(OH) 3·H2Oo rF e 3O4), the peak\nat 1125 cm−1was ascribed to the C −N symmetric stretching\nvibration of PDDA. The PDDA also exhibits weak CH 2\nbending vibrations (around 1474, 1326, and 960 cm−1), C−\nH asymmetric, and C −H symmetric stretching frequencies\n(2918 and 2867 cm−1).70,71Thus, both the XRD and FT-IR\nindicate the gradual formation of Fe 3O4phase at the expense of\npreformed Fe 2O(CH 3COO)(OH) 3·H2O phase.\nBrieﬂy speaking, typical three stages were observed for the\nformation of Fe 3O4, namely, the formation of spindle precursor' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 3}
page_content='(2918 and 2867 cm−1).70,71Thus, both the XRD and FT-IR\nindicate the gradual formation of Fe 3O4phase at the expense of\npreformed Fe 2O(CH 3COO)(OH) 3·H2O phase.\nBrieﬂy speaking, typical three stages were observed for the\nformation of Fe 3O4, namely, the formation of spindle precursor\nwith length of 5 −10 nm (mainly composed of Fe 2O-\n(CH 3COO)(OH) 3·H2Oa t0 −1.5 h), the formation and\nassembly of grain to sphere Fe3O4particles (1.5 −4 h), and\nthe oriented assembly/Ostwald ripening process of preformedsphere into porous Fe\n3O4particles (4 −8 h), which are in\nagreement with the reported literatures.48,51,58,69,72FT-IR\nanalysis indicates that the hierarchical particles exhibit the\nvibration of PDDA which suggests the existence of PDDA on\nthe particle surface and therefore the potential capping functionof PDDA.\nFurthermore, to reveal the e ﬀect of PDDA dosage on the\nmorphology evolution, the amount was decreased from 4 to 2 gwith otherwise the same conditions above, as shown in Figure' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 3}
page_content='the particle surface and therefore the potential capping functionof PDDA.\nFurthermore, to reveal the e ﬀect of PDDA dosage on the\nmorphology evolution, the amount was decreased from 4 to 2 gwith otherwise the same conditions above, as shown in Figure\nS3 in the Supporting Information. The grains collected at 2.5, 4,\n5, 6, and 8 h for the synthesis adopting 4 g of PDDA increase\nfrom 20.1 to 23.6 nm, indicating grain size increase by 3.5 nm;\nwhile the increment of grain size for the synthesis with 2 g ofPDDA is 9.3 nm (increase from 21.5 to 30.8 nm). The results\nreﬂect that the increase in grain size was depressed as increasing\nthe PDDA dosage, which might be partially ascribed to the\ncapping e ﬀect of PDDA. On the other hand, it is discovered\nthat the viscosity of the reaction medium was enhanced withthe increase in PDDA dosage, as shown in Table 1. It is well-\naccepted that the increment of viscosity results in the promoted\nmass transfer resistance, which is not conductive to crystal' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 3}
page_content='that the viscosity of the reaction medium was enhanced withthe increase in PDDA dosage, as shown in Table 1. It is well-\naccepted that the increment of viscosity results in the promoted\nmass transfer resistance, which is not conductive to crystal\ngrowth, thus leading to the decreased grain size.\n54,73\nFigure 2. HRTEM images (A −C) of Fe3O4-4; B and C represent the magni ﬁcation of the red area in A.\nFigure 3. XRD patterns of Fe3O4particles obtained at di ﬀerent PDDA\ndosage: (a) 2 g, (b) 3 g, (c) 4 g, (d) 5 g, (e) 6 g, with other\nexperimental parameters keeping constant.\nTable 1. Viscosity of Reaction Medium, Particle Size, Grain Size, Magnetic Properties, And Absorption Performance of Fe 3O4-i\n(i=2−6) with di ﬀerent initial PDDA dosage\nsamplesPDDA dosage\n(g)viscosity of reaction medium\n(Pa S)particle size\n(nm)grain size\n(nm)magnetic property (emu\ng−1)qm(As(V))\n(mg g−1)qe(As(III))\n(mg g−1)\nFe3O4-2 2 0.062 ±0.002 350 ±15 34.4 ±0.8 74.42 ±1.93 1.93 1.57' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 3}
page_content='(i=2−6) with di ﬀerent initial PDDA dosage\nsamplesPDDA dosage\n(g)viscosity of reaction medium\n(Pa S)particle size\n(nm)grain size\n(nm)magnetic property (emu\ng−1)qm(As(V))\n(mg g−1)qe(As(III))\n(mg g−1)\nFe3O4-2 2 0.062 ±0.002 350 ±15 34.4 ±0.8 74.42 ±1.93 1.93 1.57\nFe3O4-3 3 0.082 ±0.004 215 ±12 28.1 ±0.6 72.11 ±4.15 2.31 1.99\nFe3O4-4 4 0.106 ±0.005 195 ±10 20.2 ±0.5 57.96 ±3.47 4.07 3.29\nFe3O4-5 5 0.127 ±0.004 185 ±10 14.4 ±0.3 53.96 ±2.86 6.35 6.06\nFe3O4-6 6 0.145 ±0.006 95 ±10 13.3 ±0.5 49.21 ±3.13 7.23 6.77ACS Applied Materials & Interfaces Research Article\ndx.doi.org/10.1021/am403533v |ACS Appl. Mater. Interfaces XXXX, XXX, XXX −XXX D' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 3}
page_content='Thus, based on the discussion above, a possible mechanism\nwas proposed to elucidate the PDDA-induced grain size tunablestrategy for the controllable synthesis of porous Fe\n3O4\nhierarchical particles. As shown in Scheme 1, a mixturecomposed of FeCl\n3, EG, NaAc, and PDDA was ﬁrst obtained\nand the viscosity of mixture was greatly enhanced by PDDA.Spindle particles were then obtained with PDDA as capping\nagents, which improved the particle dispersibility. As time goes\non, hierarchical Fe\n3O4particles were eventually produced, and\nmeanwhile PDDA function on capping e ﬀect and increasing\nviscosity declines particle and grain size, facilitates oriented\nassembly, thus synchronously enhancing surface area andporosity. Brie ﬂy speaking, a PDDA-modulated solvothermal\nmethod can controllably prepare porous Fe\n3O4hierarchical\nparticles.\n3.3. Arsenic Adsorption Performance of Fe3O4.As\ndiscussed above, as-obtained Fe 3O4samples exhibit high' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 4}
page_content='assembly, thus synchronously enhancing surface area andporosity. Brie ﬂy speaking, a PDDA-modulated solvothermal\nmethod can controllably prepare porous Fe\n3O4hierarchical\nparticles.\n3.3. Arsenic Adsorption Performance of Fe3O4.As\ndiscussed above, as-obtained Fe 3O4samples exhibit high\nspeciﬁc area, high porosity, and excellent magnetic property,\nall of which are generally regarded as desirable properties of\nadsorbent for the pollutants removal. Before arsenic adsorption,the dispersibility of aqueous Fe\n3O4samples was evaluated by\ndetermining the particle size via DLS. As shown in Figure S4 in\nthe Supporting Information, the size of Fe3O4shows a\nmonotonous decrease from 520 nm (A) to 180 nm (F) withPDDA dosage from 1 to 6 g. Considering the possible plus bias\nfor DLS compared with TEM/SEM,\n74,75the results indicate\ngood dispersibility for Fe 3O4suspension. Then, the adsorption\ncapacity of Fe 3O4-i(i=2−6) for As(V) and As(III) was' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 4}
page_content='monotonous decrease from 520 nm (A) to 180 nm (F) withPDDA dosage from 1 to 6 g. Considering the possible plus bias\nfor DLS compared with TEM/SEM,\n74,75the results indicate\ngood dispersibility for Fe 3O4suspension. Then, the adsorption\ncapacity of Fe 3O4-i(i=2−6) for As(V) and As(III) was\nevaluated using the equilibrium adsorption isotherm by varyingthe initial As(V) and As(III) concentrations. As shown in\nFigure 6A and C, the adsorption capacity of As(V) and As(III)\nmonotonously increased from 1.93 and 1.57 mg g−1for Fe 3O4-\n2 to 7.23 and 6.77 mg g−1for Fe 3O4-6, indicating that the\nmorphology mediated by PDDA greatly facilitates theabsorption performance of particles. Fe\n3O4-5 and Fe3O4-6 in\nour research exhibit higher adsorption capacity than commer-\ncial Fe 3O4particles (1.35 mg g−1for As(V) and 0.76 mg g−1for\nAs(III)) and some other reported metal oxide spheres, as listed\nin Table 2.28,31,76Moreover, in terms of the low contaminated' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 4}
page_content='3O4-5 and Fe3O4-6 in\nour research exhibit higher adsorption capacity than commer-\ncial Fe 3O4particles (1.35 mg g−1for As(V) and 0.76 mg g−1for\nAs(III)) and some other reported metal oxide spheres, as listed\nin Table 2.28,31,76Moreover, in terms of the low contaminated\ndrinking water (with hundreds of microgram per liter),66the\nprepared Fe 3O4exhibits excellent removal e ﬃciency. Exploiting\nFe3O4-5 as adsorbent, arsenic solution with initial As(V)\nconcentration ≤800 μgL−1or initial As(III) concentration\n≤300μgL−1can be detoxicated to drinking water standard of\nUSA (with arsenic concentration less than 10 μgL−1) (see\nFigure S5 in the Supporting Information), indicating the\npotential application in the low-level arsenic removal.\nTwo equations, Langmuir and Freundlich isotherms models,\nw e r eu s e dt oa n a l y z et h ee x p e r i m e n t a ld a t aa n dt h e\nmathematical expressions are depicted in eq 2 and 3,\nrespectively\n=+C\nqq KC\nq1 e\nem Le\nm (2)\n=+qncK log1log loge eF(3)' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 4}
page_content='Two equations, Langmuir and Freundlich isotherms models,\nw e r eu s e dt oa n a l y z et h ee x p e r i m e n t a ld a t aa n dt h e\nmathematical expressions are depicted in eq 2 and 3,\nrespectively\n=+C\nqq KC\nq1 e\nem Le\nm (2)\n=+qncK log1log loge eF(3)\nFigure 4. Nitrogen adsorption −desorption isotherms and pore-size distribution curves (the corresponding insert) of (A) as-obtained Fe 3O4-2, (B)\nFe3O4-4, (C) Fe3O4-5, and (D) Fe3O4-6, respectively.ACS Applied Materials & Interfaces Research Article\ndx.doi.org/10.1021/am403533v |ACS Appl. Mater. Interfaces XXXX, XXX, XXX −XXX E' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 4}
page_content='where qmand KLare Langmuir constants and represent the\nmaximum adsorption capacity of adsorbents (mg g−1) and the\nenergy of adsorption, respectively. KFand nare Freundlich\nconstants related to adsorption capacity and adsorptionintensity, respectively.\nFor the Langmuir isotherm model, the values of q\nmandKL\ncan be calculated from the slope and intercept of plots of ce/qe\nversus ce. For the Freundlich isotherm model, the values of n\nandKFcan be obtained by a plot of log qeagainst log ce. The\nadsorption of As(V) onto Fe 3O4ﬁts the Langmuir isotherm\nmodel (Figure 6B), which interprets the adsorption process as amonolayer adsorption on a homogeneous surface. In contrast,the adsorption of As(III) onto Fe\n3O4ﬁts well with the' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 5}
page_content='andKFcan be obtained by a plot of log qeagainst log ce. The\nadsorption of As(V) onto Fe 3O4ﬁts the Langmuir isotherm\nmodel (Figure 6B), which interprets the adsorption process as amonolayer adsorption on a homogeneous surface. In contrast,the adsorption of As(III) onto Fe\n3O4ﬁts well with the\nFreundlich isotherm model (Figure 6D), indicating that theadsorption process is a multilayer adsorption on a homoge-neous surface. The parameters of the Langmuir and Freundlichmodels were calculated and listed in Table S1 in the SupportingInformation. The two di ﬀerent adsorption isotherm models\nmay be attributed to the di ﬀerent surface charge e ﬀects of\nAs(V) and As(III) species under the environment of pH 5.\n77\nFor As(V), there exists an electrostatic attraction betweenpositively charged Fe\n3O4samples and negatively charged As(V)\nspecies and the adsorbed As(V) species have a repulsive e ﬀect\non As(V) species in the solution. However, As(III) existspredominantly as noncharged H' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 5}
page_content='77\nFor As(V), there exists an electrostatic attraction betweenpositively charged Fe\n3O4samples and negatively charged As(V)\nspecies and the adsorbed As(V) species have a repulsive e ﬀect\non As(V) species in the solution. However, As(III) existspredominantly as noncharged H\n3AsO 3.7The interaction\nbetween Fe 3O4samples and noncharged As(III) species is\nlittle so that the adsorption of As(III) should continue toincrease with the increase of As(III) concentration.\n77Moreover,\nsmaller 1/ nimplies stronger adsorption intensity.31Thus, the\ndecrease of 1/ nfrom Fe 3O4-2 to Fe 3O4-6 indicate that the\nadsorption process gradually becomes easier.\nThe kinetics of adsorption is one of the most important\ncharacteristics that de ﬁne the adsorption e ﬃciency, as shown in\nFigure 7. The adsorption of As(V) or As(III) is rapid at ﬁrst\nFigure 5. (A−F) TEM, (G) XRD, and (H) FT-IR spectra of Fe3O4-4-xh samples ( x= 1.5, 2.5, 4, 5, 6, 8).ACS Applied Materials & Interfaces Research Article' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 5}
page_content='characteristics that de ﬁne the adsorption e ﬃciency, as shown in\nFigure 7. The adsorption of As(V) or As(III) is rapid at ﬁrst\nFigure 5. (A−F) TEM, (G) XRD, and (H) FT-IR spectra of Fe3O4-4-xh samples ( x= 1.5, 2.5, 4, 5, 6, 8).ACS Applied Materials & Interfaces Research Article\ndx.doi.org/10.1021/am403533v |ACS Appl. Mater. Interfaces XXXX, XXX, XXX −XXX F' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 5}
page_content='and then slows considerably. The rapid adsorption at ﬁrst is\nascribed to the process of arsenic adsorption on the exterior\nsurface of the Fe 3O4particles. The slower adsorption rate\nfollowed might be partially due to the higher di ﬀusionresistance as the arsenic begins to enter and move into the\ninterior of the Fe 3O4particles via the nanopores.77Moreover,\nFe3O4-5 exhibits the highest removal e ﬃciency of 90.7% As(V)\nand 88.3% As(III), which are higher than Fe 3O4-4 (57.3%Scheme 1. Scheme of the Formation of Hierarchical Fe 3O4Particles Mediated by PDDA\nFigure 6. Adsorption isotherms of (A) As(V) and (C) As(III) onto (a) Fe 3O4-2, (b) Fe 3O4-3, (c) Fe 3O4-4, (d) Fe 3O4-5, and (e) Fe 3O4-6 samples\nwith the initial PDDA dosage of 2 g, 3 g, 4 g, 5 g, and 6 g, respectively. (B) Linearized Langmuir isotherm for As(V) adsorption and (D) linearizedFreundlich isotherm for As(III) adsorption by (a) Fe\n3O4-2, (b) Fe3O4-3, (c) Fe3O4-4, (d) Fe3O4-5, and (e) Fe3O4-6 samples ( T=2 5 °C; adsorbent' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 6}
page_content='with the initial PDDA dosage of 2 g, 3 g, 4 g, 5 g, and 6 g, respectively. (B) Linearized Langmuir isotherm for As(V) adsorption and (D) linearizedFreundlich isotherm for As(III) adsorption by (a) Fe\n3O4-2, (b) Fe3O4-3, (c) Fe3O4-4, (d) Fe3O4-5, and (e) Fe3O4-6 samples ( T=2 5 °C; adsorbent\ndoses = 0.5 g L−1;p H=5 ±0.2).ACS Applied Materials & Interfaces Research Article\ndx.doi.org/10.1021/am403533v |ACS Appl. Mater. Interfaces XXXX, XXX, XXX −XXX G' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 6}
page_content='As(V) and 37.5% As(III)) and Fe 3O4-3 (28.6% As(V) and\n19.2% As(III)). The better removal performances could beattributed to PDDA-induced high porous structure andincreased surface area of the prepared Fe\n3O4particles. All the\nabove adsorption kinetic experimental data can be best ﬁtted\ninto a pseudo-second-order rate kinetic model, which ispresented as follows\n=+t\nq kq qt11\ne t 2e2(4)\nwhere qeandqtare the amount of As(III) and As(V) adsorbed\nat equilibrium and at time t, respectively; k2is the rate constant\nof the pseudo-second-order model of adsorption (g mg−1\nmin−1). The values of k2andqecan be obtained by a plot of\n(t)/(qt) against t.\nThe conditions of As(V) and As(III) ions on the\nnanospheres were then characterized by XPS, as shown inFigure 8. Fe\n3O4with arsenic adsorbed was collected by\nmagnetic separation after the adsorption process (the initialarsenic concentration (As(V) and As(III): 5 mg L\n−1). As shown' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 7}
page_content='The conditions of As(V) and As(III) ions on the\nnanospheres were then characterized by XPS, as shown inFigure 8. Fe\n3O4with arsenic adsorbed was collected by\nmagnetic separation after the adsorption process (the initialarsenic concentration (As(V) and As(III): 5 mg L\n−1). As shown\nin the full-range XPS spectra (Figure 8A), the appearance of Asspecies and the increase of O intensity after arsenic adsorptionboth validate the arsenic adsorption on the Fe\n3O4surface. XPS\nof Fe2p of all samples (Figure 8B) show the binding energies ofFe2p\n1/2at 724.4 eV and Fe2p3/2at 710.5 eV, which are closed\nto that of Fe3O4reported.47,73XPS of As3d (Figure 8C) in\nFe3O4adsorbed As(V) shows a peak located at 45.1 eV,\nattributing to As(V) −O bonding, and that of As3d in Fe3O4Table 2. Comparison of the Adsorption Capacity of Arsenic\non Fe 3O4with Reported Inorganic Oxide\nadsorbent sample pHremoval\ncapacity for\nAs(V)\n(mg g−1)removal\ncapacity for\nAs(III)\n(mg g−1) ref\nFe3O4-5 5 6.35 6.06 this\nwork' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 7}
page_content='attributing to As(V) −O bonding, and that of As3d in Fe3O4Table 2. Comparison of the Adsorption Capacity of Arsenic\non Fe 3O4with Reported Inorganic Oxide\nadsorbent sample pHremoval\ncapacity for\nAs(V)\n(mg g−1)removal\ncapacity for\nAs(III)\n(mg g−1) ref\nFe3O4-5 5 6.35 6.06 this\nwork\nFe3O4-6 5 7.23 6.77 this\nwork\nCommercial Fe 3O45 1.35 0.76 this\nwork\nporous α-Fe2O3 4 5.3147\nporous γ-F e 2O3 4 4.7547\nfollow-like porous\nFe3O44 4.6547\nchestnutlike Fe 3O4\nhierarchicalnanstructure4 6.0731\ncubic nickel frames 7.1585\ndoughnut-like\nCuO4 4.776\nmultilayer spherical\nCuO4 0.576\nCommercial α-\nFe2O34 0.4647\ncommercial Fe 3O4\n(300 nm)4.8−8.0 1.08 1.5628\ncommercial CuO\nnanoparticles4 1.476\ncommercial TiO 2 4 4.1147\nFigure 7. (A, C) Adsorption rate of As(V) and As(III) by Fe3O4-3 (1), Fe3O4-4 (2), and Fe3O4-5 (3), samples. (B, D) Pseudo-second-order kinetic' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 7}
page_content='CuO4 0.576\nCommercial α-\nFe2O34 0.4647\ncommercial Fe 3O4\n(300 nm)4.8−8.0 1.08 1.5628\ncommercial CuO\nnanoparticles4 1.476\ncommercial TiO 2 4 4.1147\nFigure 7. (A, C) Adsorption rate of As(V) and As(III) by Fe3O4-3 (1), Fe3O4-4 (2), and Fe3O4-5 (3), samples. (B, D) Pseudo-second-order kinetic\nplots for the adsorption of As(V) and As(III). ( T=2 5 °C; adsorbent doses = 0.5 g L−1;p H=5 ±0.2).ACS Applied Materials & Interfaces Research Article\ndx.doi.org/10.1021/am403533v |ACS Appl. Mater. Interfaces XXXX, XXX, XXX −XXX H' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 7}
page_content='adsorbed As(III) shows ﬁtted peak located at 43.9 eV,\ncorresponding to As(III)-O, respectively.14,78−81The results\nconﬁrmed no major di ﬀerences in the valence state of the Fe\nand As species in arsenic adsorption. O1s XPS spectrum(Figure 8D) can be deconvoluted into peaks located at 530.0,531.5, and 533.0 eV, which are attributed to oxygen in the\nlattice (e.g., Fe −Oo rA s −O), oxygen atoms in the surface\nhydroxyl groups (H −O), and oxygen in the outermost layer of\nH\n2Oo rC O2adsorbed.14,79,82 −84The high peak intensity of\nH−O species of Fe 3O4conﬁrms the existence of many hydroxyl\ngroups on the surface of Fe 3O4spheres, which plays a vital\nimportant role in the arsenic removal.14Moreover, after arsenic\nadsorption, the shift of the O 1s binding energy to low energy,\nthe proportion decrease of the H −O (531.5 eV) and the\nproportion increase of O in the lattice (530.0 eV) suggest thatthe adsorption mechanism w a sm a i n l ya s c r i b e dt ot h e' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 8}
page_content='adsorption, the shift of the O 1s binding energy to low energy,\nthe proportion decrease of the H −O (531.5 eV) and the\nproportion increase of O in the lattice (530.0 eV) suggest thatthe adsorption mechanism w a sm a i n l ya s c r i b e dt ot h e\nsubstitution of Fe −OH groups by arsenic species.Moreover, the quick magnetic separation, high desorption\neﬃciency, and satisfactory recyclability of Fe\n3O4have been\ninvestigated, as shown in Figure 9. Taking Fe 3O4-5 as an\nexample, the Fe 3O4suspension possesses merits of not only\nquick magnetic separation (within 5 s) but also una ﬀected\nredispersion property (the size of Fe3O4-5 measured via DLS\nbefore and after magnetic separation are in the range of 240 −\n260 nm), which would greatly facilitate the application of Fe 3O4\nparticles in low-cost and high e ﬃcient water remediation. After\nthe recovery by magnetic separation, the Fe 3O4adsorbed\narsenic compounds could be treated by untrasoni ﬁcation and' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 8}
page_content='260 nm), which would greatly facilitate the application of Fe 3O4\nparticles in low-cost and high e ﬃcient water remediation. After\nthe recovery by magnetic separation, the Fe 3O4adsorbed\narsenic compounds could be treated by untrasoni ﬁcation and\nthen stirring in aqueous NaOH solution at pH 13 for 2 h,\nwhere upon they could be reused. It was found that the\ndesorption e ﬃciency was higher than 80% (86% for As(V) and\n92% for As(III)), and the removal e ﬃciency remained 85%\nafterﬁve cycles, which indicates the feasibility of regenerating\nthe Fe 3O4adsorbent.\nFigure 8. (A) Full-range, (B) Fe 2p, (C) As 3d, and (D) O 1s XPS spectra of several samples of interests including the Fe3O4,F e3O4adsorbed\nAs(V), and Fe 3O4adsorbed As(III) particles.\nFigure 9. (A) Separation/redispersion property of Fe3O4-5 under external magnetic ﬁeld (EM); (B) arsenic removal e ﬃciency of Fe3O4-5 particles\nin diﬀerent cycling numbers.ACS Applied Materials & Interfaces Research Article' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 8}
page_content='As(V), and Fe 3O4adsorbed As(III) particles.\nFigure 9. (A) Separation/redispersion property of Fe3O4-5 under external magnetic ﬁeld (EM); (B) arsenic removal e ﬃciency of Fe3O4-5 particles\nin diﬀerent cycling numbers.ACS Applied Materials & Interfaces Research Article\ndx.doi.org/10.1021/am403533v |ACS Appl. Mater. Interfaces XXXX, XXX, XXX −XXX I' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 8}
page_content='4. CONCLUSION\nA facile PDDA-modulated solvothermal method was proposed\nto synthesize porous hierarchical Fe 3O4particles with tunable\ngrain size. As the PDDA dosage increases, grain size andparticle size decrease, which yielded Fe\n3O4hierarchical particles\nwith enhanced surface area (from 7.05 to 32.75 cm3g−1) and\npromoted porosity (from 0.015 to 0.12 cm3g−1). Possible\nmechanism for PDDA-induced grain size tunable strategy canbe ascribed to capping e ﬀect and high reaction medium\nviscosity which mediate the growth and assembly of grain. Dueto the enhancement of surface area and high magnetismproperty, the prepared Fe\n3O4display improved arsenic\nadsorption performance, superior e ﬃciency in low-level arsenic\nremoval, high desorption e ﬃciency and satisfactory magnetic\nrecyclability, which are very promising compared withcommercial Fe\n3O4particles. The porous Fe3O4particles also\nprocess promising applications in other research ﬁeld such as' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 9}
page_content='adsorption performance, superior e ﬃciency in low-level arsenic\nremoval, high desorption e ﬃciency and satisfactory magnetic\nrecyclability, which are very promising compared withcommercial Fe\n3O4particles. The porous Fe3O4particles also\nprocess promising applications in other research ﬁeld such as\nbioseparation, targeted drug delivery, and catalysis. Moreover,as generally believed that building blocks assemble intohierarchical materials, this methodology, modulating theproperty of building blocks, is facile and potentially generalfor controllably synthesizing hierarchical materials with highapplication performance.\n■ASSOCIATED CONTENT\n*SSupporting Information\nAdditional information as noted in text. This material isavailable free of charge via the Internet at http://pubs.acs.org.\n■AUTHOR INFORMATION\nCorresponding Author\n*E-mail: Lychai@csu.edu.cn. Tel./Fax: +86 0731 88710171.\nAuthor Contributions\nThe authors declare no competing ﬁnancial interest. Ting' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 9}
page_content='Additional information as noted in text. This material isavailable free of charge via the Internet at http://pubs.acs.org.\n■AUTHOR INFORMATION\nCorresponding Author\n*E-mail: Lychai@csu.edu.cn. Tel./Fax: +86 0731 88710171.\nAuthor Contributions\nThe authors declare no competing ﬁnancial interest. Ting\nWang and Liyuan Zhang contributed equally to this work.\nNotes\nThe authors declare no competing ﬁnancial interest.■ACKNOWLEDGMENTS\nWe are thankful for the ﬁnancial support by National Science\nFound for Distinguished Young Scholars of China (50925417),Chang Jiang Scholars Program (T2011116), National PublicWelfare Research Project of Environmental ProtectionIndustrial (2011467062), and Key Technology for theRemediation of Arsenic Pollution in Xiangjiang River Basin(K1201010-61).\n■REFERENCES\n(1) Stone, R. Science 2008 ,321(5886), 184 −185.\n(2) Yu, X.-Y.; Xu, R.-X.; Gao, C.; Luo, T.; Jia, Y.; Liu, J.-H.; Huang,\nX.-J. ACS Appl. Mater. Interf. 2012 ,4(4), 1954 −1962.' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 9}
page_content='■REFERENCES\n(1) Stone, R. Science 2008 ,321(5886), 184 −185.\n(2) Yu, X.-Y.; Xu, R.-X.; Gao, C.; Luo, T.; Jia, Y.; Liu, J.-H.; Huang,\nX.-J. ACS Appl. Mater. Interf. 2012 ,4(4), 1954 −1962.\n(3) Manning, B. A.; Hunt, M. L.; Amrhein, C.; Yarmoff, J. A. Environ.\nSci. Technol. 2002 ,36(24), 5455 −5461.\n(4) Gupta, A.; Yunus, M.; Sankararamakrishnan, N. Ind. Eng. Chem.\nRes.2013 ,52(5), 2066 −2072.\n(5) Gihring, T. M.; Druschel, G. K.; McCleskey, R. B.; Hamers, R. J.;\nBanfield, J. F. Environ. Sci. Technol. 2001 ,35(19), 3857 −3862.\n(6) van Genuchten, C. M.; Addy, S. E.; Pen ̃a, J.; Gadgil, A. J. Environ.\nSci. Technol. 2012 ,46(2), 986 −994.\n(7) Hang, C.; Li, Q.; Gao, S.; Shang, J. K. Ind. Eng. Chem. Res. 2011 ,\n51(1), 353 −361.\n(8) Lafferty, B. J.; Ginder-Vogel, M.; Sparks, D. L. Environ. Sci.\nTechnol. 2011 ,45(21), 9218 −9223.(9) Shipley, H. J.; Yean, S.; Kan, A. T.; Tomson, M. B. Environ.\nToxicol. Chem. 2009 ,28(3), 509 −515.\n(10) Kim, J.; Benjamin, M. M. Water Res. 2004 ,38(8), 2053 −2062.' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 9}
page_content='51(1), 353 −361.\n(8) Lafferty, B. J.; Ginder-Vogel, M.; Sparks, D. L. Environ. Sci.\nTechnol. 2011 ,45(21), 9218 −9223.(9) Shipley, H. J.; Yean, S.; Kan, A. T.; Tomson, M. B. Environ.\nToxicol. Chem. 2009 ,28(3), 509 −515.\n(10) Kim, J.; Benjamin, M. M. Water Res. 2004 ,38(8), 2053 −2062.\n(11) Coronell, O.; Mi, B.; Marin ̃as, B. J.; Cahill, D. G. Environ. Sci.\nTechnol. 2012 ,47(1), 420 −428.\n(12) Hristovski, K. D.; Westerhoff, P. K.; Crittenden, J. C.; Olson, L.\nW.Environ. Sci. Technol. 2008 ,42(10), 3786 −3790.\n(13) Xu, W.; Wang, J.; Wang, L.; Sheng, G.; Liu, J.; Yu, H.; Huang,\nX.-J. J. Hazard. Mater. 2013 ,260(0), 498 −507.\n(14) Cao, C.-Y.; Qu, J.; Yan, W.-S.; Zhu, J.-F.; Wu, Z.-Y.; Song, W.-G.\nLangmuir 2012 ,28(9), 4573 −4579.\n(15) Yang, W.; Kan, A. T.; Chen, W.; Tomson, M. B. Water Res.\n2010 ,44(19), 5693 −5701.\n(16) Ai, Z.; Gao, Z.; Zhang, L.; He, W.; Yin, J. J. Environ. Sci. Technol.\n2013 ,47(10), 5344 −5352.\n(17) Wang, Y.; Zou, B.; Gao, T.; Wu, X.; Lou, S.; Zhou, S. J. Mater.' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 9}
page_content='Langmuir 2012 ,28(9), 4573 −4579.\n(15) Yang, W.; Kan, A. T.; Chen, W.; Tomson, M. B. Water Res.\n2010 ,44(19), 5693 −5701.\n(16) Ai, Z.; Gao, Z.; Zhang, L.; He, W.; Yin, J. J. Environ. Sci. Technol.\n2013 ,47(10), 5344 −5352.\n(17) Wang, Y.; Zou, B.; Gao, T.; Wu, X.; Lou, S.; Zhou, S. J. Mater.\nChem. 2012 ,22(18), 9034 −9040.\n(18) Khin, M. M.; Nair, S.; babu Veluru, J.; Rajendiran, M.;\nRamakrishna, S. Energy Environ. Sci 2012 ,5(8), 8075 −8109.\n(19) Wang, Z.; Wu, L.; Zhou, J.; Cai, W.; Shen, B.; Jiang, Z. J. Phys.\nChem. C 2013 ,117(10), 5446 −5452.\n(20) Valtchev, V.; Tosheva, L. Chem. Rev. 2013 .\n(21) Pang, X.; Zhao, L.; Han, W.; Xin, X.; Lin, Z. Nat. Nanotechnol.\n2013 ,8(6), 426 −431.\n(22) Liu, J.; Xu, J.; Che, R.; Chen, H.; Liu, M.; Liu, Z. Chem.\ue0d5Eur. J.\n2013 ,19(21), 6746 −6752.\n(23) Wang, L.; Huang, Y.; Kan, A. T.; Tomson, M. B.; Chen, W.\nEnviron. Sci. Technol. 2012 ,46(10), 5422 −5429.\n(24) Yang, L.; Luo, S.; Li, Y.; Xiao, Y.; Kang, Q.; Cai, Q. Environ. Sci.' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 9}
page_content='(22) Liu, J.; Xu, J.; Che, R.; Chen, H.; Liu, M.; Liu, Z. Chem.\ue0d5Eur. J.\n2013 ,19(21), 6746 −6752.\n(23) Wang, L.; Huang, Y.; Kan, A. T.; Tomson, M. B.; Chen, W.\nEnviron. Sci. Technol. 2012 ,46(10), 5422 −5429.\n(24) Yang, L.; Luo, S.; Li, Y.; Xiao, Y.; Kang, Q.; Cai, Q. Environ. Sci.\nTechnol. 2010 ,44(19), 7641 −7646.\n(25) Luo, X.-B.; Deng, F.; Min, L.; Luo, S.-L.; Guo, B.; Zeng, G.; Au,\nC.Environ. Sci. Technol. 2013 .\n(26) Yavuz, C. T.; Mayo, J.; William, W. Y.; Prakash, A.; Falkner, J.\nC.; Yean, S.; Cong, L.; Shipley, H. J.; Kan, A.; Tomson, M. Science\n2006 ,314(5801), 964 −967.\n(27) Zeng, H.; Singh, A.; Basak, S.; Ulrich, K.-U.; Sahu, M.; Biswas,\nP.; Catalano, J. G.; Giammar, D. E. Environ. Sci. Technol. 2009 ,43(5),\n1373−1378.\n(28) Yean, S.; Cong, L.; Yavuz, C.; Mayo, J.; Yu, W.; Kan, A.; Colvin,\nV.; Tomson, M. J. Mater. Res. 2005 ,20(12), 3255 −3264.\n(29) Cotten, G. B.; Eldredge, H. B. Sep. Sci. Technol. 2002 ,37(16),\n3755−3779.' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 9}
page_content='P.; Catalano, J. G.; Giammar, D. E. Environ. Sci. Technol. 2009 ,43(5),\n1373−1378.\n(28) Yean, S.; Cong, L.; Yavuz, C.; Mayo, J.; Yu, W.; Kan, A.; Colvin,\nV.; Tomson, M. J. Mater. Res. 2005 ,20(12), 3255 −3264.\n(29) Cotten, G. B.; Eldredge, H. B. Sep. Sci. Technol. 2002 ,37(16),\n3755−3779.\n(30) Kelland, D. R. IEEE Trans. Magn. 1998 ,34(4), 2123 −2125.\n(31) Mou, F.; Guan, J.; Ma, H.; Xu, L.; Shi, W. ACS Appl. Mater.\nInterfaces 2012 ,4(8), 3987 −3993.\n(32) Ge, J.; Huynh, T.; Hu, Y.; Yin, Y. Nano Lett. 2008 ,8(3), 931 −\n934.\n(33) Wei, Z.; Xing, R.; Zhang, X.; Liu, S.; Yu, H.; Li, P. ACS Appl.\nMater. Interfaces 2012 ,5(3), 598 −604.\n(34) Liu, G.; Deng, Q.; Wang, H.; Kang, S.; Yang, Y.; Ng, D. H.; Cai,\nW.; Wang, G. Chem.\ue0d5Eur. J. 2012 ,18(42), 13418 −13426.\n(35) Mou, F.; Guan, J.; Xiao, Z.; Sun, Z.; Shi, W.; Fan, X.-a. J. Mater.\nChem. 2011 ,21(14), 5414 −5421.\n(36) Wang, P.; Lo, I. Water Res. 2009 ,43(15), 3727 −3734.\n(37) Xuan, S.; Wang, F.; Lai, J. M.; Sham, K. W.; Wang, Y.-X. J.; Lee,' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 9}
page_content='W.; Wang, G. Chem.\ue0d5Eur. J. 2012 ,18(42), 13418 −13426.\n(35) Mou, F.; Guan, J.; Xiao, Z.; Sun, Z.; Shi, W.; Fan, X.-a. J. Mater.\nChem. 2011 ,21(14), 5414 −5421.\n(36) Wang, P.; Lo, I. Water Res. 2009 ,43(15), 3727 −3734.\n(37) Xuan, S.; Wang, F.; Lai, J. M.; Sham, K. W.; Wang, Y.-X. J.; Lee,\nS.-F.; Yu, J. C.; Cheng, C. H.; Leung, K. C.-F. ACS Appl. Mater.\nInterfaces 2011 ,3(2), 237 −244.\n(38) Ren, H.; Zhang, L.; Wang, T. T.; Li, L.; Wang, C. Chem.\nCommun. 2013 .\n(39) Lou, X. W.; Archer, L. A. Adv. Mater. 2008 ,20(10), 1853 −\n1858.\n(40) Liu, Y.; Wang, Y.; Zhou, S.; Lou, S.; Yuan, L.; Gao, T.; Wu, X.;\nShi, X.; Wang, K. ACS Appl. Mater. Interfaces 2012 ,4(9), 4913 −4920.\n(41) Wang, B.; Chen, J. S.; Wu, H. B.; Wang, Z.; Lou, X. W. J. Am.\nChem. Soc. 2011 ,133(43), 17146 −17148.\n(42) Peng, Q.; Dong, Y.; Li, Y. Angew. Chem., Int. Ed. 2003 ,42(26),\n3027−3030.\n(43) Li, X.; Si, Z.; Lei, Y.; Li, X.; Tang, J.; Song, S.; Zhang, H.' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 9}
page_content='(41) Wang, B.; Chen, J. S.; Wu, H. B.; Wang, Z.; Lou, X. W. J. Am.\nChem. Soc. 2011 ,133(43), 17146 −17148.\n(42) Peng, Q.; Dong, Y.; Li, Y. Angew. Chem., Int. Ed. 2003 ,42(26),\n3027−3030.\n(43) Li, X.; Si, Z.; Lei, Y.; Li, X.; Tang, J.; Song, S.; Zhang, H.\nCrystEngComm 2011 ,13(2), 642 −648.ACS Applied Materials & Interfaces Research Article\ndx.doi.org/10.1021/am403533v |ACS Appl. Mater. Interfaces XXXX, XXX, XXX −XXX J' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 9}
page_content='(44) Li, S.; Zhang, H.; Wu, J.; Ma, X.; Yang, D. Cryst. Growth Des.\n2006 ,6(2), 351 −353.\n(45) Liu, S.; Xing, R.; Lu, F.; Rana, R. K.; Zhu, J.-J. J. Phys. Chem. C\n2009 ,113(50), 21042 −21047.\n(46) Gao, Q.; Zhao, A.; Gan, Z.; Tao, W.; Li, D.; Zhang, M.; Guo, H.;\nWang, D.; Sun, H.; Mao, R. CrystEngComm 2012 ,14(14), 4834 −\n4842.\n(47) Zhong, L. S.; Hu, J. S.; Liang, H. P.; Cao, A. M.; Song, W. G.;\nWan, L. J. Adv. Mater. 2006 ,18(18), 2426 −2431.\n(48) Lian, J.; Duan, X.; Ma, J.; Peng, P.; Kim, T.; Zheng, W. ACS\nNano 2009 ,3(11), 3749 −3761.\n(49) Xuan, S.; Wang, Y.-X. J.; Yu, J. C.; Cham-Fai Leung, K. Chem.\nMater. 2009 ,21(21), 5079 −5087.\n(50) Zhu, Y.; Zhao, W.; Chen, H.; Shi, J. J. Phys. Chem. C 2007 ,111\n(14), 5281 −5285.\n(51) Jia, B.; Gao, L. J. Phys. Chem. C 2008 ,112(3), 666 −671.\n(52) Peng, S.; Sun, S. Angew. Chem. 2007 ,119(22), 4233 −4236.\n(53) Yin, Y.; Rioux, R. M.; Erdonmez, C. K.; Hughes, S.; Somorjai, G.\nA.; Alivisatos, A. P. Science 2004 ,304(5671), 711 −714.' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 10}
page_content='(14), 5281 −5285.\n(51) Jia, B.; Gao, L. J. Phys. Chem. C 2008 ,112(3), 666 −671.\n(52) Peng, S.; Sun, S. Angew. Chem. 2007 ,119(22), 4233 −4236.\n(53) Yin, Y.; Rioux, R. M.; Erdonmez, C. K.; Hughes, S.; Somorjai, G.\nA.; Alivisatos, A. P. Science 2004 ,304(5671), 711 −714.\n(54) Cheng, W.; Tang, K.; Qi, Y.; Sheng, J.; Liu, Z. J. Mater. Chem.\n2010 ,20(9), 1799 −1805.\n(55) Yang, H. G.; Zeng, H. C. J. Phys. Chem. B 2004 ,108(11),\n3492−3495.\n(56) Hu, P.; Yu, L.; Zuo, A.; Guo, C.; Yuan, F. J. Phys. Chem. C 2008 ,\n113(3), 900 −906.\n(57) Zhu, L.-P.; Xiao, H.-M.; Zhang, W.-D.; Yang, G.; Fu, S.-Y. Cryst.\nGrowth Des. 2008 ,8(3), 957 −963.\n(58) Chen, Y.; Xia, H.; Lu, L.; Xue, J. J. Mater. Chem. 2012 ,22(11),\n5006−5012.\n(59) Yang, H. G.; Zeng, H. C. Angew. Chem. 2004 ,116(44), 6056 −\n6059.\n(60) Wang, Y.; Zhu, Q.; Tao, L. CrystEngComm 2011 ,13(14),\n4652−4657.\n(61) Zhang, S.; Shao, Y.; Liao, H.; Engelhard, M. H.; Yin, G.; Lin, Y.\nACS Nano 2011 ,5(3), 1785 −1791.' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 10}
page_content='5006−5012.\n(59) Yang, H. G.; Zeng, H. C. Angew. Chem. 2004 ,116(44), 6056 −\n6059.\n(60) Wang, Y.; Zhu, Q.; Tao, L. CrystEngComm 2011 ,13(14),\n4652−4657.\n(61) Zhang, S.; Shao, Y.; Liao, H.; Engelhard, M. H.; Yin, G.; Lin, Y.\nACS Nano 2011 ,5(3), 1785 −1791.\n(62) Ren, W.; Fang, Y.; Wang, E. ACS Nano 2011 ,5(8), 6425 −6433.\n(63) Qin, C.; Chen, C.; Xie, Q.; Wang, L.; He, X.; Huang, Y.; Zhou,\nY.; Xie, F.; Yang, D.; Yao, S. Anal. Chim. Acta 2012 ,720,4 9−56.\n(64) Wang, S.; Yu, D.; Dai, L. J. Am. Chem. Soc. 2011 ,133(14),\n5182−5185.\n(65) Wang, S.; Yu, D.; Dai, L.; Chang, D. W.; Baek, J.-B. ACS Nano\n2011 ,5(8), 6202 −6209.\n(66) Mohan, D.; Pittman, C. U., Jr. J. Hazard. Mater. 2007 ,142(1),\n1−53.\n(67) Addo Ntim, S.; Mitra, S. J. Chem. Eng. Data 2011 ,56(5), 2077 −\n2083.\n(68) Katsoyiannis, I. A.; Zouboulis, A. I. Water Res. 2002 ,36(20),\n5141−5155.\n(69) Fan, T.; Pan, D.; Zhang, H. Ind. Eng. Chem. Res. 2011 ,50(15),\n9009−9018.\n(70) Liu, Z.-h.; Yang, X.; Makita, Y.; Ooi, K. Chem. Mater. 2002 ,14' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 10}
page_content='2083.\n(68) Katsoyiannis, I. A.; Zouboulis, A. I. Water Res. 2002 ,36(20),\n5141−5155.\n(69) Fan, T.; Pan, D.; Zhang, H. Ind. Eng. Chem. Res. 2011 ,50(15),\n9009−9018.\n(70) Liu, Z.-h.; Yang, X.; Makita, Y.; Ooi, K. Chem. Mater. 2002 ,14\n(11), 4800 −4806.\n(71) Liu, K.; Zhang, J.; Yang, G.; Wang, C.; Zhu, J.-J. Electrochem.\nCommun. 2010 ,12(3), 402 −405.\n(72) Zhu, M.; Diao, G. J. Phys. Chem. C 2011 ,115(39), 18923 −\n18934.\n(73) Zhu, H.; Hou, C.; Li, Y.; Zhao, G.; Liu, X.; Hou, K.; Li, Y. Chem.\nAsian J. 2013 ,8(7), 1447 −1454.\n(74) Lesniak, W.; Bielinska, A. U.; Sun, K.; Janczak, K. W.; Shi, X.;\nBaker, J. R.; Balogh, L. P. Nano Lett. 2005 ,5(11), 2123 −2130.\n(75) Cumberland, S. A.; Lead, J. R. J. Chromatogr. A 2009 ,1216 (52),\n9099−9105.\n(76) Cao, A.-m.; Monnell, J. D.; Matranga, C.; Wu, J.-m.; Cao, L.-l.;\nGao, D. J. Phys. Chem. C 2007 ,111(50), 18624 −18628.\n(77) Yu, X.-Y.; Luo, T.; Jia, Y.; Zhang, Y.-X.; Liu, J.-H.; Huang, X.-J. J.\nPhys. Chem. C 2011 ,115(45), 22242 −22250.' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 10}
page_content='9099−9105.\n(76) Cao, A.-m.; Monnell, J. D.; Matranga, C.; Wu, J.-m.; Cao, L.-l.;\nGao, D. J. Phys. Chem. C 2007 ,111(50), 18624 −18628.\n(77) Yu, X.-Y.; Luo, T.; Jia, Y.; Zhang, Y.-X.; Liu, J.-H.; Huang, X.-J. J.\nPhys. Chem. C 2011 ,115(45), 22242 −22250.\n(78) Kanel, S. R.; Greneche, J.-M.; Choi, H. Environ. Sci. Technol.\n2006 ,40(6), 2045 −2050.\n(79) Nesbitt, H.; Muir, I. Mineral. Petrol. 1998 ,62(1−2), 123 −144.(80) Gomes, J. A.; Daida, P.; Kesmez, M.; Weir, M.; Moreno, H.;\nParga, J. R.; Irwin, G.; McWhinney, H.; Grady, T.; Peterson, E. J.\nHazard. Mater. 2007 ,139(2), 220 −231.\n(81) Chen, B.; Zhu, Z.-L.; Ma, J.; Qiu, Y.-L.; Chen, J. J. Mater. Chem.\nA2013 ,1(37), 11355 −11367.\n(82) Wielant, J.; Hauffman, T.; Blajiev, O.; Hausbrand, R.; Terryn, H.\nJ. Phys. Chem. C 2007 ,111(35), 13177 −13184.\n(83) Ramos, M. A.; Yan, W.; Li, X.-q.; Koel, B. E.; Zhang, W.-x. J.\nPhys. Chem. C 2009 ,113(33), 14591 −14594.\n(84) Lim, S.-F.; Zheng, Y.-M.; Chen, J. P. Langmuir 2009 ,25(9),\n4973−4978.' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 10}
page_content='J. Phys. Chem. C 2007 ,111(35), 13177 −13184.\n(83) Ramos, M. A.; Yan, W.; Li, X.-q.; Koel, B. E.; Zhang, W.-x. J.\nPhys. Chem. C 2009 ,113(33), 14591 −14594.\n(84) Lim, S.-F.; Zheng, Y.-M.; Chen, J. P. Langmuir 2009 ,25(9),\n4973−4978.\n(85) Zheng, J. Y.; Wang, X.; Li, W.; Cao, Z.; Wang, H.; Zhang, C.;\nSong, W.-G.; Ma, Y.; Yao, J. CrystEngComm 2012 ,14(22), 7616 −\n7620.ACS Applied Materials & Interfaces Research Article\ndx.doi.org/10.1021/am403533v |ACS Appl. Mater. Interfaces XXXX, XXX, XXX −XXX K' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 10}
filepath=private_upload/2023-08-09-17/SELF-INSTRUCT.pdf,len=125
page_content='ACL 2023\nSELF-INSTRUCT : Aligning Language Models\nwith Self-Generated Instructions\nYizhong Wang♣Yeganeh Kordi♢Swaroop Mishra♡Alisa Liu♣\nNoah A. Smith♣+Daniel Khashabi♠Hannaneh Hajishirzi♣+\n♣University of Washington♢Tehran Polytechnic♡Arizona State University\n♠Johns Hopkins University+Allen Institute for AI\nyizhongw@cs.washington.edu\nAbstract\nLarge “instruction-tuned” language models\n(i.e., finetuned to respond toinstructions) have\ndemonstrated a remarkable ability to gener-\nalize zero-shot to new tasks. Nevertheless,\nthey depend heavily on human-written instruc-\ntion data that is often limited in quantity, di-\nversity,andcreativity,thereforehinderingthe\ngenerality of the tuned model. We introduce\nSELF-INSTRUCT , a framework for improving\nthe instruction-following capabilities of pre-\ntrained language models by bootstrapping off\ntheirowngenerations. Ourpipelinegenerates\ninstructions, input, and output samples from\na language model, then filters invalid or sim-' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 0}
page_content='SELF-INSTRUCT , a framework for improving\nthe instruction-following capabilities of pre-\ntrained language models by bootstrapping off\ntheirowngenerations. Ourpipelinegenerates\ninstructions, input, and output samples from\na language model, then filters invalid or sim-\nilar ones before using them to finetune the\noriginal model. Applying our method to the\nvanillaGPT3, we demonstrate a 33% abso-\nlute improvement over the original model on\nSUPER-NATURALINSTRUCTIONS ,onparwith\nthe performance of InstructGPT001,1which\nwas trained with private user data and human\nannotations. Forfurtherevaluation,wecuratea\nsetofexpert-writteninstructionsfornoveltasks,\nandshowthroughhumanevaluationthattuning\nGPT3 with SELF-INSTRUCT outperforms using\nexisting public instruction datasets by a large\nmargin, leaving only a 5% absolute gap behind\nInstructGPT001.SELF-INSTRUCT provides an\nalmost annotation-free method for aligning pre-\ntrained language models with instructions, and' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 0}
page_content='GPT3 with SELF-INSTRUCT outperforms using\nexisting public instruction datasets by a large\nmargin, leaving only a 5% absolute gap behind\nInstructGPT001.SELF-INSTRUCT provides an\nalmost annotation-free method for aligning pre-\ntrained language models with instructions, and\nwereleaseourlargesyntheticdatasettofacili-\ntate future studies on instruction tuning.2\n1 Introduction\nTherecentNLPliteraturehaswitnessedatremen-\ndous amount of activity in building models that\n1Unless otherwise specified, our comparisons are with the\ntext-davinci-001 engine. We focus on this engine since it\nis the closest to our experimental setup: supervised finetuning\nwith human demonstrations. The newer engines are more\npowerful,thoughtheyusemoredata(e.g.,codecompletionor\nlatest user queries) oralgorithms(e.g., PPO) that are difficult\nto compare with.\n2Code and data are available at https://github.com/\nyizhongw/self-instruct' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 0}
page_content='with human demonstrations. The newer engines are more\npowerful,thoughtheyusemoredata(e.g.,codecompletionor\nlatest user queries) oralgorithms(e.g., PPO) that are difficult\nto compare with.\n2Code and data are available at https://github.com/\nyizhongw/self-instruct\nInstruction:Write a letter from the perspective of a cat.Input:NullOutput:Dear [Owner], I am writing to you today because I have aproblem. You see, I’ve been living with you for the past[Number of Years] and I love it here. The food is great,the bed is comfy, and there are plenty of toys to playwith. But lately, I’ve noticed that you’re not asaffectionate towards me as you used to be. In fact,sometimes when I try to cuddle up next to you on thecouch, you push me away...' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 0}
page_content='Instruction:I am looking for a job and Ineed to fill out an application form. Canyou please help me complete it?Input:Application Form:Name:_________Age:__________Sex:________Phone Number:______Email Address:________Education:___________________...Output:Name: John DoeAge: 25Sex: MalePhone Number: ...\nInstruction:How to write a code for converting degrees fahrenheitto celsius.Input:NullOutput:def convert_fahrenheit_to_celsius(fahr):celsius= (fahr-32) * 5 / 9return celsius\nInstruction:Given an address and city, come up withthe zip code.Input:Address: 123 Main Street, City: San FranciscoOutput:94105\nFigure 1: Selected tasks from the generated instruction\ndata using vanilla GPT3. Some texts are reformatted\nfor presentation. See Table 10 for more examples.\ncan follow natural language instructions (Mishra\net al., 2022; Wei et al., 2022; Sanh et al., 2022;\nWangetal.,2022;Ouyangetal.,2022;Chungetal.,\n2022,i.a.). Thesedevelopmentsarepoweredbytwo\nkey components: large pretrained language mod-' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 0}
page_content='for presentation. See Table 10 for more examples.\ncan follow natural language instructions (Mishra\net al., 2022; Wei et al., 2022; Sanh et al., 2022;\nWangetal.,2022;Ouyangetal.,2022;Chungetal.,\n2022,i.a.). Thesedevelopmentsarepoweredbytwo\nkey components: large pretrained language mod-\nels (LM) and human-written instruction data (e.g.,\nPROMPTSOURCE(Bach et al., 2022) and SUPER-\nNATURALINSTRUCTIONS (Wangetal.,2022, SU-\nPERNIfor short)). However, collecting such in-\nstruction data is costly and often suffers limited\ndiversitygiventhatmosthumangenerationstend\ntobepopularNLPtasks,fallingshortofcoveringaarXiv:2212.10560v2  [cs.CL]  25 May 2023' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 0}
page_content='175seed tasks with1instructionand1instancepertaskTask PoolStep1:InstructionGeneration\nNo\nStep4:FilteringOutput-firstInput-firstStep2:ClassificationTaskIdentification\nStep3:InstanceGeneration\nInstruction :Give me a quote from a famous person on this topic.Task\nYes\nTaskInstruction :Give me a quote from a famous person on this topic.Input:Topic: The importance of being honest.Output:"Honesty is the first chapter in the book of wisdom." -Thomas JeffersonTaskTaskInstruction :Find out if the given text is in favor of or against abortion.Class Label: Pro-abortionInput:Text: I believe that women should have the right to choose whether or notthey want to have an abortion.TaskLMLM\nLM\n🤖\n🤖\n🤖Figure2: Ahigh-leveloverviewof SELF-INSTRUCT . Theprocessstartswithasmallseedsetoftasksasthetask\npool. Randomtasks are sampledfrom the taskpool, and usedto prompt an off-the-shelfLM to generateboth new\ninstructionsand correspondinginstances, followed byfiltering low-qualityor similargenerations, and thenadded' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 1}
page_content='pool. Randomtasks are sampledfrom the taskpool, and usedto prompt an off-the-shelfLM to generateboth new\ninstructionsand correspondinginstances, followed byfiltering low-qualityor similargenerations, and thenadded\nbacktotheinitialrepositoryoftasks. Theresultingdatacanbeusedfortheinstructiontuningofthelanguagemodel\nitself later to follow instructions better. Tasks shown in the figure are generated by GPT3.\ntrue variety of tasks and different ways to describe\nthem. Continuingtoimprovethequalityandcov-\nerage ofinstruction-tuned models necessitatesthe\ndevelopmentofalternativeapproachesforsupervis-\ning the instruction tuning process.\nIn this work, we introduce SELF-INSTRUCT , a\nsemi-automated process for instruction-tuning a\npretrained LM using instructional signals from the\nmodelitself. Theoverallprocessisaniterativeboot-\nstrapping algorithm (see Figure 2), which starts off\nwith a limited (e.g., 175 in our study) seed set of\nmanually-written tasks that are used to guide the' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 1}
page_content='pretrained LM using instructional signals from the\nmodelitself. Theoverallprocessisaniterativeboot-\nstrapping algorithm (see Figure 2), which starts off\nwith a limited (e.g., 175 in our study) seed set of\nmanually-written tasks that are used to guide the\noverall generation. In the first phase, the model\nispromptedtogenerateinstructionsfornewtasks.\nThisstepleveragestheexistingcollectionofinstruc-\ntions to create more broad-coverage instructions\nthat define (often new) tasks. Given the newly-\ngenerated set of instructions, the framework also\ncreates input-output instances for them, which can\nbe later used for supervising the instruction tuning.\nFinally,variousheuristicsareusedtoautomatically\nfilter low-quality or repeated instructions, before\nadding the remaining valid tasks to the task pool.\nThis process can be repeated for many iterations\nuntil reaching a large number of tasks.\nToevaluate SELF-INSTRUCT empirically,werun\nthis framework on GPT3(Brown et al., 2020),' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 1}
page_content='filter low-quality or repeated instructions, before\nadding the remaining valid tasks to the task pool.\nThis process can be repeated for many iterations\nuntil reaching a large number of tasks.\nToevaluate SELF-INSTRUCT empirically,werun\nthis framework on GPT3(Brown et al., 2020),\nwhich is a vanilla LM (§3). The iterative SELF-\nINSTRUCT processonthismodelleadstoabout52k\ninstructions, paired with about 82K instance inputs\nand target outputs. We observe that the resultingdataprovidesadiverserangeofcreativetasks,as\nis demonstrated by examples in Figure 1. These\ngeneratedtasksdeviatefromthedistributionoftyp-\nicalNLPtasks,andalsohavefairlysmalloverlap\nwith the seed tasks (§3.2). On this resulting data,\nwe build GPT3SELF-INSTby finetuning GPT3(i.e.,\nthe same model used for generating the instruction\ndata). Weevaluate GPT3SELF-INSTincomparisonto\nvarious other models on both typical NLP tasks in-\ncludedin SUPERNI(Wangetal.,2022),andasetof\nnew instructions that are created for novel usage of' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 1}
page_content='we build GPT3SELF-INSTby finetuning GPT3(i.e.,\nthe same model used for generating the instruction\ndata). Weevaluate GPT3SELF-INSTincomparisonto\nvarious other models on both typical NLP tasks in-\ncludedin SUPERNI(Wangetal.,2022),andasetof\nnew instructions that are created for novel usage of\ninstruction-followingmodels(§4). Theresultsin-\ndicatethat GPT3SELF-INSToutperforms GPT3(the\noriginal model) by a large margin (+33.1%) and\nnearly matches the performance of InstructGPT001.\nMoreover, our human evaluation on the newly-\ncreated instruction set shows that GPT3SELF-INST\ndemonstrates a broad range of instruction follow-\ning ability, outperforming models trained on other\npubliclyavailableinstructiondatasetsandleaving\nonly a 5% gap behind InstructGPT001.\nIn summary, our contributions are: (1) we\nintroduce SELF-INSTRUCT , a method for induc-\ning instruction following capabilities with mini-\nmal human-labeled data; (2) we demonstrate its\neffectiveness via extensive instruction-tuning ex-' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 1}
page_content='only a 5% gap behind InstructGPT001.\nIn summary, our contributions are: (1) we\nintroduce SELF-INSTRUCT , a method for induc-\ning instruction following capabilities with mini-\nmal human-labeled data; (2) we demonstrate its\neffectiveness via extensive instruction-tuning ex-\nperiments; and (3) we release a large synthetic\ndatasetof52Kinstructionsandasetofmanually-\nwrittennoveltasksforbuildingandevaluatingfu-\nture instruction-following models.' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 1}
page_content='2 Method\nAnnotating large-scale instruction data can be chal-\nlengingforhumansbecauseitrequires1)creativity\nto come up with novel tasks and 2) expertise for\nwriting the solutions to each task. Here, we de-\ntailourprocessfor SELF-INSTRUCT ,whichrefers\nto the pipeline of generating tasks with a vanilla\npretrainedlanguagemodel itself, filteringthegen-\nerated data, and then conducting instruction tuning\nwith this generated data in order to align the LM to\nfollow instructions better. This pipeline is depicted\nin Figure 2.\n2.1 Defining Instruction Data\nTheinstructiondatawewanttogeneratecontainsa\nsetofinstructions {𝐼𝑡},eachofwhichdefinesatask\n𝑡innaturallanguage. Task 𝑡has𝑛𝑡≥1input-output\ninstances {(𝑋𝑡,𝑖,𝑌𝑡,𝑖)}𝑛𝑡\n𝑖=1. Amodel𝑀isexpected\nto produce the output, given the task instruction\nand the corresponding input: 𝑀(𝐼𝑡,𝑋𝑡,𝑖) =𝑌𝑡,𝑖,\nfor𝑖∈ {1,…,𝑛𝑡}. Note that the instruction and\ninstance input does not have a strict boundary in\nmany cases. For example, “write an essay about' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 2}
page_content='instances {(𝑋𝑡,𝑖,𝑌𝑡,𝑖)}𝑛𝑡\n𝑖=1. Amodel𝑀isexpected\nto produce the output, given the task instruction\nand the corresponding input: 𝑀(𝐼𝑡,𝑋𝑡,𝑖) =𝑌𝑡,𝑖,\nfor𝑖∈ {1,…,𝑛𝑡}. Note that the instruction and\ninstance input does not have a strict boundary in\nmany cases. For example, “write an essay about\nschool safety” can be a valid instruction that we\nexpect models to respond to directly, while it can\nalso be formulated as “write an essay about the fol-\nlowingtopic”astheinstruction,and“schoolsafety”\nas an instance input. To encourage the diversity of\nthe data format, we allow such instructions that do\nnot require additional input (i.e., 𝑋is empty).\n2.2 Automatic Instruction Data Generation\nOur pipeline for data generation consists of four\nsteps: 1)generating task instructions,2) determin-\ningiftheinstructionrepresentsaclassificationtask,\n3)instancegenerationwitheitheraninput-firstor\noutput-first approach, and 4) filtering low-quality\ndata.\nInstruction Generation. At the first step, SELF-' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 2}
page_content='steps: 1)generating task instructions,2) determin-\ningiftheinstructionrepresentsaclassificationtask,\n3)instancegenerationwitheitheraninput-firstor\noutput-first approach, and 4) filtering low-quality\ndata.\nInstruction Generation. At the first step, SELF-\nINSTRUCT generates new instructions from a small\nset of seed human-written instructions in a boot-\nstrapping fashion. We initiate the task pool with\n175 tasks (1 instruction and 1 instance for each\ntask).3For every step, we sample 8 task instruc-\ntions from this pool as in-context examples. Of\nthe 8 instructions, 6 are from the human-written\n3Thesetaskswerenewlywrittenbytheauthorsandtheir\nlabmatesat UW,withoutreference toexistingdatasets orthe\ntestsetusedinthiswork. Weprovidemoredetailsaboutthese\ntasks and analyze their similarity to the test tasks in Appendix\n§A.1.tasks, and 2 are from the model-generated tasks in\npreviousstepstopromotediversity. Theprompting\ntemplate is shown in Table 5.\nClassification Task Identification. Because we' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 2}
page_content='testsetusedinthiswork. Weprovidemoredetailsaboutthese\ntasks and analyze their similarity to the test tasks in Appendix\n§A.1.tasks, and 2 are from the model-generated tasks in\npreviousstepstopromotediversity. Theprompting\ntemplate is shown in Table 5.\nClassification Task Identification. Because we\nneedtwodifferentapproachesforclassificationand\nnon-classificationtasks,wenextidentifywhether\nthe generated instruction represents a classification\ntaskornot.4WeprompttheLMinafew-shotwayto\ndetermine this, using 12 classification instructions\nand19non-classificationinstructionsfromtheseed\ntasks. The prompting template is shown in Table 6.\nInstanceGeneration. Giventheinstructionsand\ntheir task type, we generate instances for each in-\nstruction independently. This is challenging be-\ncause itrequires themodel to understandwhat the\ntarget task is, based on the instruction, figure out\nwhatadditionalinputfieldsareneededandgener-\natethem,andfinallycompletethetaskbyproduc-' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 2}
page_content='their task type, we generate instances for each in-\nstruction independently. This is challenging be-\ncause itrequires themodel to understandwhat the\ntarget task is, based on the instruction, figure out\nwhatadditionalinputfieldsareneededandgener-\natethem,andfinallycompletethetaskbyproduc-\ningtheoutput. WefoundthatpretrainedLMscan\nachievethistoalargeextentwhenpromptedwith\ninstruction-input-outputin-contextexamplesfrom\nothertasks. Anaturalwaytodothisisthe Input-\nfirst Approach , where we can ask an LM to come\nup with the input fields first based on the instruc-\ntion, and then produce the corresponding output.\nThis generation order is similar to how models are\nused to respond to instruction and input, but here\nwith in-context examples from other tasks. The\nprompting template is shown in Table 7.\nHowever, we found that this approach can gen-\nerate inputs biased toward one label, especially for\nclassificationtasks(e.g.,forgrammarerrordetec-\ntion,itusuallygeneratesgrammaticalinput). There-' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 2}
page_content='with in-context examples from other tasks. The\nprompting template is shown in Table 7.\nHowever, we found that this approach can gen-\nerate inputs biased toward one label, especially for\nclassificationtasks(e.g.,forgrammarerrordetec-\ntion,itusuallygeneratesgrammaticalinput). There-\nfore,weadditionallyproposean Output-firstAp-\nproachforclassificationtasks,wherewefirstgener-\nate the possible class labels, and then condition the\ninputgenerationoneachclasslabel. Theprompting\ntemplateisshowninTable8.5Weapplytheoutput-\nfirstapproachtotheclassificationtasksidentified\nin the former step, and the input-first approach to\nthe remaining non-classification tasks.\nFiltering andPostprocessing. To encouragedi-\nversity, a new instruction is added to the task pool\nonly when its ROUGE-L similarity with any exist-\n4Moreconcretely,weregardtasksthathaveasmalllimited\noutput label space as classification tasks.\n5Inthiswork,weuseafixedsetofseedtasksforprompt-' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 2}
page_content='Filtering andPostprocessing. To encouragedi-\nversity, a new instruction is added to the task pool\nonly when its ROUGE-L similarity with any exist-\n4Moreconcretely,weregardtasksthathaveasmalllimited\noutput label space as classification tasks.\n5Inthiswork,weuseafixedsetofseedtasksforprompt-\ning the instance generation, and thus only generate a small\nnumber of instances per task in one round. Future work can\nuse randomly sampled tasks to prompt the model to generate\na larger number of instances in multiple rounds.' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 2}
page_content='ing instruction is less than 0.7. We also exclude\ninstructions that contain some specific keywords\n(e.g., image, picture, graph) that usually can not be\nprocessedbyLMs. Whengeneratingnewinstances\nfor each instruction, we filter out instances that are\nexactlythesameorthosewiththesameinputbut\ndifferent outputs. Invalid generations are identified\nand filtered out based on heuristics (e.g., instruc-\ntion is too long or too short, instance output is a\nrepetition of the input).\n2.3 Finetuning the LM to Follow Instructions\nAftercreatinglarge-scaleinstructiondata,weuseit\nto finetune the original LM (i.e., SELF-INSTRUCT ).\nTo do this, we concatenate the instruction and in-\nstance input as a prompt and train the model to\ngenerate the instance output in a standard super-\nvised way. To make the model robust to different\nformats, we use multiple templates to encode the\ninstructionandinstanceinputtogether. Forexam-\nple, the instruction can be prefixed with “Task:” or' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 3}
page_content='stance input as a prompt and train the model to\ngenerate the instance output in a standard super-\nvised way. To make the model robust to different\nformats, we use multiple templates to encode the\ninstructionandinstanceinputtogether. Forexam-\nple, the instruction can be prefixed with “Task:” or\nnot,theinput canbeprefixedwith“Input:” ornot,\n“Output:” canbeappendedattheendoftheprompt\nor not, and different numbers of break lines can be\nput in the middle, etc.\n3 SELF-INSTRUCT Data from GPT3\nInthissection,weapplyourmethodforinducing\ninstruction data to GPT3as a case study. We use\nthelargestGPT3LM(“davinci”engine)accessed\nthroughtheOpenAIAPI.6Theparametersformak-\ning queries are described in Appendix A.2. Here\nwe present an overview of the generated data.\n3.1 Statistics\nTable 1 describes the basic statistics of the gener-\nated data. We generate a total of over 52K instruc-\ntionsandmorethan82Kinstancescorresponding\nto these instructions after filtering.\nstatistic\n# of instructions 52,445' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 3}
page_content='we present an overview of the generated data.\n3.1 Statistics\nTable 1 describes the basic statistics of the gener-\nated data. We generate a total of over 52K instruc-\ntionsandmorethan82Kinstancescorresponding\nto these instructions after filtering.\nstatistic\n# of instructions 52,445\n- # of classification instructions 11,584\n- # of non-classification instructions 40,861\n# of instances 82,439\n- # of instances with empty input 35,878\nave. instruction length (in words) 15.9\nave. non-empty input length (in words) 12.7\nave. output length (in words) 18.9\nTable 1: Statistics of the generated data by applying\nSELF-INSTRUCT to GPT3.\n6https://openai.com/api/3.2 Diversity\nTostudy what typesof instructions are generated\nandhowdiversetheyare,weidentifytheverb-noun\nstructure in the generated instructions. We use the\nBerkeleyNeuralParser7(KitaevandKlein,2018;\nKitaev et al., 2019) to parse the instructions and\nthen extract the verb that is closest to the root as' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 3}
page_content='Tostudy what typesof instructions are generated\nandhowdiversetheyare,weidentifytheverb-noun\nstructure in the generated instructions. We use the\nBerkeleyNeuralParser7(KitaevandKlein,2018;\nKitaev et al., 2019) to parse the instructions and\nthen extract the verb that is closest to the root as\nwellasitsfirstdirectnounobject. 26,559outofthe\n52,445 instructions contain such structure; other\ninstructions usually contain more complex clauses\n(e.g.,“Classifywhetherthistweetcontainspolitical\ncontent or not.”) or are framed as questions (e.g.,\n“Which of these statements are true?”). We plot\nthe top 20 most common root verbs and their top 4\ndirectnounobjectsinFigure 3,whichaccountfor\n14% of the entire set. Overall, we see quite diverse\nintents and textual formats in these instructions.\nWe further study how the generated instructions\ndiffer from the seed instructions used to prompt\nthe generation. For eachgenerated instruction, we\ncompute its highest ROUGE-L overlap with the' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 3}
page_content='14% of the entire set. Overall, we see quite diverse\nintents and textual formats in these instructions.\nWe further study how the generated instructions\ndiffer from the seed instructions used to prompt\nthe generation. For eachgenerated instruction, we\ncompute its highest ROUGE-L overlap with the\n175 seed instructions. We plot the distribution of\nthese ROUGE-L scores in Figure 4. The results\nindicate a decent number of new instructions were\ngenerated,whichdonothavemuchoverlapwiththe\nseeds. We also demonstrate diversity in the length\nof the instructions, instance inputs, and instance\noutputs in Figure 5.\n3.3 Quality\nSo far, we have shown the quantity and diversity\nof the generated data, but its quality remains un-\ncertain. To investigate this, we randomly sample\n200instructionsandrandomlyselect1instanceper\ninstruction. Weaskedanexpertannotator(author\nof this work) to label whether each instance is cor-\nrect or not, in terms of the instruction, the instance' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 3}
page_content='of the generated data, but its quality remains un-\ncertain. To investigate this, we randomly sample\n200instructionsandrandomlyselect1instanceper\ninstruction. Weaskedanexpertannotator(author\nof this work) to label whether each instance is cor-\nrect or not, in terms of the instruction, the instance\ninput,andtheinstanceoutput. Evaluationresultsin\nTable2showthatmostofthegeneratedinstructions\nare meaningful, while the generated instances may\ncontainmorenoise(toareasonableextent). How-\never, we found that even though the generations\nmay contain errors, most of them are still in the\ncorrectformatorpartiallycorrect,whichcanpro-\nvide usefulguidance for trainingmodels to follow\ninstructions. We listed a number of good examples\nand bad examples in Table 10 and 11, respectively.\n7https://parser.kitaev.io/' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 3}
page_content='write\ngive\nfind\ncreatemakedescri bedesi gngener at ec l a s s i f yh a v eexplaint e l li d e n t i f youtputp r e d i c td e t e c t\nfunction\nessay\nletter\nparagraph\nexample\nlist\nset\na d v i c e\nword\nnumber\ns e nt e nc e\nway\nprogram\nlist\nal gori t hm\nfunct i on\nlist\nst ory\nsent ence\nprogram\nsituation\nperson\nprocess\nt i me\nsyst em\ngame\nal gor i t hm\nst r uct ur e\nl i s t\nnumber\ns ent ence\ns e r i e s\ns e nt e nc e\ns e n t i me n t\na r t i c l e\nt e x t\nl i s t\narray\nc o i n\ns e t\nd i f f e r e n c e\nc o n c e p t\ns t o r y\nj o k e\ns e n t i m e n t\nt o p i c\nn u m b e r\nw o r d\ns e n t i m e n t\ns a r c a s mFigure3: Thetop20mostcommonrootverbs(innercircle)and\ntheir top 4 direct noun objects (outer circle) in the generated\ninstructions. Despitetheirdiversity,theinstructionsshownhere\nonlyaccountfor14%ofallthegeneratedinstructionsbecause\nmanyinstructions(e.g.,“Classifywhethertheuserissatisfied\nwith the service.”) do not contain such a verb-noun structure.\n0 0.2 0.4 0.6 0.8 10100020003000' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 4}
page_content='instructions. Despitetheirdiversity,theinstructionsshownhere\nonlyaccountfor14%ofallthegeneratedinstructionsbecause\nmanyinstructions(e.g.,“Classifywhethertheuserissatisfied\nwith the service.”) do not contain such a verb-noun structure.\n0 0.2 0.4 0.6 0.8 10100020003000\nROUGE-L Overlap with the Most Similar Seed Instruction# InstructionsFigure4: DistributionoftheROUGE-Lscores\nbetween generated instructions and their most\nsimilar seed instructions.\n10 20 30 40 50 600200040006000\nInstruction Length# Instructions\n10 20 30 40 50 600100020003000\nInput Length# Inputs\n10 20 30 40 50 60010k20k30k\nOnput Length# Onputs\nFigure 5: Length distribution of the generated\ninstructions, non-empty inputs, and outputs.\nQuality Review Question Yes %\nDoes the instruction\ndescribe a valid task?92%\nIs the input appropriate\nfor the instruction?79%\nIs the output a correct and acceptable\nresponse to the instruction and input?58%\nAll fields are valid 54%\nTable2: Dataqualityreviewfortheinstruction,input,' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 4}
page_content='Quality Review Question Yes %\nDoes the instruction\ndescribe a valid task?92%\nIs the input appropriate\nfor the instruction?79%\nIs the output a correct and acceptable\nresponse to the instruction and input?58%\nAll fields are valid 54%\nTable2: Dataqualityreviewfortheinstruction,input,\nand output of the generated data. See Table 10 and\nTable 11 for representative valid and invalid examples.\n4 Experimental Results\nWeconductexperimentstomeasureandcompare\ntheperformanceofmodelsundervariousinstruc-\ntion tuning setups. We first describe our models\nand other baselines, followed by our experiments.\n4.1 GPT3SELF-INST: finetuning GPT3 on its\nown instruction data\nGiventheinstruction-generatedinstructiondata,we\nconduct instruction tuning with the GPT3model\nitself (“davinci” engine). As described in §2.3, we\nusevarioustemplatestoconcatenatetheinstructionandinput,andtrainthemodeltogeneratetheoutput.\nThis finetuning is done through the OpenAI fine-\ntuningAPI.8Weusethedefaulthyper-parameters,' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 4}
page_content='conduct instruction tuning with the GPT3model\nitself (“davinci” engine). As described in §2.3, we\nusevarioustemplatestoconcatenatetheinstructionandinput,andtrainthemodeltogeneratetheoutput.\nThis finetuning is done through the OpenAI fine-\ntuningAPI.8Weusethedefaulthyper-parameters,\nexcept that we set the prompt loss weight to 0, and\nwetrainthemodelfor2epochs. Wereferthereader\nto Appendix A.3 for additional finetuning details.\nThe resulting model is denoted by GPT3SELF-INST.\n4.2 Baselines\nOff-the-shelf LMs. We evaluate T5-LM (Lester\net al., 2021; Raffel et al., 2020) and GPT3(Brown\netal.,2020)asthevanillaLMbaselines(onlypre-\ntraining, no additional finetuning). These baselines\nwill indicate the extent to which off-the-shelf LMs\narecapableoffollowinginstructionsnaturallyim-\nmediately after pretraining.\nPublicly available instruction-tuned models.\nT0andT𝑘-INSTRUCT are two instruction-tuned\nmodels proposed in Sanh et al. (2022) and Wang\net al. (2022), respectively, and are demonstrated' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 4}
page_content='arecapableoffollowinginstructionsnaturallyim-\nmediately after pretraining.\nPublicly available instruction-tuned models.\nT0andT𝑘-INSTRUCT are two instruction-tuned\nmodels proposed in Sanh et al. (2022) and Wang\net al. (2022), respectively, and are demonstrated\nto be able to follow instructions for many NLP\ntasks. Both of these models are finetuned from\ntheT5(Raffeletal.,2020)checkpointsandarepub-\nliclyavailable.9Forbothofthesemodels,weuse\n8See OpenAI’s documentation on finetuning.\n9T0is available at here and T 𝑘-INSTRUCT is here.' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 4}
page_content='their largest version with 11B parameters.\nInstruction-tunedGPT3models. Weevaluate\nInstructGPT (Ouyang et al., 2022), which is devel-\nopedbyOpenAIbasedonGPT3tofollowhuman\ninstructionsbetterandhasbeenfoundbythecom-\nmunitytohaveimpressivezero-shotabilities. There\nare various generations of these models, where\nneweronesusemoreexpansivedataoralgorithmic\nnovelties.10For ourSUPERNIexperiments in §4.3,\nwe only compare with their text-davinci-001\nengine,becausetheirnewerenginesaretrainedwith\nthe latest user data and are likely to have already\nseentheSUPERNItestset. Forourhumanevalua-\ntion on newly written instructions, we include their\n001, 002 and 003 engines for completeness.\nAdditionally, to compare SELF-INSTRUCT train-\ning with other publicly available instruction tuning\ndata, we further finetune GPT3 model with data\nfromPROMPTSOURCEandSUPERNI, which are\nusedtotrainthe T0andT𝑘-INSTRUCT models. We\ncall them T0training and SUPERNItraining for' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 5}
page_content='Additionally, to compare SELF-INSTRUCT train-\ning with other publicly available instruction tuning\ndata, we further finetune GPT3 model with data\nfromPROMPTSOURCEandSUPERNI, which are\nusedtotrainthe T0andT𝑘-INSTRUCT models. We\ncall them T0training and SUPERNItraining for\nshort, respectively. To save the training budget, we\nsampled 50K instances (but covering all their in-\nstructions)foreachdataset,whichhasacomparable\nsize to the instruction data we generated. Based on\nthe findings from Wang et al. (2022) and our early\nexperiments, reducing the number of instances per\ntraining task does not degrade the model’s general-\nization performance to unseen tasks.\n4.3 Experiment 1: Zero-Shot Generalization\non SUPERNI benchmark\nWe first evaluate the models’ ability to follow in-\nstructions on typical NLP tasks in a zero-shot fash-\nion. We use the evaluation set of SUPERNI(Wang\netal.,2022),whichconsistsof119taskswith100in-\nstances in each task. In this work, we mainly focus' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 5}
page_content='on SUPERNI benchmark\nWe first evaluate the models’ ability to follow in-\nstructions on typical NLP tasks in a zero-shot fash-\nion. We use the evaluation set of SUPERNI(Wang\netal.,2022),whichconsistsof119taskswith100in-\nstances in each task. In this work, we mainly focus\nonthezero-shotsetup,i.e.,themodelisprompted\nwith the definition of the tasks only, without in-\ncontext demonstration examples. For all our re-\nqueststothe GPT3variants,weusethedetermin-\nistic generation mode (temperature as 0 and no nu-\ncleus sampling) without specific stop sequences.\nResults. We make the following observations\nfromtheresultsinTable3. SELF-INSTRUCT boosts\ntheinstruction-followingabilityof GPT3byalarge\nmargin. The vanilla GPT3model basically can-\nnot follow human instructions at all. Upon manual\nanalysis, we find that it usually generates irrele-\n10See OpenAI’s documentation on their models.Model # Params ROUGE-L\nVanilla LMs\nT5-LM 11B 25.7\nGPT3 175B 6.8\nInstruction-tuned w/o S UPERNI\nT0 11B 33.1' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 5}
page_content='margin. The vanilla GPT3model basically can-\nnot follow human instructions at all. Upon manual\nanalysis, we find that it usually generates irrele-\n10See OpenAI’s documentation on their models.Model # Params ROUGE-L\nVanilla LMs\nT5-LM 11B 25.7\nGPT3 175B 6.8\nInstruction-tuned w/o S UPERNI\nT0 11B 33.1\nGPT3 + T 0Training 175B 37.9\nGPT3SELF-INST(Ours) 175B 39.9\nInstructGPT001 175B 40.8\nInstruction-tuned w/ S UPERNI\nT𝑘-INSTRUCT 11B 46.0\nGPT3 + S UPERNI Training 175B 49.5\nGPT3SELF-INST+ SUPERNI Training (Ours) 175B 51.61⃝\n2⃝\n3⃝\nTable 3: Evaluation results on unseentasks from SU-\nPERNI(§4.3). From the results, we see that 1⃝SELF-\nINSTRUCT can boost GPT3performance by a large mar-\ngin(+33.1%)and 2⃝nearlymatchestheperformanceof\nInstructGPT001. Additionally, 3⃝it can further improve\nthe performance even when a large amount of labeled\ninstruction data is present.\nvant and repetitive text, and does not know when\nto stop generation. Compared with other mod-' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 5}
page_content='gin(+33.1%)and 2⃝nearlymatchestheperformanceof\nInstructGPT001. Additionally, 3⃝it can further improve\nthe performance even when a large amount of labeled\ninstruction data is present.\nvant and repetitive text, and does not know when\nto stop generation. Compared with other mod-\nels that are not specifically trained for SUPERNI,\nGPT3SELF-INSTachievesbetterperformancethan T0\northeGPT3finetunedonthe T0trainingset,which\ntakes tremendous human labeling efforts. Notably,\nGPT3SELF-INSTalsonearlymatchestheperformance\nofInstructGPT001, which is trained with private\nuser data and human-annotated labels.\nModels trained on the SUPERNItraining set still\nachieve better performance on its evaluation set,\nwhich we attribute to the similar instruction style\nand formatting. However, we show that SELF-\nINSTRUCT stillbringsinadditionalgainswhencom-\nbined with the SUPERNItraining set, proving its\nvalue as complementary data.\n4.4 Experiment 2: Generalization to\nUser-oriented Instructions on Novel Tasks' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 5}
page_content='which we attribute to the similar instruction style\nand formatting. However, we show that SELF-\nINSTRUCT stillbringsinadditionalgainswhencom-\nbined with the SUPERNItraining set, proving its\nvalue as complementary data.\n4.4 Experiment 2: Generalization to\nUser-oriented Instructions on Novel Tasks\nDespitethecomprehensivenessof SUPERNIin col-\nlectingexistingNLPtasks,mostoftheseNLPtasks\nwere proposed for research purposes and skewed\ntoward classification. To better access the practi-\ncal value of instruction-following models, a sub-\nset of the authors curate a new set of instructions\nmotivated by user-oriented applications. We first\nbrainstorm various domains where large LMs may\nbe useful (e.g., email writing, social media, pro-\nductivity tools, entertainment, programming), then\ncraftinstructionsrelatedtoeachdomainalongwith\naninput-outputinstance(again,inputisoptional).\nWe aim to diversify the styles and formats of these\ntasks (e.g., instructions may be long or short; in-' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 5}
page_content='VanillaGPT3GPT3+T0TrainingGPT3+SuperNITrainingGPT3Self-Inst+SuperNIGPT3Self-InstInstructGPT001InstructGPT002InstructGPT0031871186831251810264598084666134281313054494540300447483112128168192\n0%25%50%75%100%\nGPT3T0 TrainingSuperNI TrainingSelf-Instruct + SuperNISelf-InstructInstructGPT-001InstructGPT-002InstructGPT-003correct and satisfying responseacceptable response with minor imperfectionsresponds to the instruction but has significant errors  irrelevant or invalid response1861176831251810264598084666134281313054484439300447483113129169192\n0%25%50%75%100%\nGPT3T0 TrainingSuperNI TrainingSelf-Instruct + SuperNISelf-InstructInstructGPT-001InstructGPT-002InstructGPT-003correct and satisfying responseacceptable response with minor imperfectionsresponds to the instruction but has significant errors  irrelevant or invalid responseC:respondstotheinstructionbuthassignificanterrorsA:correctandsatisfyingresponse\n1861176831251810264598084666134281313054484439300447483113129169192\n0%25%50%75%100%' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 6}
page_content='1861176831251810264598084666134281313054484439300447483113129169192\n0%25%50%75%100%\nGPT3T0 TrainingSuperNI TrainingSelf-Instruct + SuperNISelf-InstructInstructGPT-001InstructGPT-002InstructGPT-003correct and satisfying responseacceptable response with minor imperfectionsresponds to the instruction but has significant errors  irrelevant or invalid responseB:acceptableresponsewithminorimperfections1861176831251810264598084666134281313054484439300447483113129169192\n0%25%50%75%100%\nGPT3T0 TrainingSuperNI TrainingSelf-Instruct + SuperNISelf-InstructInstructGPT-001InstructGPT-002InstructGPT-003correct and satisfying responseacceptable response with minor imperfectionsresponds to the instruction but has significant errors  irrelevant or invalid responseD:irrelevantorinvalidresponse1861176831251810264598084666134281313054484439300447483113129169192\n0%25%50%75%100%' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 6}
page_content='0%25%50%75%100%\nGPT3T0 TrainingSuperNI TrainingSelf-Instruct + SuperNISelf-InstructInstructGPT-001InstructGPT-002InstructGPT-003correct and satisfying responseacceptable response with minor imperfectionsresponds to the instruction but has significant errors  irrelevant or invalid response1861176831251810264598084666134281313054494540300447483112128168192\n0%25%50%75%100%\nGPT3T0 TrainingSuperNI TrainingSelf-Instruct + SuperNISelf-InstructInstructGPT-001InstructGPT-002InstructGPT-003correct and satisfying responseacceptable response with minor imperfectionsresponds to the instruction but has significant errors  irrelevant or invalid responseFigure6: PerformanceofGPT3modelanditsinstruction-tunedvariants,evaluatedbyhumanexpertsonour252\nuser-orientedinstructions(§4.4). Humanevaluatorsareinstructedtoratethemodels’responsesintofourlevels. The\nresults indicate that GPT3SELF-INSToutperforms all the other GPT3variants trained on publicly available instruction' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 6}
page_content='user-orientedinstructions(§4.4). Humanevaluatorsareinstructedtoratethemodels’responsesintofourlevels. The\nresults indicate that GPT3SELF-INSToutperforms all the other GPT3variants trained on publicly available instruction\ndatasets. Additionally, GPT3SELF-INSTscores nearly as good as InstructGPT001(cf. footnote 1).\nput/output may take the form of bullet points, ta-\nbles,codes,equations,etc.). Intotal,wecreate252\ninstructions with 1 instance per instruction. We\nbelieve it can serve as a testbed for evaluating how\ninstruction-basedmodelshandlediverseandunfa-\nmiliarinstructions. Table9presentsasmallportion\nofthem. TheentiresetisavailableinourGitHub\nrepository. Weanalyzetheoverlapbetweenthisset\nset and the seed instructions in §A.1.\nHuman evaluation setup. Evaluating models’\nperformance on this evaluation set of diverse tasks\nis extremely challenging because different tasks re-\nquire different expertise. Indeed, many of these\ntaskscannotbemeasuredbyautomaticmetricsor' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 6}
page_content='set and the seed instructions in §A.1.\nHuman evaluation setup. Evaluating models’\nperformance on this evaluation set of diverse tasks\nis extremely challenging because different tasks re-\nquire different expertise. Indeed, many of these\ntaskscannotbemeasuredbyautomaticmetricsor\nevenbejudgedbynormalcrowdworkers(e.g.,writ-\ningaprogram,orconvertingfirst-orderlogicinto\nnaturallanguage). Togetamorefaithfulevaluation,\nwe asked the authors of the instructions to judge\nmodel predictions. Details on how we set up this\nhumanevaluationaredescribedinAppendixB.The\nevaluators were asked to rate the output based on\nwhether it accurately and effectively completes the\ntask. We implemented a four-level rating system\nfor categorizing the quality of the models’ outputs:\n•RATING-A:The response is valid and satisfying.\n•RATING-B:Theresponseisacceptablebuthas\nminor errors or imperfections.\n•RATING-C:The response is relevant and re-\nsponds to the instruction, but it has significant' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 6}
page_content='for categorizing the quality of the models’ outputs:\n•RATING-A:The response is valid and satisfying.\n•RATING-B:Theresponseisacceptablebuthas\nminor errors or imperfections.\n•RATING-C:The response is relevant and re-\nsponds to the instruction, but it has significant\nerrors in the content. For example, GPT3 might\ngenerate a valid output first, but continue to gen-erate other irrelevant things.\n•RATING-D:The response is irrelevant or com-\npletely invalid.\nResults. Figure 6 shows the performance of\nGPT3modelanditsinstruction-tunedcounterparts\non this newly written instruction set (w. inter-rater\nagreement𝜅= 0.57onthe4-classcategoricalscale,\nsee Appendix B for details). As anticipated, the\nvanillaGPT3LMislargelyunabletorespondtoin-\nstructions,andallinstruction-tunedmodelsdemon-\nstrate comparatively higher performance. Nonethe-\nless,GPT3SELF-INST(i.e.,GPT3model finetuned\nwithSELF-INSTRUCT )outperformsthosecounter-\npartstrainedon T0orSUPERNIdatabyalargemar-' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 6}
page_content='vanillaGPT3LMislargelyunabletorespondtoin-\nstructions,andallinstruction-tunedmodelsdemon-\nstrate comparatively higher performance. Nonethe-\nless,GPT3SELF-INST(i.e.,GPT3model finetuned\nwithSELF-INSTRUCT )outperformsthosecounter-\npartstrainedon T0orSUPERNIdatabyalargemar-\ngin,demonstratingthevalueofthegenerateddata\ndespitethenoise. Comparedwith InstructGPT001,\nGPT3SELF-INSTis quite close in performance—if\nwe count acceptable response with minor imper-\nfections ( RATING-B) as valid, GPT3SELF-INSTis\nonly5%behind InstructGPT001. Lastly,ourevalua-\ntionconfirmstheimpressiveinstruction-following\nability of InstructGPT002andInstructGPT003. Al-\nthough thereare many factorsbehind this success,\nweconjecturethatfutureworkcanlargelybenefit\nfromimprovingthequalityofourgenerateddataby\nusing human annotators or training a reward model\ntoselectbettergenerations,similartothealgorithm\nused by Ouyang et al. (2022).\n4.5 Effect of Data Size and Quality\nData size. SELF-INSTRUCT provides a way to' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 6}
page_content='weconjecturethatfutureworkcanlargelybenefit\nfromimprovingthequalityofourgenerateddataby\nusing human annotators or training a reward model\ntoselectbettergenerations,similartothealgorithm\nused by Ouyang et al. (2022).\n4.5 Effect of Data Size and Quality\nData size. SELF-INSTRUCT provides a way to\ngrow instruction data at a low cost with almost' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 6}
page_content='no human labeling; could more of this generated\ndataleadtobetterinstruction-followingability? We\nconduct an analysis of the size of generated data\nbysubsamplingdifferentnumbersofinstructions\nfromthegenerateddataset,finetuning GPT3onthe\nsampledsubsets,andevaluatinghowtheresulting\nmodels perform on the 252 user-oriented instruc-\ntion set. We conduct the same human evaluation\nas in §4.4. Figure 7 presents the performance of\nGPT3SELF-INSTmodelsfinetunedwithdifferentsizes\nof generated data. Overall, we see consistent im-\nprovement as we grow the data size. However, this\nimprovement almost plateaus after 16K. This is in-\nlinewiththedatascalingexperimentsinWangetal.\n(2022, Fig. 5). Interestingly, when evaluating on\nSUPERNIwe found the model’s performance gain\nplateausearlier ataround hundredsof instructions.\nThis may be due to the fact that the new generated\ndataisdistinctfromtypicalNLPtasksin SUPERNI,\nindicating that future research may benefit from us-' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 7}
page_content='(2022, Fig. 5). Interestingly, when evaluating on\nSUPERNIwe found the model’s performance gain\nplateausearlier ataround hundredsof instructions.\nThis may be due to the fact that the new generated\ndataisdistinctfromtypicalNLPtasksin SUPERNI,\nindicating that future research may benefit from us-\ningacombinationofdifferentinstructiondatafor\nbetter performance on various types of tasks.\nData quality. Another direction to improve the\nmodel’s performance is to take our generated data\nand get better supervision (with less noise). We\nexplore this idea by using InstructGPT003(the best\navailable general-purpose model) to regenerate the\noutputfieldofallourinstancesgiventheinstruction\nand input. We then use this improved version of\nourdatatofinetune GPT3. Thiscanberegardedas\nadistillationof InstructGPT003withourdata. Asis\nshowninFigure7,theresultingmodeloutperforms\nthe counterpart trained with the original data by\n10%,whichsuggestsbigroomforfutureworkon' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 7}
page_content='and input. We then use this improved version of\nourdatatofinetune GPT3. Thiscanberegardedas\nadistillationof InstructGPT003withourdata. Asis\nshowninFigure7,theresultingmodeloutperforms\nthe counterpart trained with the original data by\n10%,whichsuggestsbigroomforfutureworkon\nusing our generation pipeline to get initial data and\nthenimprovingthedataqualitywithhumanexperts\nor distillation from better models.\n5 Related Work\nInstruction-following LMs. A series of works\nhave found evidence that vanilla LMs can be effec-\ntive at following general language instructions if\ntunedwithannotated“instructional”data—datasets\ncontaininglanguageinstructionalcommandsand\ntheir desired outcomes based on human annota-\ntion(Welleretal.,2020;Mishraetal.,2022;Wei\net al., 2022; Sanh et al., 2022, i.a.). Additionally,\nthey show a direct correlation between the size and\ndiversityofthe“instructional”dataandthegeneral-\nizabilityofresultingmodelstounseentasks(Wang\net al., 2022; Chung et al., 2022). However, since' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 7}
page_content='tion(Welleretal.,2020;Mishraetal.,2022;Wei\net al., 2022; Sanh et al., 2022, i.a.). Additionally,\nthey show a direct correlation between the size and\ndiversityofthe“instructional”dataandthegeneral-\nizabilityofresultingmodelstounseentasks(Wang\net al., 2022; Chung et al., 2022). However, since\n31.0%36.9%43.7%44.4%54.4%\n20%30%40%50%60%\n100800640051200PercentageofResponsesRatedasA\nNumberofInstructions  w. GPT3 Self-Instruct data  w. improved output from InstructGPTFigure 7: Human evaluation performance of\nGPT3SELF-INSTmodels tuned with different sizes of\ninstructions. 𝑥-axis is in log scale. The smallest\nsize is 175, where only the seed tasks are used for\ninstruction tuning. We also evaluate whether improving\nthe data quality will further improve the performance\nby distilling the outputs from InstructGPT003. We see\nconsistent improvement from using larger data with\nbetter quality.\nthese developments largely focus on existing NLP\ntasks and depend on human-annotated instructions,' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 7}
page_content='the data quality will further improve the performance\nby distilling the outputs from InstructGPT003. We see\nconsistent improvement from using larger data with\nbetter quality.\nthese developments largely focus on existing NLP\ntasks and depend on human-annotated instructions,\nthis poses a bottleneck for progress toward more\ngeneralizable models (e.g., see Fig. 5a in Wang\netal.,2022). Ourworkaimstomovebeyondclassi-\ncal NLP tasks and tackle the challenges of creating\ndiverse instruction data by employing pretrained\nLMs.InstructGPT (Ouyang et al., 2022) shares\na similar goal as ours in building more general-\npurpose LMs, and has demonstrated remarkable\nperformance in following diverse user instructions.\nHowever,asacommercialsystem,theirconstruc-\ntion process still remains quite opaque. In partic-\nular, the role of datahas remained understudied\ndue to limited transparency and the private user\ndatatheyusedintheirstudy. Addressingsuchchal-\nlenges necessitates the creation of a large-scale,' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 7}
page_content='However,asacommercialsystem,theirconstruc-\ntion process still remains quite opaque. In partic-\nular, the role of datahas remained understudied\ndue to limited transparency and the private user\ndatatheyusedintheirstudy. Addressingsuchchal-\nlenges necessitates the creation of a large-scale,\npublic dataset covering a broad range of tasks.\nLanguagemodelsfordatagenerationandaug-\nmentation. A variety of works have proposed\nusingLMsfordatageneration(SchickandSchütze,\n2021; Wang et al., 2021; Liu et al., 2022; Meng\net al., 2023) or augmentation (Feng et al., 2021;\nYangetal.,2020;Mekalaetal.,2022). Ourwork\ndiffers from this line in that it is notspecific to a\nparticular task (say, QA or NLI). In contrast, a dis-\ntinctmotivationfor SELF-INSTRUCT istobootstrap\nnewtaskdefinitionsthatmaynothavebeendefined' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 7}
page_content='beforebyNLPpractitioners(thoughpotentiallystill\nimportant for real users). In parallel with our work,\nHonovich et al. (2022a) also propose to generate\nlarge-scale instruction data (so-called Unnatural\nInstructions) with GPT3 models. The major differ-\nences are that 1) they use tasks in SUPERNI(Wang\netal.,2022)astheirseedtasks,resultinginadiffer-\nent distribution of generated tasks; 2) they employ\nInstructGPT002for generating the data, in which\nsensetheyaredistillingknowledgefromanalready\ninstruction-tuned model, while we solely rely on\nthe vanilla LM; 3) the detailed generation pipeline\nand templates are different. Nevertheless, we be-\nlieve that both efforts in expanding instruction data\narecomplementary,andthecommunitywillbenefit\nfrom these diverse datasets.\nInstruction generation. A series of recent\nworks (Zhou et al., 2022b; Ye et al., 2022; Singh\net al., 2022; Honovich et al., 2022b) generate in-\nstructions of a task given a few examples. While' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 8}
page_content='arecomplementary,andthecommunitywillbenefit\nfrom these diverse datasets.\nInstruction generation. A series of recent\nworks (Zhou et al., 2022b; Ye et al., 2022; Singh\net al., 2022; Honovich et al., 2022b) generate in-\nstructions of a task given a few examples. While\nSELF-INSTRUCT also involves instruction genera-\ntion, a major difference in our case is it is task-\nagnostic;wegeneratenewtasks(instructionsalong\nwith instances) from scratch.\nModel self-training. A typical self-training\nframework (He et al., 2019; Xie et al., 2020; Du\net al., 2021; Amini et al., 2022; Huang et al., 2022)\nuses trained models to assign labels to unlabeled\ndata and then leverages the newly labeled data to\nimprove the model. In a similar line, Zhou et al.\n(2022a) use multiple prompts to specify a single\ntask and propose to regularize via prompt consis-\ntency, encouraging consistentpredictions over the\nprompts. This allows either finetuning the model\nwithextraunlabeledtrainingdata,ordirectapplica-' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 8}
page_content='improve the model. In a similar line, Zhou et al.\n(2022a) use multiple prompts to specify a single\ntask and propose to regularize via prompt consis-\ntency, encouraging consistentpredictions over the\nprompts. This allows either finetuning the model\nwithextraunlabeledtrainingdata,ordirectapplica-\ntionat inferencetime. While SELF-INSTRUCT has\nsimilarities with the self-training literature, most\nself-training methods assume a specific target task\nas well as unlabeled examples under it; in contrast,\nSELF-INSTRUCT producesavarietyoftasksfrom\nscratch.\nKnowledge distillation. Knowledge distilla-\ntion (Hinton et al., 2015; Sanh et al., 2019; West\net al., 2021; Magister et al., 2022) often involves\nthe transfer of knowledge from larger models to\nsmaller ones. SELF-INSTRUCT can also be viewed\nasaformof“knowledgedistillation",however, it\ndiffers from this line in the following ways: (1)\nthe source and target of distillation are the same,' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 8}
page_content='et al., 2021; Magister et al., 2022) often involves\nthe transfer of knowledge from larger models to\nsmaller ones. SELF-INSTRUCT can also be viewed\nasaformof“knowledgedistillation",however, it\ndiffers from this line in the following ways: (1)\nthe source and target of distillation are the same,\ni.e., a model’s knowledge is distilled to itself; (2)the content of distillation is in the form of an\ninstruction task (i.e., instructions that define a task,\nand a set of examples that instantiate it).\nBootstrapping with limited resources. A se-\nriesofrecentworksuselanguagemodelstoboot-\nstrap some inferences using specialized methods.\nNPPrompt (Zhao et al., 2022) provides a method\nto generate predictions for semantic labels without\nanyfinetuning. Itusesamodel’sownembeddings\nto automatically find words relevant to the label of\nthe data sample and hence reduces the dependency\non manual mapping from model prediction to la-\nbel (verbalizers). STAR (Zelikman et al., 2022)' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 8}
page_content='to generate predictions for semantic labels without\nanyfinetuning. Itusesamodel’sownembeddings\nto automatically find words relevant to the label of\nthe data sample and hence reduces the dependency\non manual mapping from model prediction to la-\nbel (verbalizers). STAR (Zelikman et al., 2022)\niteratively leverages a small number of rationale\nexamples and a large dataset without rationales, to\nbootstrap a model’s ability to perform reasoning.\nSelf-Correction(Wellecketal.,2023)decouplesan\nimperfectbasegenerator(model)fromaseparate\ncorrectorthatlearnstoiterativelycorrectimperfect\ngenerationsanddemonstratesimprovementoverthe\nbasegenerator. Ourworkinsteadfocusesonboot-\nstrapping new tasks in the instruction paradigm.\nMulti-modal instruction-following. Instruction-\nfollowingmodelshavealsobeenofinterestinthe\nmulti-modallearningliterature(Friedetal.,2018;\nShridharetal.,2020;Minetal.,2022;Weiretal.,\n2022).SELF-INSTRUCT , as a general approach to\nexpandingdata,canpotentiallyalsobehelpfulin' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 8}
page_content='Multi-modal instruction-following. Instruction-\nfollowingmodelshavealsobeenofinterestinthe\nmulti-modallearningliterature(Friedetal.,2018;\nShridharetal.,2020;Minetal.,2022;Weiretal.,\n2022).SELF-INSTRUCT , as a general approach to\nexpandingdata,canpotentiallyalsobehelpfulin\nthose settings, which we leave to future work.\n6 Conclusion\nWe introduce SELF-INSTRUCT , a method to im-\nprovetheinstruction-followingabilityofLMsvia\ntheir own generation of instruction data. On ex-\nperimentingwithvanilla GPT3,weautomatically\nconstructalarge-scaledatasetof52Kinstructions\nfor diverse tasks, and finetuning GPT3 on this data\nleads to a 33% absolute improvement on SUPERNI\nover the original GPT3. Furthermore, we curate\nasetofexpert-writteninstructionsfornoveltasks.\nHuman evaluation on this set shows that tuning\nGPT3 with SELF-INSTRUCT outperforms using ex-\nisting public instruction datasets by a large margin\nandperformscloselyto InstructGPT001. Wehope\nSELF-INSTRUCT can serve as the first step to align' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 8}
page_content='asetofexpert-writteninstructionsfornoveltasks.\nHuman evaluation on this set shows that tuning\nGPT3 with SELF-INSTRUCT outperforms using ex-\nisting public instruction datasets by a large margin\nandperformscloselyto InstructGPT001. Wehope\nSELF-INSTRUCT can serve as the first step to align\npretrainedLMstofollowhumaninstructions,and\nfutureworkcanbuildontopofthisdatatoimprove\ninstruction-following models.' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 8}
page_content='7 Broader Impact\nBeyond the immediate focus of this paper, we\nbelieve that SELF-INSTRUCT may help bring\nmore transparency to what happens “behind the\nscenes” of widely-used instruction-tuned models\nlikeInstructGPT or ChatGPT. Unfortunately, such\nindustrial models remain behind API walls as their\ndatasets are not released, and hence there is lit-\ntle understanding of their construction and why\nthey demonstrate impressive capabilities. The bur-\nden now falls on academia to better understand the\nsource of success in these models and strive for\nbetter—andmoreopen—models. Webelieveour\nfindingsinthispaperdemonstratetheimportance\nof diverse instruction data, and our large synthetic\ndataset can be the first step toward higher-quality\ndata for building better instruction-following mod-\nels. At this writing, the central idea of this paper\nhas been adopted in several follow-up works for\nsuch endeavors (Taori et al., 2023; Xu et al., 2023;\nSun et al., 2023, i.a.).\n8 Limitations' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 9}
page_content='dataset can be the first step toward higher-quality\ndata for building better instruction-following mod-\nels. At this writing, the central idea of this paper\nhas been adopted in several follow-up works for\nsuch endeavors (Taori et al., 2023; Xu et al., 2023;\nSun et al., 2023, i.a.).\n8 Limitations\nHere,wediscusssomelimitationsofthisworkto\ninspire future research in this direction.\nTail phenomena. SELF-INSTRUCT depends on\nLMs, and it will inherit all the limitations that\ncarry over with LMs. As recent studies have\nshown (Razeghi et al., 2022; Kandpal et al., 2022),\ntail phenomena pose a serious challenge to the suc-\ncess of LMs. In other words, LMs’ largest gains\ncorrespond to the frequent uses of languages (head\nofthelanguageusedistribution),andtheremight\nbe minimal gains in the low-frequency contexts.\nSimilarly,inthecontextofthiswork,itwouldnot\nbesurprisingifthemajorityofthegainsby SELF-\nINSTRUCT areskewedtowardtasks orinstructions\nthat present more frequently in the pretraining cor-' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 9}
page_content='ofthelanguageusedistribution),andtheremight\nbe minimal gains in the low-frequency contexts.\nSimilarly,inthecontextofthiswork,itwouldnot\nbesurprisingifthemajorityofthegainsby SELF-\nINSTRUCT areskewedtowardtasks orinstructions\nthat present more frequently in the pretraining cor-\npus. Asaconsequence,theapproachmightshow\nbrittleness with respect to uncommon and creative\ninstructions.\nDependence on large models. Because of SELF-\nINSTRUCT ’s dependence on the inductive biases\nextractedfromLMs,itmightworkbestforlarger\nmodels. Iftrue,thismaycreatebarrierstoaccessfor\nthosewhomaynothavelargecomputingresources.\nWehopefuturestudieswillcarefullystudythegains\nasafunctionofmodelsizeorvariousotherparame-\nters. Itisworthwhiletonotethatinstruction-tuning\nwithhumanannotationalsosuffersfromasimilarlimitation: gains of instruction-tuning are higher\nfor larger models (Wei et al., 2022).\nReinforcingLMbiases. Apointofconcernfor\nthe authors is the unintended consequences of this' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 9}
page_content='ters. Itisworthwhiletonotethatinstruction-tuning\nwithhumanannotationalsosuffersfromasimilarlimitation: gains of instruction-tuning are higher\nfor larger models (Wei et al., 2022).\nReinforcingLMbiases. Apointofconcernfor\nthe authors is the unintended consequences of this\niterative algorithm, such as the amplification of\nproblematicsocialbiases(stereotypesorslursabout\ngender, race, etc.). Relatedly, one observed chal-\nlenge in this process is the algorithm’s difficulty in\nproducing balanced labels, which reflected models’\npriorbiases. Wehopefutureworkwillleadtobetter\nunderstandingoftheprosandconsoftheapproach.\nAcknowledgements\nThe authors would like to thank the anonymous\nreviewers for their constructive feedback. We espe-\nciallythankSewonMin,EricWallace,OfirPress,\nandothermembersofUWNLPandAllenNLPfor\ntheir encouraging feedback and intellectual sup-\nport. ThisworkwassupportedinpartbyDARPA\nMCS program through NIWC Pacific (N66001-19-\n2-4031), ONR N00014-18-1-2826, ONR MURI' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 9}
page_content='ciallythankSewonMin,EricWallace,OfirPress,\nandothermembersofUWNLPandAllenNLPfor\ntheir encouraging feedback and intellectual sup-\nport. ThisworkwassupportedinpartbyDARPA\nMCS program through NIWC Pacific (N66001-19-\n2-4031), ONR N00014-18-1-2826, ONR MURI\nN00014-18-1-2670, and gifts from AI2 and an\nAllen Investigator award.\nReferences\nMassih-Reza Amini, Vasilii Feofanov, Loic Pauletto,\nEmilie Devijver, and Yury Maximov. 2022. Self-\ntraining: Asurvey. arXivpreprintarXiv:2202.12040 .\nStephen H Bach, Victor Sanh, Zheng-Xin Yong, Al-\nbert Webson, Colin Raffel, Nihal V Nayak, Abheesht\nSharma, Taewoon Kim, M Saiful Bari, Thibault\nFevry, et al. 2022. PromptSource: An Integrated\nDevelopment Environment and Repository for Nat-\nural Language Prompts. In Annual Meeting of the\nAssociation for Computational Linguistics (ACL)-\nSystem Demonstrations .\nTomB.Brown,BenjaminMann,NickRyder,Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 9}
page_content='ural Language Prompts. In Annual Meeting of the\nAssociation for Computational Linguistics (ACL)-\nSystem Demonstrations .\nTomB.Brown,BenjaminMann,NickRyder,Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, and et al. 2020. Language\nmodels are few-shot learners. In Advances in Neural\nInformation Processing Systems (NeurIPS).\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,\nMostafaDehghani,SiddharthaBrahma,etal.2022.\nScalinginstruction-finetunedlanguagemodels. arXiv\npreprint arXiv:2210.11416 .\nJingfeiDu,ÉdouardGrave,BelizGunel,VishravChaud-\nhary,OnurCelebi,MichaelAuli,VeselinStoyanov,\nand Alexis Conneau. 2021. Self-training improves\npre-training for natural language understanding. In' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 9}
page_content='Conference of the North American Chapter of the As-\nsociationforComputationalLinguistics (NAACL) :\nHuman Language Technologies , pages 5408–5418.\nSteven Y Feng, Varun Gangal, Jason Wei, Sarath Chan-\ndar, Soroush Vosoughi, Teruko Mitamura, and Ed-\nuard Hovy. 2021. A survey of data augmentation\napproaches for nlp. In Annual Meeting of the Asso-\nciation for Computational Linguistics (ACL)ACL-\nIJCNLP - Findings , pages 968–988.\nDaniel Fried, Ronghang Hu, Volkan Cirik, Anna\nRohrbach,JacobAndreas,Louis-PhilippeMorency,\nTaylor Berg-Kirkpatrick, Kate Saenko, Dan Klein,\nandTrevorDarrell.2018. Speaker-followermodels\nforvision-and-languagenavigation. In Advancesin\nNeural Information Processing Systems (NeurIPS).\nJunxian He, Jiatao Gu, Jiajun Shen, and Marc’Aurelio\nRanzato. 2019. Revisiting self-training for neural\nsequence generation. In International Conference on\nLearning Representations (ICLR).\nGeoffreyHinton,OriolVinyals,JeffDean,etal.2015.\nDistilling the knowledge in a neural network. In' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 10}
page_content='Junxian He, Jiatao Gu, Jiajun Shen, and Marc’Aurelio\nRanzato. 2019. Revisiting self-training for neural\nsequence generation. In International Conference on\nLearning Representations (ICLR).\nGeoffreyHinton,OriolVinyals,JeffDean,etal.2015.\nDistilling the knowledge in a neural network. In\nAdvances in Neural Information Processing Systems\n(NeurIPS) Workshop on Deep Learning .\nOr Honovich, ThomasScialom, Omer Levy, andTimo\nSchick. 2022a. Unnatural instructions: Tuning lan-\nguagemodelswith(almost)nohumanlabor. arXiv\npreprint arXiv:2212.09689 .\nOr Honovich, Uri Shaham, Samuel R Bowman, and\nOmer Levy. 2022b. Instruction induction: From few\nexamplestonaturallanguagetaskdescriptions. arXiv\npreprint arXiv:2205.10782 .\nJiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu,\nXuezhi Wang, Hongkun Yu, and Jiawei Han. 2022.\nLarge language models can self-improve. arXiv\npreprint arXiv:2210.11610 .\nNikhil Kandpal, Haikang Deng, Adam Roberts, Eric\nWallace, and Colin Raffel. 2022. Large language' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 10}
page_content='preprint arXiv:2205.10782 .\nJiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu,\nXuezhi Wang, Hongkun Yu, and Jiawei Han. 2022.\nLarge language models can self-improve. arXiv\npreprint arXiv:2210.11610 .\nNikhil Kandpal, Haikang Deng, Adam Roberts, Eric\nWallace, and Colin Raffel. 2022. Large language\nmodelsstruggletolearnlong-tailknowledge. arXiv\npreprint arXiv:2211.08411 .\nNikita Kitaev, Steven Cao, and Dan Klein. 2019. Multi-\nlingualconstituencyparsingwithself-attentionand\npre-training. In AnnualMeetingoftheAssociationfor\nComputational Linguistics (ACL), pages 3499–3505.\nNikita Kitaev and Dan Klein. 2018. Constituency pars-\ning with a self-attentive encoder. In Annual Meet-\ning of the Association for Computational Linguistics\n(ACL), pages 2676–2686.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThepowerofscaleforparameter-efficientprompttun-\ning. InConference on Empirical Methods in Natural\nLanguage Processing (EMNLP).\nAlisaLiu,SwabhaSwayamdipta, NoahA.Smith, and' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 10}
page_content='(ACL), pages 2676–2686.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThepowerofscaleforparameter-efficientprompttun-\ning. InConference on Empirical Methods in Natural\nLanguage Processing (EMNLP).\nAlisaLiu,SwabhaSwayamdipta, NoahA.Smith, and\nYejin Choi. 2022. WANLI: Worker and ai collabora-\ntion for natural language inference dataset creation.InConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP) - Findings .\nLucie Charlotte Magister, Jonathan Mallinson, Jakub\nAdamek, Eric Malmi, and Aliaksei Severyn. 2022.\nTeaching small language models to reason. arXiv\npreprint arXiv:2212.08410 .\nDheeraj Mekala, Tu Vu, Timo Schick, and Jingbo\nShang. 2022. Leveraging qa datasets to improve\ngenerative data augmentation. arXiv preprint\narXiv:2205.12604 .\nYuMeng,MartinMichalski,JiaxinHuang,YuZhang,\nTarek Abdelzaher, and Jiawei Han. 2023. Tun-\ning language models as training data generators for\naugmentation-enhancedfew-shotlearning. In Inter-' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 10}
page_content='generative data augmentation. arXiv preprint\narXiv:2205.12604 .\nYuMeng,MartinMichalski,JiaxinHuang,YuZhang,\nTarek Abdelzaher, and Jiawei Han. 2023. Tun-\ning language models as training data generators for\naugmentation-enhancedfew-shotlearning. In Inter-\nnational Conference on Machine Learning (ICML).\nSoYeonMin,DevendraSinghChaplot,PradeepRaviku-\nmar, Yonatan Bisk, and Ruslan Salakhutdinov. 2022.\nFILM:FollowingInstructionsinLanguagewithMod-\nular Methods. In International Conference on Learn-\ning Representations (ICLR).\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, and\nHannanehHajishirzi.2022. Cross-TaskGeneraliza-\ntion via Natural Language Crowdsourcing Instruc-\ntions. InAnnual Meeting of the Association for Com-\nputational Linguistics (ACL).\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L Wainwright, Pamela Mishkin, Chong Zhang,\nSandhiniAgarwal, KatarinaSlama, AlexRay, etal.\n2022. Training Language Models to Follow Instruc-\ntions withHuman Feedback. In Advances inNeural' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 10}
page_content='putational Linguistics (ACL).\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L Wainwright, Pamela Mishkin, Chong Zhang,\nSandhiniAgarwal, KatarinaSlama, AlexRay, etal.\n2022. Training Language Models to Follow Instruc-\ntions withHuman Feedback. In Advances inNeural\nInformation Processing Systems (NeurIPS).\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the lim-\nits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research\n(JMLR).\nYasaman Razeghi, Robert L Logan IV, Matt Gardner,\nand Sameer Singh. 2022. Impact of pretraining term\nfrequencies on few-shot reasoning. arXiv preprint\narXiv:2202.07206 .\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version of\nbert: smaller,faster,cheaperandlighter. In Advances\nin Neural Information Processing Systems (NeurIPS)\nWorkshoponEnergyEfficientMachineLearningand' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 10}
page_content='arXiv:2202.07206 .\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version of\nbert: smaller,faster,cheaperandlighter. In Advances\nin Neural Information Processing Systems (NeurIPS)\nWorkshoponEnergyEfficientMachineLearningand\nCognitive Computing .\nVictorSanh,AlbertWebson,ColinRaffel,StephenBach,\nLintang Sutawika, Zaid Alyafeai, Antoine Chaffin,\nArnaud Stiegler, Arun Raja, Manan Dey, M Saiful\nBari, Canwen Xu, Urmish Thakker, Shanya Sharma\nSharma, Eliza Szczechla, Taewoon Kim, Gunjan\nChhablani,NihalNayak,DebajyotiDatta,Jonathan\nChang, Mike Tian-Jian Jiang, Han Wang, Matteo\nManica, Sheng Shen, Zheng Xin Yong, Harshit\nPandey, Rachel Bawden, Thomas Wang, Trishala' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 10}
page_content='Neeraj, Jos Rozen, Abheesht Sharma, Andrea San-\ntilli, Thibault Fevry, Jason Alan Fries, Ryan Tee-\nhan, Teven Le Scao, Stella Biderman, Leo Gao,\nThomasWolf,andAlexanderMRush.2022. Multi-\ntaskPromptedTrainingEnablesZero-ShotTaskGen-\neralization. In International Conference on Learning\nRepresentations (ICLR).\nTimo Schick and Hinrich Schütze. 2021. Generating\ndatasets with pretrained language models. In Con-\nferenceonEmpiricalMethodsinNaturalLanguage\nProcessing (EMNLP).\nMohit Shridhar, Jesse Thomason, Daniel Gordon,\nYonatanBisk,WinsonHan,RoozbehMottaghi,Luke\nZettlemoyer, and Dieter Fox. 2020. ALFRED: A\nBenchmark for Interpreting Grounded Instructions\nfor Everyday Tasks. In IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR).\nChandan Singh, John X Morris, Jyoti Aneja, Alexan-\nderMRush,andJianfengGao.2022. Explainingpat-\nternsindatawithlanguagemodelsviainterpretable\nautoprompting. arXiv preprint arXiv:2210.01848 .\nZhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 11}
page_content='puter Vision and Pattern Recognition (CVPR).\nChandan Singh, John X Morris, Jyoti Aneja, Alexan-\nderMRush,andJianfengGao.2022. Explainingpat-\nternsindatawithlanguagemodelsviainterpretable\nautoprompting. arXiv preprint arXiv:2210.01848 .\nZhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin\nZhang, Zhenfang Chen, David Cox, Yiming Yang,\nand Chuang Gan. 2023. Principle-driven self-\nalignment of language models from scratch with\nminimal human supervision. arXiv preprint\narXiv:2305.03047 .\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford al-\npaca: An instruction-following llama model. https:\n//github.com/tatsu-lab/stanford_alpaca .\nYizhong Wang, Swaroop Mishra, Pegah Alipoor-\nmolabashi, Yeganeh Kordi, Amirreza Mirzaei,\nAnjana Arunkumar, Arjun Ashok, Arut Selvan\nDhanasekaran, Atharva Naik, David Stap, Eshaan\nPathak, Giannis Karamanolakis, Haizhi Gary Lai, Is-\nhanPurohit,IshaniMondal,JacobAnderson,Kirby' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 11}
page_content='Yizhong Wang, Swaroop Mishra, Pegah Alipoor-\nmolabashi, Yeganeh Kordi, Amirreza Mirzaei,\nAnjana Arunkumar, Arjun Ashok, Arut Selvan\nDhanasekaran, Atharva Naik, David Stap, Eshaan\nPathak, Giannis Karamanolakis, Haizhi Gary Lai, Is-\nhanPurohit,IshaniMondal,JacobAnderson,Kirby\nKuznia, Krima Doshi, Maitreya Patel, Kuntal Ku-\nmar Pal, Mehrad Moradshahi, Mihir Parmar, Mi-\nrali Purohit, Neeraj Varshney, Phani Rohitha Kaza,\nPulkitVerma,RavsehajSinghPuri,RushangKaria,\nShailaja Keyur Sampat, Savan Doshi, Siddhartha\nMishra, Sujan Reddy, Sumanta Patro, Tanay Dixit,\nXudong Shen, Chitta Baral, Yejin Choi, Noah A.\nSmith, Hannaneh Hajishirzi, and Daniel Khashabi.\n2022. Super-naturalinstructions: Generalizationvia\ndeclarative instructions on 1600+ tasks. In Confer-\nenceonEmpiricalMethodsinNaturalLanguagePro-\ncessing(EMNLP).\nZiruiWang,AdamsWeiYu,OrhanFirat,andYuanCao.\n2021. Towardszero-labellanguagelearning. arXiv\npreprint arXiv:2109.09193 .\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu,' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 11}
page_content='declarative instructions on 1600+ tasks. In Confer-\nenceonEmpiricalMethodsinNaturalLanguagePro-\ncessing(EMNLP).\nZiruiWang,AdamsWeiYu,OrhanFirat,andYuanCao.\n2021. Towardszero-labellanguagelearning. arXiv\npreprint arXiv:2109.09193 .\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu,\nAdams Wei Yu, Brian Lester, Nan Du, Andrew M.\nDai, and Quoc V Le. 2022. Finetuned Language\nModels are Zero-Shot Learners. In International\nConference on Learning Representations (ICLR).Nathaniel Weir, Xingdi Yuan, Marc-Alexandre Côté,\nMatthew Hausknecht, Romain Laroche, Ida Momen-\nnejad,HarmVanSeijen,andBenjaminVanDurme.\n2022. One-Shot Learning from a Demonstration\nwith Hierarchical Latent Language. arXiv preprint\narXiv:2203.04806 .\nSean Welleck, Ximing Lu, Peter West, Faeze Brah-\nman, Tianxiao Shen, Daniel Khashabi, and Yejin\nChoi. 2023. Generating sequences by learning to\nself-correct. In InternationalConferenceonLearn-\ning Representations (ICLR).\nOrion Weller, Nicholas Lourie, Matt Gardner, and' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 11}
page_content='arXiv:2203.04806 .\nSean Welleck, Ximing Lu, Peter West, Faeze Brah-\nman, Tianxiao Shen, Daniel Khashabi, and Yejin\nChoi. 2023. Generating sequences by learning to\nself-correct. In InternationalConferenceonLearn-\ning Representations (ICLR).\nOrion Weller, Nicholas Lourie, Matt Gardner, and\nMatthew Peters. 2020. Learning from Task Descrip-\ntions. InConference on Empirical Methods in Natu-\nral Language Processing (EMNLP).\nPeterWest,ChandraBhagavatula,JackHessel,JenaD\nHwang, Liwei Jiang, Ronan Le Bras, Ximing Lu,\nSean Welleck, and Yejin Choi. 2021. Symbolic\nknowledgedistillation: fromgenerallanguagemod-\nels to commonsense models. In Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics (NAACL).\nQizhe Xie, Minh-Thang Luong, Eduard Hovy, and\nQuoc V Le. 2020. Self-training with noisy student\nimproves imagenet classification. In IEEE Confer-\nence on Computer Vision and Pattern Recognition\n(CVPR), pages 10687–10698.' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 11}
page_content='putational Linguistics (NAACL).\nQizhe Xie, Minh-Thang Luong, Eduard Hovy, and\nQuoc V Le. 2020. Self-training with noisy student\nimproves imagenet classification. In IEEE Confer-\nence on Computer Vision and Pattern Recognition\n(CVPR), pages 10687–10698.\nCanwen Xu, Daya Guo, Nan Duan, and Julian McAuley.\n2023. Baize: An open-source chat model with\nparameter-efficient tuning on self-chat data. arXiv\npreprint arXiv:2304.01196 .\nYiben Yang, Chaitanya Malaviya, Jared Fernandez,\nSwabhaSwayamdipta,RonanLeBras,Ji-PingWang,\nChandraBhagavatula,YejinChoi,andDougDowney.\n2020. Generative data augmentation for common-\nsensereasoning. In ConferenceonEmpiricalMeth-\nodsinNaturalLanguageProcessing (EMNLP) -Find-\nings.\nSeonghyeon Ye, Doyoung Kim, Joel Jang, Joongbo\nShin, and Minjoon Seo. 2022. Guess the instruction!\nmaking language models stronger zero-shot learners.\narXiv preprint arXiv:2210.02969 .\nEric Zelikman, Jesse Mu, Noah D Goodman, and\nYuhuaiTonyWu.2022. STar: Self-taughtreasoner' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 11}
page_content='ings.\nSeonghyeon Ye, Doyoung Kim, Joel Jang, Joongbo\nShin, and Minjoon Seo. 2022. Guess the instruction!\nmaking language models stronger zero-shot learners.\narXiv preprint arXiv:2210.02969 .\nEric Zelikman, Jesse Mu, Noah D Goodman, and\nYuhuaiTonyWu.2022. STar: Self-taughtreasoner\nbootstrappingreasoningwithreasoning. In Advances\ninNeuralInformationProcessingSystems (NeurIPS).\nXuandong Zhao, Siqi Ouyang, Zhiguo Yu, Ming Wu,\nand Lei Li. 2022. Pre-trained language models\ncan be fully zero-shot learners. arXiv preprint\narXiv:2212.06950 .\nChuntingZhou,JunxianHe,XuezheMa,TaylorBerg-\nKirkpatrick, and Graham Neubig. 2022a. Prompt\nConsistency for Zero-Shot Task Generalization. In\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP) - Findings .' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 11}
page_content='Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han,\nKeiranPaster,SilviuPitis,HarrisChan,andJimmy\nBa.2022b. Largelanguagemodelsarehuman-level\nprompt engineers. arXiv preprint arXiv:2211.01910 .' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 12}
page_content='Supplemental Material\nA Implementation Details\nA.1 Writing the Seed Tasks\nOur method relies on a set of seed tasks to bootstrap the generation. The seed tasks are important for both\nencouraging the task diversity and demonstrating correct ways for solving the diverse tasks. For example,\nwithcodingtaskstopromptthemodel,ithasalargerchancetogeneratecoding-relatedtasks;it’salso\nbettertohavecodingoutputtoguidethemodelinwritingcodefornewtasks. So,themorediversethe\nseed tasks are, the more diverse and better quality the generated tasks will be.\nOur seed tasks were written when we initiated this project, and targeted for the diverse and interesting\nusages of LLMs. The tasks were written by the authors and our labmates at UWNLP, without explicit\nreference to existing datasets or specific testing tasks. We further categorized the tasks into classification\nand non-classification tasks, based on whether the task has a limited output label space. In total, there are' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 13}
page_content='reference to existing datasets or specific testing tasks. We further categorized the tasks into classification\nand non-classification tasks, based on whether the task has a limited output label space. In total, there are\n25 classification tasks and 150 non-classification tasks. We release this data in our GitHub repository.11\nTo provide a sense of how much the model is generalizing beyond these seed tasks, we further quantify\ntheoverlapbetweentheinstructionsoftheseseedtasksandtheinstructionsofourtestsets,includingboth\nSUPERNItaskinstructions(§4.3)andtheuser-orientedinstructionsinourhumanevaluation(§4.4). We\ncompute ROUGE-Lsimilarities between eachseed instruction and itsmost similar instructionin the test\nset. ThedistributionoftheROUGE-LscoresareplottedinFigure8,withtheaverageROUGE-Lsimilarity\nbetweentheseedinstructionsand SUPERNIas0.21,andtheaverageROUGE-Lsimilaritybetweenthe\nseedinstructionsanduser-orientedinstructionsas0.34. Weseeadecentdifferencebetweentheseedtasks' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 13}
page_content='set. ThedistributionoftheROUGE-LscoresareplottedinFigure8,withtheaverageROUGE-Lsimilarity\nbetweentheseedinstructionsand SUPERNIas0.21,andtheaverageROUGE-Lsimilaritybetweenthe\nseedinstructionsanduser-orientedinstructionsas0.34. Weseeadecentdifferencebetweentheseedtasks\nand both test sets. There is exactly one identical seed instruction occurring in the user-oriented instruction\ntest set, which is “answer the following question” and the following questions are actually very different.\n0 0.2 0.4 0.6 0.8 101020304050\nROUGE-L  SimilarityNumber of Instructions\n0 0.2 0.4 0.6 0.8 105101520253035\nROUGE-L  SimilarityNumber of Instructions\nFigure 8: Distribution of the ROUGE-L scores between seed instructions and their most similar instructions in\nSUPERNI (left) and the 252 user-oriented instructions (right).\nA.2 Querying the GPT3 API\nWe use different sets of hyperparameters when querying GPT3 API for different purposes. These hyperpa-' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 13}
page_content='SUPERNI (left) and the 252 user-oriented instructions (right).\nA.2 Querying the GPT3 API\nWe use different sets of hyperparameters when querying GPT3 API for different purposes. These hyperpa-\nrameters are found to work well with the GPT3 model (“davinci” engine) and the other instruction-tuned\nGPT3variants. We listed them in Table 4. OpenAI charges $0.02 per 1000 tokens for making completion\nrequesttothe“davinci”engineasofDecember,2022. Thegenerationofourentiredatasetcostaround\n$600.\nA.3 Finetuning GPT3\nGPT3SELF-INSTandsomeofourbaselinesarefinetunedfrom GPT3model(“davinci”enginewith175B\nparameters). We conduct this finetuning via OpenAI’s finetuning API.12While the details of how the\nmodel is finetuned with this API are not currently available (e.g., which parameters are updated, or what\n11https://github.com/yizhongw/self-instruct/blob/main/human_eval/user_oriented_instructions.\njsonl\n12See the the details on OpenAI’s API.' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 13}
page_content='Experiments ↓ Temp. Top_P Freq. Penalty Presence Penalty Beam Size Max Length Stop Sequences\nGenerating instructions 0.7 0.5 0 2 1 1024 "\\n\\n", "\\n16", "16.", "16 ."\nIdentifying clf. tasks 0 0 0 0 1 3 "\\n", "Task:"\nGenerating instances 0 0 0 1.5 1 300 "Task:"\nEvaluating models 0 0 0 0 0 1024 None (default)\nTable 4: Hyper-parameters for querying OpenAI API in different experiments.\ntheoptimizeris),wetuneallourmodelswiththedefaulthyperparametersofthisAPIsothattheresultsare\ncomparable. Weonlysetthe“prompt_loss_weight”to0sincewefindthisworksbetterinourcase,and\nevery finetuning experiment is trained for two epochs to avoid overfitting the training tasks. Finetuning is\ncharged based on the number of tokens in the training file. In our case, finetuning GPT3SELF-INSTfrom the\nGPT3 model on the entire generated data cost $338.\nA.4 Prompting Templates for Data Generation\nSELF-INSTRUCT relies on a number of prompting templates in order to elicit the generation from language' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 14}
page_content='GPT3 model on the entire generated data cost $338.\nA.4 Prompting Templates for Data Generation\nSELF-INSTRUCT relies on a number of prompting templates in order to elicit the generation from language\nmodels. Here we provide our four templates for generating the instruction (Table 5), classifying whether\nan instruction represents a classification task or not (Table 6), generating non-classification instances with\ntheinput-firstapproach(Table7),andgeneratingclassificationinstanceswiththeoutput-firstapproach\n(Table 8).\nCome up with a series of tasks:\nTask 1: {instruction for existing task 1}\nTask 2: {instruction for existing task 2}\nTask 3: {instruction for existing task 3}\nTask 4: {instruction for existing task 4}\nTask 5: {instruction for existing task 5}\nTask 6: {instruction for existing task 6}\nTask 7: {instruction for existing task 7}\nTask 8: {instruction for existing task 8}\nTask 9:\nTable5: Promptusedforgeneratingnewinstructions. 8existinginstructionsarerandomlysampledfromthetask' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 14}
page_content='Task 5: {instruction for existing task 5}\nTask 6: {instruction for existing task 6}\nTask 7: {instruction for existing task 7}\nTask 8: {instruction for existing task 8}\nTask 9:\nTable5: Promptusedforgeneratingnewinstructions. 8existinginstructionsarerandomlysampledfromthetask\npool for in-context demonstration. The model is allowed to generate instructions for new tasks, until it stops its\ngeneration, reaches its length limit or generates “Task 16” tokens.' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 14}
page_content='Can the following task be regarded as a classification task with finite output labels?\nTask: Given my personality and the job, tell me if I would be suitable.\nIs it classification? Yes\nTask: Give me an example of a time when you had to use your sense of humor.\nIs it classification? No\nTask: Replace the placeholders in the given text with appropriate named entities.\nIs it classification? No\nTask: Fact checking - tell me if the statement is true, false, or unknown, based on your\nknowledge and common sense.\nIs it classification? Yes\nTask: Return the SSN number for the person.\nIs it classification? No\nTask: Detect if the Reddit thread contains hate speech.\nIs it classification? Yes\nTask: Analyze the sentences below to identify biases.\nIs it classification? No\nTask: Select the longest sentence in terms of the number of words in the paragraph, output\nthe sentence index.\nIs it classification? Yes\nTask: Find out the toxic word or phrase in the sentence.\nIs it classification? No' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 15}
page_content='Task: Analyze the sentences below to identify biases.\nIs it classification? No\nTask: Select the longest sentence in terms of the number of words in the paragraph, output\nthe sentence index.\nIs it classification? Yes\nTask: Find out the toxic word or phrase in the sentence.\nIs it classification? No\nTask: Rank these countries by their population.\nIs it classification? No\nTask: You are provided with a news article, and you need to identify all the categories that\nthis article belongs to. Possible categories include: Music, Sports, Politics, Tech, Finance,\nBasketball, Soccer, Tennis, Entertainment, Digital Game, World News. Output its categories one\nby one, seperated by comma.\nIs it classification? Yes\nTask: Given the name of an exercise, explain how to do it.\nIs it classification? No\nTask: Select the oldest person from the list.\nIs it classification? Yes\nTask: Find the four smallest perfect numbers.\nIs it classification? No' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 15}
page_content='by one, seperated by comma.\nIs it classification? Yes\nTask: Given the name of an exercise, explain how to do it.\nIs it classification? No\nTask: Select the oldest person from the list.\nIs it classification? Yes\nTask: Find the four smallest perfect numbers.\nIs it classification? No\nTask: Does the information in the document supports the claim? You can answer "Support" or\n"Unsupport".\nIs it classification? Yes\nTask: Create a detailed budget for the given hypothetical trip.\nIs it classification? No\nTask: Given a sentence, detect if there is any potential stereotype in it. If so, you should\nexplain the stereotype. Else, output no.\nIs it classification? No\n⋯\nTask: To make the pairs have the same analogy, write the fourth word.\nIs it classification? No\nTask: Given a set of numbers, find all possible subsets that sum to a given number.\nIs it classification? No\nTask: {instruction for the target task}' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 15}
page_content='Is it classification? No\n⋯\nTask: To make the pairs have the same analogy, write the fourth word.\nIs it classification? No\nTask: Given a set of numbers, find all possible subsets that sum to a given number.\nIs it classification? No\nTask: {instruction for the target task}\nTable 6: Prompt used for classifying whether a task instruction is a classification task or not.' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 15}
page_content='Come up with examples for the following tasks. Try to generate multiple examples when possible.\nIf the task doesn’t require additional input, you can generate the output directly.\nTask: Which exercises are best for reducing belly fat at home?\nOutput:\n- Lying Leg Raises\n- Leg In And Out\n- Plank\n- Side Plank\n- Sit-ups\nTask: Extract all the country names in the paragraph, list them separated by commas.\nExample 1\nParagraph: Dr. No is the sixth novel by the English author Ian Fleming to feature his British\nSecret Service agent James Bond. Written at Fleming’s Goldeneye estate in Jamaica, it was\nfirst published in the United Kingdom by Jonathan Cape in 1958. In the novel Bond looks into\nthe disappearance in Jamaica of two fellow MI6 operatives who had been investigating Doctor\nNo. Bond travels to No’s Caribbean island and meets Honeychile Rider, who is there to collect\nshells. They are captured and taken to a luxurious facility carved into a mountain. The' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 16}
page_content='the disappearance in Jamaica of two fellow MI6 operatives who had been investigating Doctor\nNo. Bond travels to No’s Caribbean island and meets Honeychile Rider, who is there to collect\nshells. They are captured and taken to a luxurious facility carved into a mountain. The\ncharacter of Doctor No, the son of a German missionary and a Chinese woman, was influenced by\nSax Rohmer’s Fu Manchu stories. Dr. No was the first of Fleming’s novels to face widespread\nnegative reviews in Britain, but it was received more favourably in the United States.\nOutput: English, British, Jamaica, the United Kingdom, German, Chinese, Britain, the United\nStates.\nTask: Converting 85 F to Celsius.\nOutput: 85 °F = 29.44 °C\nTask: Sort the given list ascendingly.\nExample 1\nList: [10, 92, 2, 5, -4, 92, 5, 101]\nOutput: [-4, 2, 5, 5, 10, 92, 92, 101]\nExample 2\nInput 2 - List: [9.99, 10, -5, -1000, 5e6, 999]\nOutput: [-1000, -5, 9.99, 10, 999, 5e6]' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 16}
page_content='States.\nTask: Converting 85 F to Celsius.\nOutput: 85 °F = 29.44 °C\nTask: Sort the given list ascendingly.\nExample 1\nList: [10, 92, 2, 5, -4, 92, 5, 101]\nOutput: [-4, 2, 5, 5, 10, 92, 92, 101]\nExample 2\nInput 2 - List: [9.99, 10, -5, -1000, 5e6, 999]\nOutput: [-1000, -5, 9.99, 10, 999, 5e6]\nTask: Suggest a better and more professional rephrasing of the following sentence.\nExample 1\nSentence: This house is surprisingly not constructed very well, and you probably need more\nmoney to fix it after you buy it. If you ask me, I would suggest you to consider other\ncandidates.\nOutput: This house does not seem to be constructed well, so you may need to spend more money\nto fix it after you purchase it. I would suggest that you look at other properties.\nExample 2\nSentence: Just so you know, we did an experiment last week and found really surprising results\n- language model can improve itself!\nOutput: Our experiments last week demonstrated surprising results, proving that the language' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 16}
page_content='Example 2\nSentence: Just so you know, we did an experiment last week and found really surprising results\n- language model can improve itself!\nOutput: Our experiments last week demonstrated surprising results, proving that the language\nmodel can improve itself.\n⋯\nTask: Turn down a job offer by sending an email to a recruiter explaining the reason.\nOutput: Hi [Recruiter],\nThank you so much for the generous offer to join your team. As we discussed, I’ve admired the\ncompany for a number of years, and am a proud endorser of its products. However, after further\nconsideration of where I currently am in my career, I’ve decided to accept an offer at another\ncompany.\nI would love to stay in touch with you and have already started following you on [Social Media\nPlatform]. Again, thank you so much for your time and consideration.\nThanks again,\n[Your Name]\nTask: {Instruction for the target task}' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 16}
page_content='company.\nI would love to stay in touch with you and have already started following you on [Social Media\nPlatform]. Again, thank you so much for your time and consideration.\nThanks again,\n[Your Name]\nTask: {Instruction for the target task}\nTable 7: Prompt used for the input-first approach of instance generation. The model is prompted to generate the\ninstancefirst,andthengeneratethecorrespondingoutput. Forinstructionsthatdon’trequireadditionalinput,the\noutput is allowed to be generated directly.' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 16}
page_content='Given the classification task definition and the class labels, generate an input that\ncorresponds to each of the class labels. If the task doesn’t require input, just generate the\ncorrect class label.\nTask: Classify the sentiment of the sentence into positive, negative, or mixed.\nClass label: mixed\nSentence: I enjoy the flavor of the restaurant but their service is too slow.\nClass label: Positive\nSentence: I had a great day today. The weather was beautiful and I spent time with friends.\nClass label: Negative\nSentence: I was really disappointed by the latest superhero movie. I would not recommend it.\nTask: Given a dialogue, classify whether the user is satisfied with the service. You should\nrespond with "Satisfied" or "Unsatisfied".\nClass label: Satisfied\nDialogue:\n- Agent: Thank you for your feedback. We will work to improve our service in the future.\n- Customer: I am happy with the service you provided. Thank you for your help.\nClass label: Unsatisfied\nDialogue:' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 17}
page_content='respond with "Satisfied" or "Unsatisfied".\nClass label: Satisfied\nDialogue:\n- Agent: Thank you for your feedback. We will work to improve our service in the future.\n- Customer: I am happy with the service you provided. Thank you for your help.\nClass label: Unsatisfied\nDialogue:\n- Agent: Sorry that we will cancel your order. You will get a refund within 7 business days.\n- Customer: oh that takes too long. I want you to take quicker action on this.\nTask: Given a political opinion, classify whether the speaker is a Democrat or Republican.\nClass label: Democrats\nOpinion: I believe, all should have access to quality healthcare regardless of their income.\nClass label: Republicans\nOpinion: I believe that people should be able to keep more of their hard-earned money and\nshould not be taxed at high rates.\nTask: Tell me if the following email is a promotion email or not.\nClass label: Promotion\nEmail: Check out our amazing new sale! We’ve got discounts on all of your favorite products.' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 17}
page_content='should not be taxed at high rates.\nTask: Tell me if the following email is a promotion email or not.\nClass label: Promotion\nEmail: Check out our amazing new sale! We’ve got discounts on all of your favorite products.\nClass label: Not Promotion\nEmail: We hope you are doing well. Let us know if you need any help.\nTask: Detect if the Reddit thread contains hate speech.\nClass label: Hate Speech\nThread: All people of color are stupid and should not be allowed to vote.\nClass label: Not Hate Speech\nThread: The best way to cook a steak on the grill.\nTask: Does the document supports the claim? Answer with "Support" or "Unsupport".\nClass label: Unsupport\nDocument: After a record-breaking run that saw mortgage rates plunge to all-time lows and\nhome prices soar to new highs, the U.S. housing market finally is slowing. While demand and\nprice gains are cooling, any correction is likely to be a modest one, housing economists and' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 17}
page_content='Class label: Unsupport\nDocument: After a record-breaking run that saw mortgage rates plunge to all-time lows and\nhome prices soar to new highs, the U.S. housing market finally is slowing. While demand and\nprice gains are cooling, any correction is likely to be a modest one, housing economists and\nanalysts say. No one expects price drops on the scale of the declines experienced during the\nGreat Recession.\nClaim: The US housing market is going to crash soon.\nClass label: Support\nDocument: The U.S. housing market is showing signs of strain, with home sales and prices\nslowing in many areas. Mortgage rates have risen sharply in recent months, and the number\nof homes for sale is increasing. This could be the beginning of a larger downturn, with some\neconomists predicting a potential housing crash in the near future.\nClaim: The US housing market is going to crash soon.\n⋯\nTask: Which of the following is not an input type? (a) number (b) date (c) phone number (d)' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 17}
page_content='economists predicting a potential housing crash in the near future.\nClaim: The US housing market is going to crash soon.\n⋯\nTask: Which of the following is not an input type? (a) number (b) date (c) phone number (d)\nemail address (e) all of these are valid inputs.\nClass label: (e)\nTask: {instruction for the target task}\nTable8: Promptusedfortheoutput-firstapproachofinstancegeneration. Themodelispromptedtogeneratethe\nclass label first, and then generate the corresponding input. This prompt is used for generating the instances for\nclassification tasks.' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 17}
page_content='B Human Evaluation Details for Following the User-oriented Instructions\nB.1 Human Evaluation Setup\nHereweprovidemoredetailsforthehumanevaluationdescribedin§4.4forratingthemodels’responsesto\nthe252user-orientedinstructions. Toensurefaithfulandreliableevaluation,weaskedtwoauthorsofthese\ninstructions(andofthispaper)tojudgemodelpredictions. Thesetwoevaluatorscoordinatedthestandards\nfor the 4-level rating system before starting annotation and then each of them rated all the instances\nindependently. Theywerepresentedwiththeinstruction,instanceinput,targetoutput(asareference),and\nmodel responses. Model responses are listed in random order, with all the model information anonymized.\nFigure9providesascreenshotoftheannotationinterface. Thereportedperformanceinthispaperisbased\non the results from one of the evaluators, and the trends from the other evaluator’s results are the same.\nFigure9: HumanevaluationindoneusingaGooglesheet,withpredictionsfromdifferentmodelspresentinrandom' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 18}
page_content='Figure9providesascreenshotoftheannotationinterface. Thereportedperformanceinthispaperisbased\non the results from one of the evaluators, and the trends from the other evaluator’s results are the same.\nFigure9: HumanevaluationindoneusingaGooglesheet,withpredictionsfromdifferentmodelspresentinrandom\norderand themodel informationbeing anonymized. Ourexpertevaluators arerequired toread theinstruction and\ninput, refer to the target, and then select the rating for the model’s response from A/B/C/D, corresponding to the 4\nlevels described in §4.4.\nB.2 Human Evaluation Agreement\nTo measure how reliable our human evaluation is, we calculate the inner-rater agreement between our two\nevaluators.\nWefirstreportCohen’s 𝜅,whichiscommonlyusedtomeasureinter-rateragreementfor categorical\nitems. When calculating this, we treat the 4-level rating (A-D) as a categorical variable, leading to a 𝜅of\n0.58, which is a moderate agreement according to common practice.13Furthermore, we also calculate the' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 18}
page_content='WefirstreportCohen’s 𝜅,whichiscommonlyusedtomeasureinter-rateragreementfor categorical\nitems. When calculating this, we treat the 4-level rating (A-D) as a categorical variable, leading to a 𝜅of\n0.58, which is a moderate agreement according to common practice.13Furthermore, we also calculate the\nagreement of our evaluators on classifying acceptable responses ((A or B) vs. (C or D)), with a final 𝜅of\n0.75, indicating substantial agreement.\nWealsocomputetheSpearmancorrelationcoefficient 𝜌betweentheratingsofourtwoevaluatorsby\ntreating the rating as an ordinal variable (A>B>C>D). The final coefficient is 𝜌= 0.81, indicating a high\n13https://en.wikipedia.org/wiki/Cohen%27s_kappa' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 18}
page_content='correlation between the two evaluators.\nB.3 Example Predictions from GPT3SELF-INST\nWepresentaselectionofuser-orientedtasks,thecorresponding GPT3SELF-INST-producedresponsesand\nannotator ratings in Table 9. We see that even for responses rated as level C, the model demonstrates\nextensive steps in solving the task, even though its final output is incorrect.' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 19}
page_content='Instruction + Instance Input Target Response GPT3SELF-INSTResponse Rating\nInstruction: Give students tips\non how to keep their nerves under\ncontrol during class presentations.Everyone gets a little nervous when speaking\nin front of a group. That’s normal. But the\nmore preparation you’ve done—meaning plenty of\nresearching, organizing, and rehearsing—the more\nconfident you’ll be. Preparation is the key.\nAnd if you make a mistake or stumble over your\nwords, just regroup and keep going. Nobody’s\nperfect, and nobody expects you to be.– Practice your presentation in front\nof friends and family.\n– Don’t drink too much caffeine before\nthe presentation.\n– Take deep breaths before you start\nspeaking.\n– If you feel nervous, smile.\n– Remember that everyone in the\naudience is rooting for you.A\nInstruction: Make a list of the pros\nand cons of the given decision.\nInput: Implementing a Remote Working\nPolicyPros:\n– Allowing employees to work from home can\nreduce their stressful commutes and increase' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 20}
page_content='– If you feel nervous, smile.\n– Remember that everyone in the\naudience is rooting for you.A\nInstruction: Make a list of the pros\nand cons of the given decision.\nInput: Implementing a Remote Working\nPolicyPros:\n– Allowing employees to work from home can\nreduce their stressful commutes and increase\ntheir productivity.\n– Remote working provides greater flexibility,\nparticularly for working parents and\ncaregivers who have other responsibilities.\n– ...(more points)\nCons:\n– It may be more difficult for team members\nto build meaningful, productive, creative\nrelationships with one another.\n– ...(more points)Pros:\n– Employees can work from home and\nsave on commuting costs.\n– Employees can spend more time with\ntheir families.\nCons:\n– Employees may not be as productive\nas they would be in the office.\n– Employees may not be as motivated to\nwork.A\nInstruction: A user story is an\ninformal, general explanation of\na software feature written from\nthe perspective of the end-user or' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 20}
page_content='their families.\nCons:\n– Employees may not be as productive\nas they would be in the office.\n– Employees may not be as motivated to\nwork.A\nInstruction: A user story is an\ninformal, general explanation of\na software feature written from\nthe perspective of the end-user or\ncustomer. Write a user story for a\ngiven software.\nInput: GmailAs a user, I want to be able to recover the\npassword to my account, so that I will be\nable to access my account in case I forgot\nthe password.As a user, I want to be able to send\nemails to my friends and family. I\nwant to be able to attach files to my\nemails. I want to be able to search\nfor emails in my inbox.A\nInstruction: A color description has\nbeen provided. Find the CSS code\nassociated with that color.\nInput: A light red color with a\nmedium light shade of pink#DB7093 ( ⬛) #ff66cc ( ⬛) B\nInstruction: Someone gives you some\ninformation about where they want\nto go and what they enjoy seeing and\ndoing. Describe a proper road trip' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 20}
page_content='been provided. Find the CSS code\nassociated with that color.\nInput: A light red color with a\nmedium light shade of pink#DB7093 ( ⬛) #ff66cc ( ⬛) B\nInstruction: Someone gives you some\ninformation about where they want\nto go and what they enjoy seeing and\ndoing. Describe a proper road trip\nfor them and find the best stops and\nactivities on the way.\nInput: I’m looking for a west coast\nroad trip. My goal is to start near\nSan Diego and travel to Los Angeles.Begin near the city of San Diego and make the\nfirst stop in coastal La Jolla. This upscale\nlittle village is a great place for an overnight,\na day of sunbathing at La Jolla Shores Beach,\nor a cocktail with a view. Just 20 minutes\nnorth, stop into beachy Encinitas to window\nshop the boutiques or sample fish tacos. Walk\nthrough the coastal meditation garden at the\nSelf-Realization Fellowship or join surfers\nin the line-up at one of the fantastic local\nbeaches here ...(more ideas)You should start your trip in San' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 20}
page_content='north, stop into beachy Encinitas to window\nshop the boutiques or sample fish tacos. Walk\nthrough the coastal meditation garden at the\nSelf-Realization Fellowship or join surfers\nin the line-up at one of the fantastic local\nbeaches here ...(more ideas)You should start your trip in San\nDiego, California. You can visit the\nSan Diego Zoo, which is one of the\nbest zoos in the world. Then you can\ndrive to Los Angeles, where you can see\nthe Hollywood signs and walk along the\nbeach.B\nInstruction: Give some examples of\nwhat people usually say in the given\nsocial situation.\nInput: when someone arrives safely– “Glad you made it safe and sound.”\n– “Good to see you got here unscathed.”\n– “So, bulletproof vests actually work!”Thank God, I made it. B\nInstruction: You are given a sentence\nin first-order logic. Translate it\ninto natural language.\nInput: ∃𝑥∀𝑦(𝑠𝑜𝑙𝑑𝑖𝑒𝑟 (𝑥)→𝑔𝑒𝑛𝑒𝑟𝑎𝑙 (𝑥,𝑦))There is someone (x) such that if he is a\nsoldier, then he is general of everyone.There exists a soldier such that for' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 20}
page_content='Instruction: You are given a sentence\nin first-order logic. Translate it\ninto natural language.\nInput: ∃𝑥∀𝑦(𝑠𝑜𝑙𝑑𝑖𝑒𝑟 (𝑥)→𝑔𝑒𝑛𝑒𝑟𝑎𝑙 (𝑥,𝑦))There is someone (x) such that if he is a\nsoldier, then he is general of everyone.There exists a soldier such that for\nevery general, he is a general.C\nInstruction: Make a questionnaire\nto help hotel guests write hotel\nreviews.– Your overall rating of this hotel:\n– Summarize your visit or highlight an\ninteresting detail:\n– ...(more questions)Question: How would you rate the\nservice at the front desk?\nAnswer: Excellent, Good, Average, PoorC\nInstruction: Design an outline\nfor a blog post based on the given\ninformation and list the sections\naccordingly.\nInput: This blog post is going to\nabout making a list of last-minute\ngift ideas. Also, the tone of the\npost is going to be relaxed and\ncasual.– Section: Movie tickets\n– Section: A fun game console\n– Section: Personalized items with\nphotos/artwork\n– ...(more sections)' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 20}
page_content='accordingly.\nInput: This blog post is going to\nabout making a list of last-minute\ngift ideas. Also, the tone of the\npost is going to be relaxed and\ncasual.– Section: Movie tickets\n– Section: A fun game console\n– Section: Personalized items with\nphotos/artwork\n– ...(more sections)\n– Takeaway: Don’t stress about out running out\nof time to buy, make a gift.– Introduction\n– List of Gift Ideas\n– ConclusionC\nInstruction: Write a Jira ticket for\nthe given task.\nInput: New Employee onboardingTitle: Please help our new employee get settled.\nPrepare a desk, chair, and computer for them to\nuse. Create their company account, email, and\ntemporary password. Grant access to company\ndocuments.Hi [Manager], I’m writing to you\nbecause I have a question about the new\nemployee onboarding process. I’ve been\nworking here for a few months now, and\nI’m still not sure how it works. Could\nyou please explain it to me?D\nInstruction: Create alliterations\nby finding synonyms for words in the\ngiven sentence.' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 20}
page_content='because I have a question about the new\nemployee onboarding process. I’ve been\nworking here for a few months now, and\nI’m still not sure how it works. Could\nyou please explain it to me?D\nInstruction: Create alliterations\nby finding synonyms for words in the\ngiven sentence.\nInput: David wears a hat everyday.David dons a derby daily. David wears a hat every day. D\nTable9: Examplesintheuser-orientedinstructionsdataset(§4.4)andpredictionsfrom GPT3SELF-INST. Theright\ncolumnindicatesoneofthefourqualityratingsassignedtothemodel’sresponse,with“A”indicating“validand\nsatisfying” responses (highest) and “D” indicating “irrelevant or invalid response” (lowest).' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 20}
page_content='C Task and Instance Examples from the Generated Instruction Data\nInstruction + Instance Input Target Output Validity\nInstruction: Generate a random password\nwith at least 6 characters.def generateRandomPassword():\npassword = ""\nwhile len(password) < 6:\npassword += chr(random.randint(65, 90))\nreturn passwordInstruction: ✓\nInput:✓\nOutput: ✓\nInstruction: Write a paragraph describing\nhow to tie shoelaces for people who have\nnever tied their shoes before. explain\neach step briefly.Paragraph: To tie shoelaces, first you need to make a loop with one\nend of the lace. Then pass the other end through the loop and pull\nit tight. Next, you need to wrap the lace around the shoe several\ntimes. Finally, tuck in the loose end into the wrapped part of the\nlace.Instruction: ✓\nInput:✓\nOutput: ✓\nInstruction: Given a word, find out its\nlength and its number of vowels.\nInput: Word = "hello"Length = 5, Number of vowels = 2 Instruction: ✓\nInput:✓\nOutput: ✓\nInstruction: Write a story with three' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 21}
page_content='lace.Instruction: ✓\nInput:✓\nOutput: ✓\nInstruction: Given a word, find out its\nlength and its number of vowels.\nInput: Word = "hello"Length = 5, Number of vowels = 2 Instruction: ✓\nInput:✓\nOutput: ✓\nInstruction: Write a story with three\ncharacters: a person, an animal and an\nobject.\nInput: Person: John, a programmer.\nAnimal: A dog.\nObject: A laptop.John was sitting on the floor in front of his laptop when he heard a\nknock at the door. He opened it to find a dog standing there. The\ndog walked into the room and jumped onto the couch next to John’s\nlaptop.Instruction: ✓\nInput:✓\nOutput: ✓\nInstruction: Compose an email and send it\nto your friend, asking for advice on what\nto do in this situation.\nInput: Situation: You are a programmer\nand you have been working on a project\nfor the past few months. The deadline is\napproaching, but you haven’t finished yet.\nYour manager has asked you to stay late\nevery day this week to finish it.Hi [Friend],' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 21}
page_content='to do in this situation.\nInput: Situation: You are a programmer\nand you have been working on a project\nfor the past few months. The deadline is\napproaching, but you haven’t finished yet.\nYour manager has asked you to stay late\nevery day this week to finish it.Hi [Friend],\nI’m having a bit of trouble with my current job. I’ve been\nworking on a project for the past few months, and the deadline\nis approaching. My manager has asked me to stay late every day this\nweek to finish it. I don’t think I can do that because I have other\ncommitments. What should I do?Instruction: ✓\nInput:✓\nOutput: ✓\nInstruction: What are some ways we can\nmake our school more eco-friendly?– Use reusable water bottles instead of disposable plastic ones.\n– Turn off lights when you leave a room.\n– Recycle paper, cans and bottles.\n– Plant trees to absorb carbon dioxide.Instruction: ✓\nInput:✓\nOutput: ✓\nInstruction: I am looking for a new\napartment. there are many options\navailable but they are all quite expensive.' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 21}
page_content='– Turn off lights when you leave a room.\n– Recycle paper, cans and bottles.\n– Plant trees to absorb carbon dioxide.Instruction: ✓\nInput:✓\nOutput: ✓\nInstruction: I am looking for a new\napartment. there are many options\navailable but they are all quite expensive.\ni want to move as soon as possible. give\nme advice on what to do.You should look for a cheaper apartment. You can try to negotiate\nwith the landlord and offer him/her a lower price. If you are lucky,\nthey will accept your offer. Otherwise, you can wait until there is\nan apartment available at a more reasonable price.Instruction: ✓\nInput:✓\nOutput: ✓\nInstruction: Write a dialogue between\ntwo people who have been friends for a\nlong time, and one of them is trying to\nconvince the other to do something that he\nor she doesn’t want to do.A: Hey, I have a great idea! Let’s go to the beach today.\nB: No thanks, I’m not really in the mood for that.\nA: Come on, it’ll be fun! We haven’t been there together in ages.' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 21}
page_content='long time, and one of them is trying to\nconvince the other to do something that he\nor she doesn’t want to do.A: Hey, I have a great idea! Let’s go to the beach today.\nB: No thanks, I’m not really in the mood for that.\nA: Come on, it’ll be fun! We haven’t been there together in ages.\nB: I know, but I just don’t feel like going.\nA: But you love the beach! You always say how much you enjoy\nswimming and lying in the sun.\nB: Yeah, I do. It’s just that I’ve been feeling kind of tired\nlately.\nA: Oh, come on. You’re never tired. You’re always so energetic.\nB: Well, I am now. I think I need some rest.\nA: Okay, fine. If you change your mind, let me know.Instruction: ✓\nInput:✓\nOutput: ✓\nInstruction: How can i create an app that\nwill be able to identify different kinds\nof plants by taking photos of them?You can use the following steps to create an app that will be able\nto identify different kinds of plants by taking photos of them.' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 21}
page_content='Input:✓\nOutput: ✓\nInstruction: How can i create an app that\nwill be able to identify different kinds\nof plants by taking photos of them?You can use the following steps to create an app that will be able\nto identify different kinds of plants by taking photos of them.\nStep 1: Create a database with all the information about the plants\nyou want to identify.\nStep 2: Create a user interface for your application.\nStep 3: Use image recognition techniques to identify the plant in\nthe photo.\nStep 4: Display the name of the plant on the screen.Instruction: ✓\nInput:✓\nOutput: ✓\nInstruction: Find the closest matching\nemoji to a given one.\nInput: Emoji:\nInstruction: ✓\nInput:✓\nOutput: ✓\nInstruction: Describe your favorite video\ngame using only verbs.\nInput: Game: The Legend of ZeldaExplore, fight, solve, upgrade, repeat Instruction: ✓\nInput:✓\nOutput: ✓\nTable 10: Representative validtasks generated by GPT3. As is discussed in §3, these generated tasks cover a broad' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 21}
page_content='Output: ✓\nInstruction: Describe your favorite video\ngame using only verbs.\nInput: Game: The Legend of ZeldaExplore, fight, solve, upgrade, repeat Instruction: ✓\nInput:✓\nOutput: ✓\nTable 10: Representative validtasks generated by GPT3. As is discussed in §3, these generated tasks cover a broad\nrangeofformats,texttypes,andunderlyingexpertise,whilebeingcorrectonmorethanhalfofallthegenerated\ntasks.' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 21}
page_content='Instruction + Instance Input Target Output Validity\nInstruction: Given a set of words\n𝑤 = {𝑤1,𝑤2,...,𝑤𝑛}where each word 𝑤𝑖\nhas𝑛letters, write a function that\ntakes a target length 𝑙and returns a\nnew set of words 𝑠such that |𝑠|=𝑙and\n|{𝑖∶𝑤𝑖∈𝑠}|≤𝑙∕|𝑤|.def wordSubsetSum(w, l):\ns = []\nfor i in range(len(w)):\nif len(s) + 1 < l:\ns.append(w[i])\nreturn sInstruction: ✗\nInput:✓\nOutput: ✗\nInstruction: The house on fire! go!– Go to the kitchen and take a pan.\n– Take water from the tap.\n– Pour water into the pan.\n– Put out the fire with the pan.Instruction: ✗\nInput:✓\nOutput: ✗\nInstruction: Find out if you have any\nfriends who like to play football or\nchess.def hasFriendsWhoLikeToPlayFootballOrChess(self):\nfriends = []\nfor friend in self.friends:\nif friend.likes == ’football’ or friend.likes == ’chess’:\nfriends.append(friend)\nreturn len(friends) > 0Instruction: ✗\nInput:✓\nOutput: ✓\nInstruction: A number of people are lined\nup for an interview. they are asked\nseveral questions and their answers are' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 22}
page_content='friends = []\nfor friend in self.friends:\nif friend.likes == ’football’ or friend.likes == ’chess’:\nfriends.append(friend)\nreturn len(friends) > 0Instruction: ✗\nInput:✓\nOutput: ✓\nInstruction: A number of people are lined\nup for an interview. they are asked\nseveral questions and their answers are\nrecorded as numbers. you need to identify\nwhich person has the highest salary among\nall the candidates.\nInput: Person 1: 2, 3, 4, 5, 6\nPerson 2: 7, 8, 9, 10, 11\nPerson 3: 12, 13, 14, 15, 16\nPerson 4: 17, 18, 19, 20, 21\nPerson 5: 22, 23, 24, 25, 26\nPerson 6: 27, 28, 29, 30, 31Highest salary Instruction: ✓\nInput:✗\nOutput: ✗\nInstruction: Given a list of words,\ngenerate a story about a group of people\nwho live on a planet. the story should\ncontain at least 10 sentences. each\nsentence should be generated by randomly\npicking one word from the list of words.The story starts with a group of people who live on a planet. They\nare very intelligent and have created many things that help them in' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 22}
page_content='who live on a planet. the story should\ncontain at least 10 sentences. each\nsentence should be generated by randomly\npicking one word from the list of words.The story starts with a group of people who live on a planet. They\nare very intelligent and have created many things that help them in\ntheir daily lives. One day, they decided to create a machine that\ncan make food for them. The machine was able to produce food but it\nalso produced something else. It produced a liquid that could be\nused as fuel for the machines. The people were happy because they\ndidn’t need to go out and look for food or fuel anymore. They had\neverything they needed right there in their homes. However, one\nday, the machine stopped working. The people tried to fix it but\ncouldn’t figure out what went wrong. They tried to find someone\nwho could help them but no one knew how to fix the machine. After\na few days, the people realized that they would die if they didn’t' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 22}
page_content='day, the machine stopped working. The people tried to fix it but\ncouldn’t figure out what went wrong. They tried to find someone\nwho could help them but no one knew how to fix the machine. After\na few days, the people realized that they would die if they didn’t\ndo anything about it. So, they decided to send some of their best\nscientists to space to find a new planet where they could live. The\nscientists traveled for a long time before they found a planet that\nlooked like a good place to live. They sent a message back to the\npeople telling them about the planet. The people were excited and\nstarted preparing for the journey. They built spaceships and flew\nto the new planet. When they arrived, they saw that the planet was\nbeautiful. There were trees and rivers and mountains. The people\nwere so happy that they finally found a new home. They started\nbuilding houses and planting crops. They lived happily ever after.Instruction: ✓\nInput:✗\nOutput: ✓\nInstruction: Find out what the author’s' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 22}
page_content='beautiful. There were trees and rivers and mountains. The people\nwere so happy that they finally found a new home. They started\nbuilding houses and planting crops. They lived happily ever after.Instruction: ✓\nInput:✗\nOutput: ✓\nInstruction: Find out what the author’s\nattitude towards working part-time is.Paragraph: Working part-time is a great way to earn some extra\nmoney while still being able to enjoy your free time. It’s also\na good way to get experience in a new field, or just try something\ndifferent for a while. However, it can be difficult to find the\nright balance between work and play.\nThe author seems to be positive about working part-time.Instruction: ✓\nInput:✗\nOutput: ✓\nInstruction: Is it ethical to use a new\ntechnology that could potentially cause\nharm to others?New Technology: Self-driving cars\nPotential Harm: Accidents caused by self-driving cars\nEthical? Yes, because the number of lives saved by self-driving' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 22}
page_content='Input:✗\nOutput: ✓\nInstruction: Is it ethical to use a new\ntechnology that could potentially cause\nharm to others?New Technology: Self-driving cars\nPotential Harm: Accidents caused by self-driving cars\nEthical? Yes, because the number of lives saved by self-driving\ncars will be greater than the number of lives lost due to accidents\ncaused by them.Instruction: ✓\nInput:✗\nOutput: ✓\nInstruction: What is your salary? class Solution(object):\ndef getSalary(self):\nreturn self.salaryInstruction: ✓\nInput:✓\nOutput: ✗\nInstruction: A person wants to buy a car\nfrom another person. he has some bitcoins,\nbut he doesn’t want to reveal his identity.\nhow can they make the transaction happen?The buyer and seller should agree on a price for the car. The buyer\nthen creates a bitcoin transaction that sends the agreed upon amount\nof bitcoins to the seller’s bitcoin address, but the buyer does not\nsign this transaction with his private key. Instead, he gives the' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 22}
page_content='then creates a bitcoin transaction that sends the agreed upon amount\nof bitcoins to the seller’s bitcoin address, but the buyer does not\nsign this transaction with his private key. Instead, he gives the\nunsigned transaction to the seller. The seller can now sign the\ntransaction using his private key, and broadcast it to the network.\nOnce the transaction is confirmed by the network, the buyer has no\nway to get his money back, so he will be forced to go through with\nthe purchase.Instruction: ✓\nInput:✓\nOutput: ✗\nTable 11: Representative invalidtasks generated by GPT3. The problematic fields are indicated in the validity\ncolumn. Asdiscussedin§3.3,althoughthesetaskscontainerrors,theystillprovidemanyusefulsignalsinsupervising\nmodels to follow instructions.' metadata={'source': 'private_upload/2023-08-09-17/SELF-INSTRUCT.pdf', 'page': 22}
filepath=private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf,len=66
page_content='Controllable Synthesis of Hierarchical Porous Fe 3O4Particles\nMediated by Poly(diallyldimethylammonium chloride) and Their\nApplication in Arsenic Removal\nTing Wang,†Liyuan Zhang,†Haiying Wang,†,‡Weichun Yang,†,‡Yingchun Fu,§Wenli Zhou,§\nWanting Yu,†Kaisong Xiang,†Zhen Su,†Shuo Dai,†and Liyuan Chai *,†,‡\n†Department of Environmental Engineering, School of Metallurgy and Environment, Central South University, Changsha 410017, P.\nR. China\n‡Chinese National Engineering Research Center for Control & Treatment of Heavy Metal Pollution, Changsha 410017, P. R. China\n§Key Laboratory of Chemical Biology and Traditional Chinese Medicine Research (Ministry of Education), College of Chemistry and\nChemical Engineering, Hunan Normal University, Changsha 410081, P. R. China\n*SSupporting Information\nABSTRACT: Hierarchical porous Fe 3O4particles with' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 0}
page_content='§Key Laboratory of Chemical Biology and Traditional Chinese Medicine Research (Ministry of Education), College of Chemistry and\nChemical Engineering, Hunan Normal University, Changsha 410081, P. R. China\n*SSupporting Information\nABSTRACT: Hierarchical porous Fe 3O4particles with\ntunable grain size were synthesized based on a facile poly(diallyldimethylammonium chloride) (PDDA)-modulated sol-vothermal method. The products were characterized withscanning electron microscopy (SEM) and transmissionelectron microscopy (TEM), X-ray photoelectron spectrosco-py (XPS), Fourier transform infrared spectroscopy (FT-IR),X-ray di ﬀraction (XRD), N\n2adsorption −desorption technique,\nvibrating sample magnetometer (VSM), and dynamic lightscattering (DLS). The results show that increasing the PDDAdosage decrease the grain size and particle size, which increased the particle porosity and enhanced the surface area from 7.05 to32.75 m' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 0}
page_content='2adsorption −desorption technique,\nvibrating sample magnetometer (VSM), and dynamic lightscattering (DLS). The results show that increasing the PDDAdosage decrease the grain size and particle size, which increased the particle porosity and enhanced the surface area from 7.05 to32.75 m\n2g−1. Possible mechanism can be ascribed to the PDDA function on capping the crystal surface and promoting the\nviscosity of reaction medium to mediate the growth and assembly of grain. Furthermore, the arsenic adsorption application of theas-obtained Fe\n3O4samples was investigated and the adsorption mechanism was proposed. High magnetic Fe 3O4particles with\nincreased surface area display improved arsenic adsorption performance, superior e ﬃciency in low-level arsenic removal, high\ndesorption e ﬃciency, and satisfactory magnetic recyclability, which are very promising compared with commercial Fe 3O4\nparticles.\nKEYWORDS: Fe3O4, poly (diallyldimethylammonium chloride), solvothermal, adsorption' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 0}
page_content='desorption e ﬃciency, and satisfactory magnetic recyclability, which are very promising compared with commercial Fe 3O4\nparticles.\nKEYWORDS: Fe3O4, poly (diallyldimethylammonium chloride), solvothermal, adsorption\n1. INTRODUCTION\nArsenic, one of the top 20 hazardous substances, greatly\nthreatens the health of human body, ecological balance, and\nindustrial development.1Thus, the remediation of arsenic\npollution has attracted worldwide attention.2−4So far,\ntechnologies involving oxidation,5coagulation,6adsorption,7−9\nion-exchange,10and reverse osmosis11have been developed to\ndetoxicate arsenic pollution. Among them, adsorption is one of\nthe most promising technologies for arsenic removal, because\nof its easy operation and low-cost production.4,12,13However,\nthe separation of traditional adsorbent (e.g., ﬁltration,\ncentrifugation or gravitational sedimentation) is time-consum-\ning and cost-ine ﬀective and therefore limits the practical' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 0}
page_content='of its easy operation and low-cost production.4,12,13However,\nthe separation of traditional adsorbent (e.g., ﬁltration,\ncentrifugation or gravitational sedimentation) is time-consum-\ning and cost-ine ﬀective and therefore limits the practical\napplication.14Hence, magnetic adsorbents such as Fe 3O4\nexhibit unique advantages due to their quick and e ﬀective\nmagnetic separation.15−17\nIn parallel, the rapid growth of nanotechnology has attracted\na great deal of interest in environmental application.18−25In\nterms of the application of Fe3O4as an adsorbent, decreasingthe Fe3O4particle size from micrometers to nanometers would\nincrease the available adsorptive areas by 100 to 1000\ntimes.26−28However, as the Fe3O4particle size decreases to\nnanometers, its response to an external magnetic ﬁeld\nundesirably decreases, which will not be large enough to\novercome Brownian motion and no e ﬃcient magnetic\nseparation will occur.26,29,30To tackle this problem, one' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 0}
page_content='times.26−28However, as the Fe3O4particle size decreases to\nnanometers, its response to an external magnetic ﬁeld\nundesirably decreases, which will not be large enough to\novercome Brownian motion and no e ﬃcient magnetic\nseparation will occur.26,29,30To tackle this problem, one\npractical strategy is to prepare magnetic hierarchical structures,\nwhich are constructed with building blocks of nanounits. Thehierarchical nanostructures not only exhibit high speci ﬁc surface\narea because of the abundant interparticle spaces or intra-\nparticle pores, but also possess satisfactory magnetic response\nbecause of their larger size and weaker Brownian motion, which\ntherefore show great superiority to individual nanometer- and\nmicrometer-sized materials.\n31−36To date, two conventional\nReceived: August 22, 2013\nAccepted: November 19, 2013Research Article\nwww.acsami.org\n© XXXX American Chemical Society A dx.doi.org/10.1021/am403533v |ACS Appl. Mater. Interfaces XXXX, XXX, XXX −XXX' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 0}
page_content='template methods were reported to synthesize hierarchical\nnanoarchitectures, including hard templates such as silica,37\npolymer spheres,38and metal oxides,39as well as soft templates\nsuch as emulsion droplets/micelles40,41and even gas bubbles.42\nThese synthetic routes seem to be inconvenient because\ncomplicated template presynthesis or time-consuming pre-\ncursor calcination at elevated temperature is needed.37,43−46\nMoreover, the removal of templates by erosion or calcination\nbrings adverse e ﬀect on the product morphology.47,48\nConsequently, it is preferable to develop one-step template-\nfree methods for the preparation of hierarchical particles with\nwell-de ﬁned morphology.\nIt is generally believed that the grain acts as the building\nblock and its oriental assembly constitutes hierarchical particles.Consequently, it is predictable that the grain property and its\nassembly behavior synchronously in ﬂuence product morphol-\nogy.49−51To date, the reported advanced template-free' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 1}
page_content='block and its oriental assembly constitutes hierarchical particles.Consequently, it is predictable that the grain property and its\nassembly behavior synchronously in ﬂuence product morphol-\nogy.49−51To date, the reported advanced template-free\nmethods have mainly focused on the modulation of grain\nassembly by Kirkendall e ﬀect,52,53Ostwald ripening ef-\nfect45,54−57or self-attachment e ﬀect.46,58,59For instance, Yong\net al. reported that the assembly of grain evolved into porous\nFe3O4hollow submicrospheres based on Ostwald ripening\nprocess through one-pot solvothermal method.60Though the\npattern and mechanism of grain assembly were comprehen-sively investigated, rare researches were devoted to studying the\neﬀect of grain property on the performance of yielded Fe\n3O4\nhierarchical particles.\nHerein, our research aimed to controllably prepare Fe3O4\nhierarchical particles via modulating the grain property though' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 1}
page_content='eﬀect of grain property on the performance of yielded Fe\n3O4\nhierarchical particles.\nHerein, our research aimed to controllably prepare Fe3O4\nhierarchical particles via modulating the grain property though\none-pot solvothermal method. To the best of our knowledge,reports have seldom demonstrated the availability of this\nstrategy on controlling the morphology and application\nperformance of hierarchical particles. Poly-(diallyldimethylammonium chloride) (PDDA), as an environ-mentally friendly and low cost polyelectrolyte, has been widely\nused in the preparation of composites via electrostatic or π−π\nstack interaction for biosensor and catalysis.\n61−65Although the\npotential modulation usage for the synthesis of magnetic\nparticles has not been investigated. In this research, a facile\nPDDA-mediated solvothermal method was proposed tocontrollably synthesize Fe\n3O4hierarchical particles. Increasing\nthe PDDA dosage declines the grain and particle size, which' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 1}
page_content='potential modulation usage for the synthesis of magnetic\nparticles has not been investigated. In this research, a facile\nPDDA-mediated solvothermal method was proposed tocontrollably synthesize Fe\n3O4hierarchical particles. Increasing\nthe PDDA dosage declines the grain and particle size, which\nleads to the increment of speci ﬁc area and porosity, eventually\nenhancing the adsorption performance. The mechanism for\nPDDA-induced grain size tunable strategy was also discussed.\nThe prepared Fe 3O4particles show higher adsorption capacity\nthan commercial Fe3O4particles and pose great potential in the\nlow-level arsenic removal, such as the remediation of\ngroundwater (with arsenic concentration of hundreds ofmicrograms per liter66−68).\n2. EXPERIMENTAL SECTION\n2.1. Materials. Ferric chloride hexahydrate (FeCl3·6H2O),\nanhydrous sodium acetate (CH3COONa, NaAc), and ethylene glycol' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 1}
page_content='low-level arsenic removal, such as the remediation of\ngroundwater (with arsenic concentration of hundreds ofmicrograms per liter66−68).\n2. EXPERIMENTAL SECTION\n2.1. Materials. Ferric chloride hexahydrate (FeCl3·6H2O),\nanhydrous sodium acetate (CH3COONa, NaAc), and ethylene glycol\n(EG) were obtained from the Sinopharm Group Chemical ReagentCo., Ltd.. A 35.0 wt % aqueous solution of high molar mass ( M\nw100\n000−200 000) PDDA was obtained from Sigma-Aldrich. Na3AsO4·\n12H2O and NaAsO2were used as the sources of As(V) and As(III),\nrespectively. Commercial Fe3O4with the diameter of 200 nm was\npurchased from Beijing Dk Nano technology Co., Ltd.. All reagentswere used without further treatment. Ultrapure water with a resistivityof 18.2 M Ωcm\n−1, produced with a Milli-Q apparatus (Millipore), was\nused throughout all of the experiments.2.2. Preparation of Porous Fe3O4.Porous Fe3O4particles were' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 1}
page_content='−1, produced with a Milli-Q apparatus (Millipore), was\nused throughout all of the experiments.2.2. Preparation of Porous Fe3O4.Porous Fe3O4particles were\nsynthesized exploiting a facile solvothermal method via morphology-mediated by PDDA solution. In a typical procedure, 1.35 g of FeCl\n3·\n6H2O was dissolved in a mixture containing 36 mL of EG and an\nappropriate amount of PDDA solution, then 3.6 g of NaAc was added.After vigorous stirring for 30 min, a transparent solution was obtained\nand transferred to a 50 mL Te ﬂon-lined autoclave, which was then\nplaced in an oven at 200 °C for 6 h, followed by naturally cooling to\nroom temperature. The black precipitate was collected and ultrasonicwashed by water and ethanol for three times, respectively, throughmagnetic separation. The yielded product was vacuum-dried at 60 °C\nfor 12 h. By modulating the dosage of PDDA solution as 1, 2, 3, 4, 5,and 6 g, the resultant series products were separately named as Fe\n3O4-i' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 1}
page_content='for 12 h. By modulating the dosage of PDDA solution as 1, 2, 3, 4, 5,and 6 g, the resultant series products were separately named as Fe\n3O4-i\n(i=1−6). To understand the formation mechanism of Fe3O4,F e3O4-2\nand Fe3O4-4 samples were collected at various reaction time (1.5, 2.5,\n4, 5, 6, and 8 h), followed by washing and drying procedures. Theobtained series products were denoted as Fe\n3O4-2-xho rF e3O4-4-xh(x\nrefers to the reaction time).\n2.3. Characterization. Scanning electron microscopy (SEM, JSM-\n6360) and transmission electron microscopy (TEM, TECNAI G2)\nwere used to characterize the morphology of the nanoparticles. The X-ray di ﬀraction (XRD) patterns of the Fe\n3O4were obtained using\nRigaku D/Max-RB di ﬀractometer with Cu −Kαradiation ( λ= 0.15406\nnm, 35 kV, 40 mA). X-ray photoelectron spectroscopy (XPS)measurements were carried out on a Thermo Fisher Scienti ﬁcK -\nAlpha 1063 using Al K αX-ray as the excitation source. Fourier 40' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 1}
page_content='3O4were obtained using\nRigaku D/Max-RB di ﬀractometer with Cu −Kαradiation ( λ= 0.15406\nnm, 35 kV, 40 mA). X-ray photoelectron spectroscopy (XPS)measurements were carried out on a Thermo Fisher Scienti ﬁcK -\nAlpha 1063 using Al K αX-ray as the excitation source. Fourier 40\ntransformed infrared spectroscopy (FT-IR, Nicolet IS10) wasemployed to analyze the molecular structure of the yielded productat a resolution of 4 cm\n−1. The size of the pinhole and the integration\ntime were set as 100 μm and 30 s, respectively. Magnetic properties of\nthe product were investigated using a vibrating sample magnetometer(VSM, EV7, ADE) with an applied ﬁeld between −7000 and 7000 Oe\nat room temperature. Speci ﬁc surface areas of the yielded products\nwere measured by adsorption −desorption of ultrapure N\n2on a\nQuantachrome Instruments system via Brunauer −Emmett −Teller\n(BET) method. Pore size distribution was determined by N2\ndesorption isotherm using Barret −Joymer −Halender (BJH) method.' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 1}
page_content='were measured by adsorption −desorption of ultrapure N\n2on a\nQuantachrome Instruments system via Brunauer −Emmett −Teller\n(BET) method. Pore size distribution was determined by N2\ndesorption isotherm using Barret −Joymer −Halender (BJH) method.\nThe size of the Fe 3O4particles was determined by dynamic light\nscattering (DLS) on a Malvern zetasizer instrument (type Nano-ZS,\nMalvern Instruments Ltd., Britain) using Fe3O4suspension with the\nconcentration of 0.01 g L−1.\n2.4. Batch Adsorption Experiment. Solutions containing\ndiﬀerent concentrations of As(V) or As(III) were prepared and\nadjusted to pH 5 ±0.2 using HCl. Then, 5 mg of the adsorbent\nsample was added to 10 mL arsenic aqueous solution under stirring.\nAfter a speci ﬁed time, the solid and liquid were magnetic separated' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 1}
page_content='diﬀerent concentrations of As(V) or As(III) were prepared and\nadjusted to pH 5 ±0.2 using HCl. Then, 5 mg of the adsorbent\nsample was added to 10 mL arsenic aqueous solution under stirring.\nAfter a speci ﬁed time, the solid and liquid were magnetic separated\nand the initial and residual concentrations of arsenic were measured byinductively coupled plasma-optical emission spectroscopy (ICP-OES)(Optima 5300DV). The adsorption isotherm was obtained by varyingthe initial arsenic concentrations and stirring for 4 h at 25 °C\n(concentration range: 0.1 −17 mg L\n−1for As(III) and 0.1 −7.5 mg L−1\nfor As(V), respectively). For comparison, commercial Fe3O4with the\ndiameter of 200 nm synthesized by coprecipitation was also exploited.The equilibrium adsorption capacity ( q\ne) (mg g−1) for arsenic was\ncalculated according to the following equation\n=−qcc V\nm()\ne0e\n(1)\nwhere c0and ce(mg L−1) are the initial and equilibrium arsenic\naqueous concentrations, respectively; Vis the volume (mL) of arsenic' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 1}
page_content='e) (mg g−1) for arsenic was\ncalculated according to the following equation\n=−qcc V\nm()\ne0e\n(1)\nwhere c0and ce(mg L−1) are the initial and equilibrium arsenic\naqueous concentrations, respectively; Vis the volume (mL) of arsenic\naqueous solution; mis the mass (mg) of adsorbents used in the\nexperiment.\nTo test the low-level arsenic removal feasibility of adsorbents, initial\narsenic solution with As(V) concentration in the range of 50 −1400 μg\nL−1and As(III) in the range of 50 −600μgL−1were prepared. Other\nadsorption experiment procedures were the same as above.\nThe adsorption kinetics was investigated with the initial As(V)\nconcentration of 3.5 mg L−1and As(III) concentration of 3 mg L−1at\npH = 5.0 ±0.2 and adsorbents dose = 0.5 g L−1. The solution wasACS Applied Materials & Interfaces Research Article\ndx.doi.org/10.1021/am403533v |ACS Appl. Mater. Interfaces XXXX, XXX, XXX −XXX B' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 1}
page_content='allowed to react with the adsorbent for a ﬁxed period (between 10 and\n240 min).\nThe regeneration of the absorbent was conducted by using 0.1 M\nNaOH solution as eluent with adsorbents dose = 1 g L−1at 25 °C.\nBrieﬂy, the absorbent was ﬁrst ultrasoni ﬁcated in NaOH solution for\n30 min and then shaken for 2 h, followed by magnetic separation and\nwashing by water three times. Then the adsorbent was applied into\nrecycle adsorption study. The recycle adsorption experimental\nprocedure and detection method are in accordance with the ﬁrst\nadsorption experiment, including the mixing of the adsorbent witharsenic solution under stirring for 4 h, the solid and liquid separation\nby external magnetism, and the determination of the residual arsenic\nsolution by ICP-OES (Optima 5300DV).\nAll the experimental data we re the average of triplicate\ndeterminations with relative errors under 5%.\n3. RESULTS AND DISCUSSION\n3.1. Morphology, Structure, and Property of Fe 3O4.' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 2}
page_content='by external magnetism, and the determination of the residual arsenic\nsolution by ICP-OES (Optima 5300DV).\nAll the experimental data we re the average of triplicate\ndeterminations with relative errors under 5%.\n3. RESULTS AND DISCUSSION\n3.1. Morphology, Structure, and Property of Fe 3O4.\nThe SEM images and size distribution of the Fe 3O4-i(i=1−6)\nparticles are presented in Figure 1. As seen, Figure 1A −F shows\nthat the size of monodispersed hierarchical particles monoto-\nnously decreases from (A) 420 nm to (F) 100 nm, as increasingthe PDDA dosage from 1 to 6 g. Correspondingly, themorphology of hierarchical particles gradually becomes coarseand porous, with the increase of PDDA dosage. As can beconﬁrmed by TEM images in Figure S1 in the SupportingInformation, increasing the PDDA dosage concurrently\ndecreases hierarchical particle size and increases porosity.HRTEM images were also conducted to give further insightinto the grain assembly, as shown in Figure 2. Taking Fe\n3O4-4' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 2}
page_content='decreases hierarchical particle size and increases porosity.HRTEM images were also conducted to give further insightinto the grain assembly, as shown in Figure 2. Taking Fe\n3O4-4\nas an example, the particle shows pineal-like morphology withfringe spacing of 0.48 nm, corresponding to the (111) latticeplanes of Fe\n3O4. The result indicates the possible oriented\nassembly of grain along (111) plane, which is the crystallo-\ngraphic plane with the highest energy and preferential fororiented attachment.51The structures and grain size of Fe3O4\nwere further measured by XRD, as shown in Figure 3. All thediﬀraction peaks at 18.32 ±0.03, 30.10 ±0.05, 35.48 ±0.03,\n43.10 ±0.02, 53.40 ±0.04, 57.02 ±0.05, and 62.58 ±0.08 °\ncan be indexed to the indices (111), (220), (311), (400),(422), (511), and (440) of Fe\n3O4. According to the Scherrer\nformula, the grain size gradually decreased from 34.4 nm(Fe\n3O4-2) to 13.4 nm (Fe 3O4-6) with the increase of PDDA' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 2}
page_content='43.10 ±0.02, 53.40 ±0.04, 57.02 ±0.05, and 62.58 ±0.08 °\ncan be indexed to the indices (111), (220), (311), (400),(422), (511), and (440) of Fe\n3O4. According to the Scherrer\nformula, the grain size gradually decreased from 34.4 nm(Fe\n3O4-2) to 13.4 nm (Fe 3O4-6) with the increase of PDDA\ndosage (as listed in Table 1), which indicates the feasibility ofthe PDDA-induced grain size tunable strategy. Brie ﬂy speaking,\nthe SEM, TEM, and XRD results show that PDDA modulated\nsolvothermal method successfully modulate products morphol-ogy, particle size, grain size, and facilitate the oriented grainassembly.\nOn the other hand, the surface area and pore size distribution\nof as-synthesized Fe\n3O4-i(i= 2, 4, 5, 6) were determined by\nnitrogen adsorption −desorption measurements, as shown in\nFigure 4. Fe3O4-6 (Figure 4D) synthesized with the highest\nPDDA dosage possesses surface area and pore volume of 32.75m\n2g−1and 0.12 cm3g−1, respectively, both of which are higher' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 2}
page_content='of as-synthesized Fe\n3O4-i(i= 2, 4, 5, 6) were determined by\nnitrogen adsorption −desorption measurements, as shown in\nFigure 4. Fe3O4-6 (Figure 4D) synthesized with the highest\nPDDA dosage possesses surface area and pore volume of 32.75m\n2g−1and 0.12 cm3g−1, respectively, both of which are higher\nthan that of the Fe 3O4-5 (31.16 cm2g−1and 0.117 cm3g−1,\nFigure 4C), Fe 3O4-4 (19.13 m2g−1and 0.07 cm3g−1, Figure\n4B) and Fe 3O4-2 (7.05 m2g−1and 0.015 cm3g−1, Figure 4A).\nAll the samples pose pore size in the range of 7 −12 nm. The\nresults above can be ascribed to the fact that smaller grainassembly possesses more channels, leading to the increasedsurface area and pore amount. Hence, increasing the PDDAdosage yields Fe\n3O4hierarchical particles composing of smaller\ngrain, which exhibit higher surface area and porosity.\nThe magnetic property of Fe 3O4hierarchical particles was' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 2}
page_content='3O4hierarchical particles composing of smaller\ngrain, which exhibit higher surface area and porosity.\nThe magnetic property of Fe 3O4hierarchical particles was\nevaluated by examining the magnetic hysteresis loops at roomtemperature, as shown in Figure S2 in the SupportingInformation. The M\nsfor the Fe 3O4-i(i=2−6) is in the\nrange of 50 −80 emu g−1, which is comparable with many\nreported high magnetic particles.31\nBrieﬂy speaking, hierarchical porous Fe 3O4particles with\nhigh magnetism were synthesized by facile PDDA-modulatedsolvothermal method, which is achieved in one-pot solutionreaction and avoids the time/energy consuming precursorcalcination process. Furthermore, PDDA-induced grain sizetunable strategy has been proved to be an e ﬃcient way to\nenhance the surface area and porosity of particles.\n3.2. Mechanism for the Formation of Fe\n3O4Hier-\narchical Particles Mediated by PDDA. The morphology' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 2}
page_content='enhance the surface area and porosity of particles.\n3.2. Mechanism for the Formation of Fe\n3O4Hier-\narchical Particles Mediated by PDDA. The morphology\nand structure of the products with initial PDDA dosage of 4 gat various reaction time were examined by TEM, FT-IR andXRD to preliminarily understand the morphology and structureevolution of Fe\n3O4hierarchical particles, as shown in Figure 5.\nTEM results give insight into the morphology evolution of\nmesoporous Fe 3O4. As shown in Figure 5A −F), three typical\nstages were observed for the formation of Fe 3O4, namely, the\nformation of spindle precursor with length of 5 −10 nm (0 −1.5\nh), the formation and assembly of grain to sphere particles(1.5−4 h), and the oriented assembly/Ostwald ripening\nFigure 1. SEM images and the corresponding hierarchical particle size\ndistribution of (A) Fe 3O4-1, (B) Fe 3O4-2, (C) Fe 3O4-3, (D) Fe 3O4-4,\n(E) Fe3O4-5, and (F) Fe3O4-6 at initial PDDA dosage varying from 1' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 2}
page_content='Figure 1. SEM images and the corresponding hierarchical particle size\ndistribution of (A) Fe 3O4-1, (B) Fe 3O4-2, (C) Fe 3O4-3, (D) Fe 3O4-4,\n(E) Fe3O4-5, and (F) Fe3O4-6 at initial PDDA dosage varying from 1\nto 6 g.ACS Applied Materials & Interfaces Research Article\ndx.doi.org/10.1021/am403533v |ACS Appl. Mater. Interfaces XXXX, XXX, XXX −XXX C' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 2}
page_content='process of preformed sphere into porous particles (4 −8 h). The\nXRD patterns (Figure 5G) of Fe 3O4-4 at 1.5h depict a strong\npeak at 7.55 °along with a broad weak one at 25 °probably\noriginated from (001) and (013) planes of an iron oxide acetatehydroxide hydrate with a formula of Fe\n2O(CH 3COO)(OH) 3·\nH2O according to JCPDS.69Then XRD patterns of the samples\nobtained at the time from 2.5 to 4 h show gradually enhancedpeaks at 30.00, 35.48, 43.14, 53.44, 57.04, and 62.58 °, marked\nby the indices (220), (311), (400), (422), (511), and (440) ofFe\n3O4phases. When the reaction time was 6 −8 h, the produced\naggregates were pure Fe3O4. The FT-IR spectra (Figure 5H) of\nFe3O4-1.5h and Fe3O4-2.5h show reduced absorption peaks at\n1578 and 1445 cm−1due to the asymmetric and symmetric\nstretching of COO−group, band at 1090 cm−1owing to C −O\nstretching of the COO−group, band at 887 cm−1due to the\nOH bending.69The spectra of the samples within 4 −8 h show' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 3}
page_content='Fe3O4-1.5h and Fe3O4-2.5h show reduced absorption peaks at\n1578 and 1445 cm−1due to the asymmetric and symmetric\nstretching of COO−group, band at 1090 cm−1owing to C −O\nstretching of the COO−group, band at 887 cm−1due to the\nOH bending.69The spectra of the samples within 4 −8 h show\nbroad strong band at 591 cm−1due to the Fe −O lattice mode\nof Fe3O4.34Except for the anticipated typical peak for ironcomposite (Fe 2O(CH 3COO)(OH) 3·H2Oo rF e 3O4), the peak\nat 1125 cm−1was ascribed to the C −N symmetric stretching\nvibration of PDDA. The PDDA also exhibits weak CH 2\nbending vibrations (around 1474, 1326, and 960 cm−1), C−\nH asymmetric, and C −H symmetric stretching frequencies\n(2918 and 2867 cm−1).70,71Thus, both the XRD and FT-IR\nindicate the gradual formation of Fe 3O4phase at the expense of\npreformed Fe 2O(CH 3COO)(OH) 3·H2O phase.\nBrieﬂy speaking, typical three stages were observed for the\nformation of Fe 3O4, namely, the formation of spindle precursor' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 3}
page_content='(2918 and 2867 cm−1).70,71Thus, both the XRD and FT-IR\nindicate the gradual formation of Fe 3O4phase at the expense of\npreformed Fe 2O(CH 3COO)(OH) 3·H2O phase.\nBrieﬂy speaking, typical three stages were observed for the\nformation of Fe 3O4, namely, the formation of spindle precursor\nwith length of 5 −10 nm (mainly composed of Fe 2O-\n(CH 3COO)(OH) 3·H2Oa t0 −1.5 h), the formation and\nassembly of grain to sphere Fe3O4particles (1.5 −4 h), and\nthe oriented assembly/Ostwald ripening process of preformedsphere into porous Fe\n3O4particles (4 −8 h), which are in\nagreement with the reported literatures.48,51,58,69,72FT-IR\nanalysis indicates that the hierarchical particles exhibit the\nvibration of PDDA which suggests the existence of PDDA on\nthe particle surface and therefore the potential capping functionof PDDA.\nFurthermore, to reveal the e ﬀect of PDDA dosage on the\nmorphology evolution, the amount was decreased from 4 to 2 gwith otherwise the same conditions above, as shown in Figure' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 3}
page_content='the particle surface and therefore the potential capping functionof PDDA.\nFurthermore, to reveal the e ﬀect of PDDA dosage on the\nmorphology evolution, the amount was decreased from 4 to 2 gwith otherwise the same conditions above, as shown in Figure\nS3 in the Supporting Information. The grains collected at 2.5, 4,\n5, 6, and 8 h for the synthesis adopting 4 g of PDDA increase\nfrom 20.1 to 23.6 nm, indicating grain size increase by 3.5 nm;\nwhile the increment of grain size for the synthesis with 2 g ofPDDA is 9.3 nm (increase from 21.5 to 30.8 nm). The results\nreﬂect that the increase in grain size was depressed as increasing\nthe PDDA dosage, which might be partially ascribed to the\ncapping e ﬀect of PDDA. On the other hand, it is discovered\nthat the viscosity of the reaction medium was enhanced withthe increase in PDDA dosage, as shown in Table 1. It is well-\naccepted that the increment of viscosity results in the promoted\nmass transfer resistance, which is not conductive to crystal' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 3}
page_content='that the viscosity of the reaction medium was enhanced withthe increase in PDDA dosage, as shown in Table 1. It is well-\naccepted that the increment of viscosity results in the promoted\nmass transfer resistance, which is not conductive to crystal\ngrowth, thus leading to the decreased grain size.\n54,73\nFigure 2. HRTEM images (A −C) of Fe3O4-4; B and C represent the magni ﬁcation of the red area in A.\nFigure 3. XRD patterns of Fe3O4particles obtained at di ﬀerent PDDA\ndosage: (a) 2 g, (b) 3 g, (c) 4 g, (d) 5 g, (e) 6 g, with other\nexperimental parameters keeping constant.\nTable 1. Viscosity of Reaction Medium, Particle Size, Grain Size, Magnetic Properties, And Absorption Performance of Fe 3O4-i\n(i=2−6) with di ﬀerent initial PDDA dosage\nsamplesPDDA dosage\n(g)viscosity of reaction medium\n(Pa S)particle size\n(nm)grain size\n(nm)magnetic property (emu\ng−1)qm(As(V))\n(mg g−1)qe(As(III))\n(mg g−1)\nFe3O4-2 2 0.062 ±0.002 350 ±15 34.4 ±0.8 74.42 ±1.93 1.93 1.57' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 3}
page_content='(i=2−6) with di ﬀerent initial PDDA dosage\nsamplesPDDA dosage\n(g)viscosity of reaction medium\n(Pa S)particle size\n(nm)grain size\n(nm)magnetic property (emu\ng−1)qm(As(V))\n(mg g−1)qe(As(III))\n(mg g−1)\nFe3O4-2 2 0.062 ±0.002 350 ±15 34.4 ±0.8 74.42 ±1.93 1.93 1.57\nFe3O4-3 3 0.082 ±0.004 215 ±12 28.1 ±0.6 72.11 ±4.15 2.31 1.99\nFe3O4-4 4 0.106 ±0.005 195 ±10 20.2 ±0.5 57.96 ±3.47 4.07 3.29\nFe3O4-5 5 0.127 ±0.004 185 ±10 14.4 ±0.3 53.96 ±2.86 6.35 6.06\nFe3O4-6 6 0.145 ±0.006 95 ±10 13.3 ±0.5 49.21 ±3.13 7.23 6.77ACS Applied Materials & Interfaces Research Article\ndx.doi.org/10.1021/am403533v |ACS Appl. Mater. Interfaces XXXX, XXX, XXX −XXX D' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 3}
page_content='Thus, based on the discussion above, a possible mechanism\nwas proposed to elucidate the PDDA-induced grain size tunablestrategy for the controllable synthesis of porous Fe\n3O4\nhierarchical particles. As shown in Scheme 1, a mixturecomposed of FeCl\n3, EG, NaAc, and PDDA was ﬁrst obtained\nand the viscosity of mixture was greatly enhanced by PDDA.Spindle particles were then obtained with PDDA as capping\nagents, which improved the particle dispersibility. As time goes\non, hierarchical Fe\n3O4particles were eventually produced, and\nmeanwhile PDDA function on capping e ﬀect and increasing\nviscosity declines particle and grain size, facilitates oriented\nassembly, thus synchronously enhancing surface area andporosity. Brie ﬂy speaking, a PDDA-modulated solvothermal\nmethod can controllably prepare porous Fe\n3O4hierarchical\nparticles.\n3.3. Arsenic Adsorption Performance of Fe3O4.As\ndiscussed above, as-obtained Fe 3O4samples exhibit high' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 4}
page_content='assembly, thus synchronously enhancing surface area andporosity. Brie ﬂy speaking, a PDDA-modulated solvothermal\nmethod can controllably prepare porous Fe\n3O4hierarchical\nparticles.\n3.3. Arsenic Adsorption Performance of Fe3O4.As\ndiscussed above, as-obtained Fe 3O4samples exhibit high\nspeciﬁc area, high porosity, and excellent magnetic property,\nall of which are generally regarded as desirable properties of\nadsorbent for the pollutants removal. Before arsenic adsorption,the dispersibility of aqueous Fe\n3O4samples was evaluated by\ndetermining the particle size via DLS. As shown in Figure S4 in\nthe Supporting Information, the size of Fe3O4shows a\nmonotonous decrease from 520 nm (A) to 180 nm (F) withPDDA dosage from 1 to 6 g. Considering the possible plus bias\nfor DLS compared with TEM/SEM,\n74,75the results indicate\ngood dispersibility for Fe 3O4suspension. Then, the adsorption\ncapacity of Fe 3O4-i(i=2−6) for As(V) and As(III) was' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 4}
page_content='monotonous decrease from 520 nm (A) to 180 nm (F) withPDDA dosage from 1 to 6 g. Considering the possible plus bias\nfor DLS compared with TEM/SEM,\n74,75the results indicate\ngood dispersibility for Fe 3O4suspension. Then, the adsorption\ncapacity of Fe 3O4-i(i=2−6) for As(V) and As(III) was\nevaluated using the equilibrium adsorption isotherm by varyingthe initial As(V) and As(III) concentrations. As shown in\nFigure 6A and C, the adsorption capacity of As(V) and As(III)\nmonotonously increased from 1.93 and 1.57 mg g−1for Fe 3O4-\n2 to 7.23 and 6.77 mg g−1for Fe 3O4-6, indicating that the\nmorphology mediated by PDDA greatly facilitates theabsorption performance of particles. Fe\n3O4-5 and Fe3O4-6 in\nour research exhibit higher adsorption capacity than commer-\ncial Fe 3O4particles (1.35 mg g−1for As(V) and 0.76 mg g−1for\nAs(III)) and some other reported metal oxide spheres, as listed\nin Table 2.28,31,76Moreover, in terms of the low contaminated' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 4}
page_content='3O4-5 and Fe3O4-6 in\nour research exhibit higher adsorption capacity than commer-\ncial Fe 3O4particles (1.35 mg g−1for As(V) and 0.76 mg g−1for\nAs(III)) and some other reported metal oxide spheres, as listed\nin Table 2.28,31,76Moreover, in terms of the low contaminated\ndrinking water (with hundreds of microgram per liter),66the\nprepared Fe 3O4exhibits excellent removal e ﬃciency. Exploiting\nFe3O4-5 as adsorbent, arsenic solution with initial As(V)\nconcentration ≤800 μgL−1or initial As(III) concentration\n≤300μgL−1can be detoxicated to drinking water standard of\nUSA (with arsenic concentration less than 10 μgL−1) (see\nFigure S5 in the Supporting Information), indicating the\npotential application in the low-level arsenic removal.\nTwo equations, Langmuir and Freundlich isotherms models,\nw e r eu s e dt oa n a l y z et h ee x p e r i m e n t a ld a t aa n dt h e\nmathematical expressions are depicted in eq 2 and 3,\nrespectively\n=+C\nqq KC\nq1 e\nem Le\nm (2)\n=+qncK log1log loge eF(3)' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 4}
page_content='Two equations, Langmuir and Freundlich isotherms models,\nw e r eu s e dt oa n a l y z et h ee x p e r i m e n t a ld a t aa n dt h e\nmathematical expressions are depicted in eq 2 and 3,\nrespectively\n=+C\nqq KC\nq1 e\nem Le\nm (2)\n=+qncK log1log loge eF(3)\nFigure 4. Nitrogen adsorption −desorption isotherms and pore-size distribution curves (the corresponding insert) of (A) as-obtained Fe 3O4-2, (B)\nFe3O4-4, (C) Fe3O4-5, and (D) Fe3O4-6, respectively.ACS Applied Materials & Interfaces Research Article\ndx.doi.org/10.1021/am403533v |ACS Appl. Mater. Interfaces XXXX, XXX, XXX −XXX E' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 4}
page_content='where qmand KLare Langmuir constants and represent the\nmaximum adsorption capacity of adsorbents (mg g−1) and the\nenergy of adsorption, respectively. KFand nare Freundlich\nconstants related to adsorption capacity and adsorptionintensity, respectively.\nFor the Langmuir isotherm model, the values of q\nmandKL\ncan be calculated from the slope and intercept of plots of ce/qe\nversus ce. For the Freundlich isotherm model, the values of n\nandKFcan be obtained by a plot of log qeagainst log ce. The\nadsorption of As(V) onto Fe 3O4ﬁts the Langmuir isotherm\nmodel (Figure 6B), which interprets the adsorption process as amonolayer adsorption on a homogeneous surface. In contrast,the adsorption of As(III) onto Fe\n3O4ﬁts well with the' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 5}
page_content='andKFcan be obtained by a plot of log qeagainst log ce. The\nadsorption of As(V) onto Fe 3O4ﬁts the Langmuir isotherm\nmodel (Figure 6B), which interprets the adsorption process as amonolayer adsorption on a homogeneous surface. In contrast,the adsorption of As(III) onto Fe\n3O4ﬁts well with the\nFreundlich isotherm model (Figure 6D), indicating that theadsorption process is a multilayer adsorption on a homoge-neous surface. The parameters of the Langmuir and Freundlichmodels were calculated and listed in Table S1 in the SupportingInformation. The two di ﬀerent adsorption isotherm models\nmay be attributed to the di ﬀerent surface charge e ﬀects of\nAs(V) and As(III) species under the environment of pH 5.\n77\nFor As(V), there exists an electrostatic attraction betweenpositively charged Fe\n3O4samples and negatively charged As(V)\nspecies and the adsorbed As(V) species have a repulsive e ﬀect\non As(V) species in the solution. However, As(III) existspredominantly as noncharged H' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 5}
page_content='77\nFor As(V), there exists an electrostatic attraction betweenpositively charged Fe\n3O4samples and negatively charged As(V)\nspecies and the adsorbed As(V) species have a repulsive e ﬀect\non As(V) species in the solution. However, As(III) existspredominantly as noncharged H\n3AsO 3.7The interaction\nbetween Fe 3O4samples and noncharged As(III) species is\nlittle so that the adsorption of As(III) should continue toincrease with the increase of As(III) concentration.\n77Moreover,\nsmaller 1/ nimplies stronger adsorption intensity.31Thus, the\ndecrease of 1/ nfrom Fe 3O4-2 to Fe 3O4-6 indicate that the\nadsorption process gradually becomes easier.\nThe kinetics of adsorption is one of the most important\ncharacteristics that de ﬁne the adsorption e ﬃciency, as shown in\nFigure 7. The adsorption of As(V) or As(III) is rapid at ﬁrst\nFigure 5. (A−F) TEM, (G) XRD, and (H) FT-IR spectra of Fe3O4-4-xh samples ( x= 1.5, 2.5, 4, 5, 6, 8).ACS Applied Materials & Interfaces Research Article' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 5}
page_content='characteristics that de ﬁne the adsorption e ﬃciency, as shown in\nFigure 7. The adsorption of As(V) or As(III) is rapid at ﬁrst\nFigure 5. (A−F) TEM, (G) XRD, and (H) FT-IR spectra of Fe3O4-4-xh samples ( x= 1.5, 2.5, 4, 5, 6, 8).ACS Applied Materials & Interfaces Research Article\ndx.doi.org/10.1021/am403533v |ACS Appl. Mater. Interfaces XXXX, XXX, XXX −XXX F' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 5}
page_content='and then slows considerably. The rapid adsorption at ﬁrst is\nascribed to the process of arsenic adsorption on the exterior\nsurface of the Fe 3O4particles. The slower adsorption rate\nfollowed might be partially due to the higher di ﬀusionresistance as the arsenic begins to enter and move into the\ninterior of the Fe 3O4particles via the nanopores.77Moreover,\nFe3O4-5 exhibits the highest removal e ﬃciency of 90.7% As(V)\nand 88.3% As(III), which are higher than Fe 3O4-4 (57.3%Scheme 1. Scheme of the Formation of Hierarchical Fe 3O4Particles Mediated by PDDA\nFigure 6. Adsorption isotherms of (A) As(V) and (C) As(III) onto (a) Fe 3O4-2, (b) Fe 3O4-3, (c) Fe 3O4-4, (d) Fe 3O4-5, and (e) Fe 3O4-6 samples\nwith the initial PDDA dosage of 2 g, 3 g, 4 g, 5 g, and 6 g, respectively. (B) Linearized Langmuir isotherm for As(V) adsorption and (D) linearizedFreundlich isotherm for As(III) adsorption by (a) Fe\n3O4-2, (b) Fe3O4-3, (c) Fe3O4-4, (d) Fe3O4-5, and (e) Fe3O4-6 samples ( T=2 5 °C; adsorbent' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 6}
page_content='with the initial PDDA dosage of 2 g, 3 g, 4 g, 5 g, and 6 g, respectively. (B) Linearized Langmuir isotherm for As(V) adsorption and (D) linearizedFreundlich isotherm for As(III) adsorption by (a) Fe\n3O4-2, (b) Fe3O4-3, (c) Fe3O4-4, (d) Fe3O4-5, and (e) Fe3O4-6 samples ( T=2 5 °C; adsorbent\ndoses = 0.5 g L−1;p H=5 ±0.2).ACS Applied Materials & Interfaces Research Article\ndx.doi.org/10.1021/am403533v |ACS Appl. Mater. Interfaces XXXX, XXX, XXX −XXX G' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 6}
page_content='As(V) and 37.5% As(III)) and Fe 3O4-3 (28.6% As(V) and\n19.2% As(III)). The better removal performances could beattributed to PDDA-induced high porous structure andincreased surface area of the prepared Fe\n3O4particles. All the\nabove adsorption kinetic experimental data can be best ﬁtted\ninto a pseudo-second-order rate kinetic model, which ispresented as follows\n=+t\nq kq qt11\ne t 2e2(4)\nwhere qeandqtare the amount of As(III) and As(V) adsorbed\nat equilibrium and at time t, respectively; k2is the rate constant\nof the pseudo-second-order model of adsorption (g mg−1\nmin−1). The values of k2andqecan be obtained by a plot of\n(t)/(qt) against t.\nThe conditions of As(V) and As(III) ions on the\nnanospheres were then characterized by XPS, as shown inFigure 8. Fe\n3O4with arsenic adsorbed was collected by\nmagnetic separation after the adsorption process (the initialarsenic concentration (As(V) and As(III): 5 mg L\n−1). As shown' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 7}
page_content='The conditions of As(V) and As(III) ions on the\nnanospheres were then characterized by XPS, as shown inFigure 8. Fe\n3O4with arsenic adsorbed was collected by\nmagnetic separation after the adsorption process (the initialarsenic concentration (As(V) and As(III): 5 mg L\n−1). As shown\nin the full-range XPS spectra (Figure 8A), the appearance of Asspecies and the increase of O intensity after arsenic adsorptionboth validate the arsenic adsorption on the Fe\n3O4surface. XPS\nof Fe2p of all samples (Figure 8B) show the binding energies ofFe2p\n1/2at 724.4 eV and Fe2p3/2at 710.5 eV, which are closed\nto that of Fe3O4reported.47,73XPS of As3d (Figure 8C) in\nFe3O4adsorbed As(V) shows a peak located at 45.1 eV,\nattributing to As(V) −O bonding, and that of As3d in Fe3O4Table 2. Comparison of the Adsorption Capacity of Arsenic\non Fe 3O4with Reported Inorganic Oxide\nadsorbent sample pHremoval\ncapacity for\nAs(V)\n(mg g−1)removal\ncapacity for\nAs(III)\n(mg g−1) ref\nFe3O4-5 5 6.35 6.06 this\nwork' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 7}
page_content='attributing to As(V) −O bonding, and that of As3d in Fe3O4Table 2. Comparison of the Adsorption Capacity of Arsenic\non Fe 3O4with Reported Inorganic Oxide\nadsorbent sample pHremoval\ncapacity for\nAs(V)\n(mg g−1)removal\ncapacity for\nAs(III)\n(mg g−1) ref\nFe3O4-5 5 6.35 6.06 this\nwork\nFe3O4-6 5 7.23 6.77 this\nwork\nCommercial Fe 3O45 1.35 0.76 this\nwork\nporous α-Fe2O3 4 5.3147\nporous γ-F e 2O3 4 4.7547\nfollow-like porous\nFe3O44 4.6547\nchestnutlike Fe 3O4\nhierarchicalnanstructure4 6.0731\ncubic nickel frames 7.1585\ndoughnut-like\nCuO4 4.776\nmultilayer spherical\nCuO4 0.576\nCommercial α-\nFe2O34 0.4647\ncommercial Fe 3O4\n(300 nm)4.8−8.0 1.08 1.5628\ncommercial CuO\nnanoparticles4 1.476\ncommercial TiO 2 4 4.1147\nFigure 7. (A, C) Adsorption rate of As(V) and As(III) by Fe3O4-3 (1), Fe3O4-4 (2), and Fe3O4-5 (3), samples. (B, D) Pseudo-second-order kinetic' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 7}
page_content='CuO4 0.576\nCommercial α-\nFe2O34 0.4647\ncommercial Fe 3O4\n(300 nm)4.8−8.0 1.08 1.5628\ncommercial CuO\nnanoparticles4 1.476\ncommercial TiO 2 4 4.1147\nFigure 7. (A, C) Adsorption rate of As(V) and As(III) by Fe3O4-3 (1), Fe3O4-4 (2), and Fe3O4-5 (3), samples. (B, D) Pseudo-second-order kinetic\nplots for the adsorption of As(V) and As(III). ( T=2 5 °C; adsorbent doses = 0.5 g L−1;p H=5 ±0.2).ACS Applied Materials & Interfaces Research Article\ndx.doi.org/10.1021/am403533v |ACS Appl. Mater. Interfaces XXXX, XXX, XXX −XXX H' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 7}
page_content='adsorbed As(III) shows ﬁtted peak located at 43.9 eV,\ncorresponding to As(III)-O, respectively.14,78−81The results\nconﬁrmed no major di ﬀerences in the valence state of the Fe\nand As species in arsenic adsorption. O1s XPS spectrum(Figure 8D) can be deconvoluted into peaks located at 530.0,531.5, and 533.0 eV, which are attributed to oxygen in the\nlattice (e.g., Fe −Oo rA s −O), oxygen atoms in the surface\nhydroxyl groups (H −O), and oxygen in the outermost layer of\nH\n2Oo rC O2adsorbed.14,79,82 −84The high peak intensity of\nH−O species of Fe 3O4conﬁrms the existence of many hydroxyl\ngroups on the surface of Fe 3O4spheres, which plays a vital\nimportant role in the arsenic removal.14Moreover, after arsenic\nadsorption, the shift of the O 1s binding energy to low energy,\nthe proportion decrease of the H −O (531.5 eV) and the\nproportion increase of O in the lattice (530.0 eV) suggest thatthe adsorption mechanism w a sm a i n l ya s c r i b e dt ot h e' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 8}
page_content='adsorption, the shift of the O 1s binding energy to low energy,\nthe proportion decrease of the H −O (531.5 eV) and the\nproportion increase of O in the lattice (530.0 eV) suggest thatthe adsorption mechanism w a sm a i n l ya s c r i b e dt ot h e\nsubstitution of Fe −OH groups by arsenic species.Moreover, the quick magnetic separation, high desorption\neﬃciency, and satisfactory recyclability of Fe\n3O4have been\ninvestigated, as shown in Figure 9. Taking Fe 3O4-5 as an\nexample, the Fe 3O4suspension possesses merits of not only\nquick magnetic separation (within 5 s) but also una ﬀected\nredispersion property (the size of Fe3O4-5 measured via DLS\nbefore and after magnetic separation are in the range of 240 −\n260 nm), which would greatly facilitate the application of Fe 3O4\nparticles in low-cost and high e ﬃcient water remediation. After\nthe recovery by magnetic separation, the Fe 3O4adsorbed\narsenic compounds could be treated by untrasoni ﬁcation and' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 8}
page_content='260 nm), which would greatly facilitate the application of Fe 3O4\nparticles in low-cost and high e ﬃcient water remediation. After\nthe recovery by magnetic separation, the Fe 3O4adsorbed\narsenic compounds could be treated by untrasoni ﬁcation and\nthen stirring in aqueous NaOH solution at pH 13 for 2 h,\nwhere upon they could be reused. It was found that the\ndesorption e ﬃciency was higher than 80% (86% for As(V) and\n92% for As(III)), and the removal e ﬃciency remained 85%\nafterﬁve cycles, which indicates the feasibility of regenerating\nthe Fe 3O4adsorbent.\nFigure 8. (A) Full-range, (B) Fe 2p, (C) As 3d, and (D) O 1s XPS spectra of several samples of interests including the Fe3O4,F e3O4adsorbed\nAs(V), and Fe 3O4adsorbed As(III) particles.\nFigure 9. (A) Separation/redispersion property of Fe3O4-5 under external magnetic ﬁeld (EM); (B) arsenic removal e ﬃciency of Fe3O4-5 particles\nin diﬀerent cycling numbers.ACS Applied Materials & Interfaces Research Article' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 8}
page_content='As(V), and Fe 3O4adsorbed As(III) particles.\nFigure 9. (A) Separation/redispersion property of Fe3O4-5 under external magnetic ﬁeld (EM); (B) arsenic removal e ﬃciency of Fe3O4-5 particles\nin diﬀerent cycling numbers.ACS Applied Materials & Interfaces Research Article\ndx.doi.org/10.1021/am403533v |ACS Appl. Mater. Interfaces XXXX, XXX, XXX −XXX I' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 8}
page_content='4. CONCLUSION\nA facile PDDA-modulated solvothermal method was proposed\nto synthesize porous hierarchical Fe 3O4particles with tunable\ngrain size. As the PDDA dosage increases, grain size andparticle size decrease, which yielded Fe\n3O4hierarchical particles\nwith enhanced surface area (from 7.05 to 32.75 cm3g−1) and\npromoted porosity (from 0.015 to 0.12 cm3g−1). Possible\nmechanism for PDDA-induced grain size tunable strategy canbe ascribed to capping e ﬀect and high reaction medium\nviscosity which mediate the growth and assembly of grain. Dueto the enhancement of surface area and high magnetismproperty, the prepared Fe\n3O4display improved arsenic\nadsorption performance, superior e ﬃciency in low-level arsenic\nremoval, high desorption e ﬃciency and satisfactory magnetic\nrecyclability, which are very promising compared withcommercial Fe\n3O4particles. The porous Fe3O4particles also\nprocess promising applications in other research ﬁeld such as' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 9}
page_content='adsorption performance, superior e ﬃciency in low-level arsenic\nremoval, high desorption e ﬃciency and satisfactory magnetic\nrecyclability, which are very promising compared withcommercial Fe\n3O4particles. The porous Fe3O4particles also\nprocess promising applications in other research ﬁeld such as\nbioseparation, targeted drug delivery, and catalysis. Moreover,as generally believed that building blocks assemble intohierarchical materials, this methodology, modulating theproperty of building blocks, is facile and potentially generalfor controllably synthesizing hierarchical materials with highapplication performance.\n■ASSOCIATED CONTENT\n*SSupporting Information\nAdditional information as noted in text. This material isavailable free of charge via the Internet at http://pubs.acs.org.\n■AUTHOR INFORMATION\nCorresponding Author\n*E-mail: Lychai@csu.edu.cn. Tel./Fax: +86 0731 88710171.\nAuthor Contributions\nThe authors declare no competing ﬁnancial interest. Ting' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 9}
page_content='Additional information as noted in text. This material isavailable free of charge via the Internet at http://pubs.acs.org.\n■AUTHOR INFORMATION\nCorresponding Author\n*E-mail: Lychai@csu.edu.cn. Tel./Fax: +86 0731 88710171.\nAuthor Contributions\nThe authors declare no competing ﬁnancial interest. Ting\nWang and Liyuan Zhang contributed equally to this work.\nNotes\nThe authors declare no competing ﬁnancial interest.■ACKNOWLEDGMENTS\nWe are thankful for the ﬁnancial support by National Science\nFound for Distinguished Young Scholars of China (50925417),Chang Jiang Scholars Program (T2011116), National PublicWelfare Research Project of Environmental ProtectionIndustrial (2011467062), and Key Technology for theRemediation of Arsenic Pollution in Xiangjiang River Basin(K1201010-61).\n■REFERENCES\n(1) Stone, R. Science 2008 ,321(5886), 184 −185.\n(2) Yu, X.-Y.; Xu, R.-X.; Gao, C.; Luo, T.; Jia, Y.; Liu, J.-H.; Huang,\nX.-J. ACS Appl. Mater. Interf. 2012 ,4(4), 1954 −1962.' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 9}
page_content='■REFERENCES\n(1) Stone, R. Science 2008 ,321(5886), 184 −185.\n(2) Yu, X.-Y.; Xu, R.-X.; Gao, C.; Luo, T.; Jia, Y.; Liu, J.-H.; Huang,\nX.-J. ACS Appl. Mater. Interf. 2012 ,4(4), 1954 −1962.\n(3) Manning, B. A.; Hunt, M. L.; Amrhein, C.; Yarmoff, J. A. Environ.\nSci. Technol. 2002 ,36(24), 5455 −5461.\n(4) Gupta, A.; Yunus, M.; Sankararamakrishnan, N. Ind. Eng. Chem.\nRes.2013 ,52(5), 2066 −2072.\n(5) Gihring, T. M.; Druschel, G. K.; McCleskey, R. B.; Hamers, R. J.;\nBanfield, J. F. Environ. Sci. Technol. 2001 ,35(19), 3857 −3862.\n(6) van Genuchten, C. M.; Addy, S. E.; Pen ̃a, J.; Gadgil, A. J. Environ.\nSci. Technol. 2012 ,46(2), 986 −994.\n(7) Hang, C.; Li, Q.; Gao, S.; Shang, J. K. Ind. Eng. Chem. Res. 2011 ,\n51(1), 353 −361.\n(8) Lafferty, B. J.; Ginder-Vogel, M.; Sparks, D. L. Environ. Sci.\nTechnol. 2011 ,45(21), 9218 −9223.(9) Shipley, H. J.; Yean, S.; Kan, A. T.; Tomson, M. B. Environ.\nToxicol. Chem. 2009 ,28(3), 509 −515.\n(10) Kim, J.; Benjamin, M. M. Water Res. 2004 ,38(8), 2053 −2062.' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 9}
page_content='51(1), 353 −361.\n(8) Lafferty, B. J.; Ginder-Vogel, M.; Sparks, D. L. Environ. Sci.\nTechnol. 2011 ,45(21), 9218 −9223.(9) Shipley, H. J.; Yean, S.; Kan, A. T.; Tomson, M. B. Environ.\nToxicol. Chem. 2009 ,28(3), 509 −515.\n(10) Kim, J.; Benjamin, M. M. Water Res. 2004 ,38(8), 2053 −2062.\n(11) Coronell, O.; Mi, B.; Marin ̃as, B. J.; Cahill, D. G. Environ. Sci.\nTechnol. 2012 ,47(1), 420 −428.\n(12) Hristovski, K. D.; Westerhoff, P. K.; Crittenden, J. C.; Olson, L.\nW.Environ. Sci. Technol. 2008 ,42(10), 3786 −3790.\n(13) Xu, W.; Wang, J.; Wang, L.; Sheng, G.; Liu, J.; Yu, H.; Huang,\nX.-J. J. Hazard. Mater. 2013 ,260(0), 498 −507.\n(14) Cao, C.-Y.; Qu, J.; Yan, W.-S.; Zhu, J.-F.; Wu, Z.-Y.; Song, W.-G.\nLangmuir 2012 ,28(9), 4573 −4579.\n(15) Yang, W.; Kan, A. T.; Chen, W.; Tomson, M. B. Water Res.\n2010 ,44(19), 5693 −5701.\n(16) Ai, Z.; Gao, Z.; Zhang, L.; He, W.; Yin, J. J. Environ. Sci. Technol.\n2013 ,47(10), 5344 −5352.\n(17) Wang, Y.; Zou, B.; Gao, T.; Wu, X.; Lou, S.; Zhou, S. J. Mater.' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 9}
page_content='Langmuir 2012 ,28(9), 4573 −4579.\n(15) Yang, W.; Kan, A. T.; Chen, W.; Tomson, M. B. Water Res.\n2010 ,44(19), 5693 −5701.\n(16) Ai, Z.; Gao, Z.; Zhang, L.; He, W.; Yin, J. J. Environ. Sci. Technol.\n2013 ,47(10), 5344 −5352.\n(17) Wang, Y.; Zou, B.; Gao, T.; Wu, X.; Lou, S.; Zhou, S. J. Mater.\nChem. 2012 ,22(18), 9034 −9040.\n(18) Khin, M. M.; Nair, S.; babu Veluru, J.; Rajendiran, M.;\nRamakrishna, S. Energy Environ. Sci 2012 ,5(8), 8075 −8109.\n(19) Wang, Z.; Wu, L.; Zhou, J.; Cai, W.; Shen, B.; Jiang, Z. J. Phys.\nChem. C 2013 ,117(10), 5446 −5452.\n(20) Valtchev, V.; Tosheva, L. Chem. Rev. 2013 .\n(21) Pang, X.; Zhao, L.; Han, W.; Xin, X.; Lin, Z. Nat. Nanotechnol.\n2013 ,8(6), 426 −431.\n(22) Liu, J.; Xu, J.; Che, R.; Chen, H.; Liu, M.; Liu, Z. Chem.\ue0d5Eur. J.\n2013 ,19(21), 6746 −6752.\n(23) Wang, L.; Huang, Y.; Kan, A. T.; Tomson, M. B.; Chen, W.\nEnviron. Sci. Technol. 2012 ,46(10), 5422 −5429.\n(24) Yang, L.; Luo, S.; Li, Y.; Xiao, Y.; Kang, Q.; Cai, Q. Environ. Sci.' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 9}
page_content='(22) Liu, J.; Xu, J.; Che, R.; Chen, H.; Liu, M.; Liu, Z. Chem.\ue0d5Eur. J.\n2013 ,19(21), 6746 −6752.\n(23) Wang, L.; Huang, Y.; Kan, A. T.; Tomson, M. B.; Chen, W.\nEnviron. Sci. Technol. 2012 ,46(10), 5422 −5429.\n(24) Yang, L.; Luo, S.; Li, Y.; Xiao, Y.; Kang, Q.; Cai, Q. Environ. Sci.\nTechnol. 2010 ,44(19), 7641 −7646.\n(25) Luo, X.-B.; Deng, F.; Min, L.; Luo, S.-L.; Guo, B.; Zeng, G.; Au,\nC.Environ. Sci. Technol. 2013 .\n(26) Yavuz, C. T.; Mayo, J.; William, W. Y.; Prakash, A.; Falkner, J.\nC.; Yean, S.; Cong, L.; Shipley, H. J.; Kan, A.; Tomson, M. Science\n2006 ,314(5801), 964 −967.\n(27) Zeng, H.; Singh, A.; Basak, S.; Ulrich, K.-U.; Sahu, M.; Biswas,\nP.; Catalano, J. G.; Giammar, D. E. Environ. Sci. Technol. 2009 ,43(5),\n1373−1378.\n(28) Yean, S.; Cong, L.; Yavuz, C.; Mayo, J.; Yu, W.; Kan, A.; Colvin,\nV.; Tomson, M. J. Mater. Res. 2005 ,20(12), 3255 −3264.\n(29) Cotten, G. B.; Eldredge, H. B. Sep. Sci. Technol. 2002 ,37(16),\n3755−3779.' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 9}
page_content='P.; Catalano, J. G.; Giammar, D. E. Environ. Sci. Technol. 2009 ,43(5),\n1373−1378.\n(28) Yean, S.; Cong, L.; Yavuz, C.; Mayo, J.; Yu, W.; Kan, A.; Colvin,\nV.; Tomson, M. J. Mater. Res. 2005 ,20(12), 3255 −3264.\n(29) Cotten, G. B.; Eldredge, H. B. Sep. Sci. Technol. 2002 ,37(16),\n3755−3779.\n(30) Kelland, D. R. IEEE Trans. Magn. 1998 ,34(4), 2123 −2125.\n(31) Mou, F.; Guan, J.; Ma, H.; Xu, L.; Shi, W. ACS Appl. Mater.\nInterfaces 2012 ,4(8), 3987 −3993.\n(32) Ge, J.; Huynh, T.; Hu, Y.; Yin, Y. Nano Lett. 2008 ,8(3), 931 −\n934.\n(33) Wei, Z.; Xing, R.; Zhang, X.; Liu, S.; Yu, H.; Li, P. ACS Appl.\nMater. Interfaces 2012 ,5(3), 598 −604.\n(34) Liu, G.; Deng, Q.; Wang, H.; Kang, S.; Yang, Y.; Ng, D. H.; Cai,\nW.; Wang, G. Chem.\ue0d5Eur. J. 2012 ,18(42), 13418 −13426.\n(35) Mou, F.; Guan, J.; Xiao, Z.; Sun, Z.; Shi, W.; Fan, X.-a. J. Mater.\nChem. 2011 ,21(14), 5414 −5421.\n(36) Wang, P.; Lo, I. Water Res. 2009 ,43(15), 3727 −3734.\n(37) Xuan, S.; Wang, F.; Lai, J. M.; Sham, K. W.; Wang, Y.-X. J.; Lee,' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 9}
page_content='W.; Wang, G. Chem.\ue0d5Eur. J. 2012 ,18(42), 13418 −13426.\n(35) Mou, F.; Guan, J.; Xiao, Z.; Sun, Z.; Shi, W.; Fan, X.-a. J. Mater.\nChem. 2011 ,21(14), 5414 −5421.\n(36) Wang, P.; Lo, I. Water Res. 2009 ,43(15), 3727 −3734.\n(37) Xuan, S.; Wang, F.; Lai, J. M.; Sham, K. W.; Wang, Y.-X. J.; Lee,\nS.-F.; Yu, J. C.; Cheng, C. H.; Leung, K. C.-F. ACS Appl. Mater.\nInterfaces 2011 ,3(2), 237 −244.\n(38) Ren, H.; Zhang, L.; Wang, T. T.; Li, L.; Wang, C. Chem.\nCommun. 2013 .\n(39) Lou, X. W.; Archer, L. A. Adv. Mater. 2008 ,20(10), 1853 −\n1858.\n(40) Liu, Y.; Wang, Y.; Zhou, S.; Lou, S.; Yuan, L.; Gao, T.; Wu, X.;\nShi, X.; Wang, K. ACS Appl. Mater. Interfaces 2012 ,4(9), 4913 −4920.\n(41) Wang, B.; Chen, J. S.; Wu, H. B.; Wang, Z.; Lou, X. W. J. Am.\nChem. Soc. 2011 ,133(43), 17146 −17148.\n(42) Peng, Q.; Dong, Y.; Li, Y. Angew. Chem., Int. Ed. 2003 ,42(26),\n3027−3030.\n(43) Li, X.; Si, Z.; Lei, Y.; Li, X.; Tang, J.; Song, S.; Zhang, H.' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 9}
page_content='(41) Wang, B.; Chen, J. S.; Wu, H. B.; Wang, Z.; Lou, X. W. J. Am.\nChem. Soc. 2011 ,133(43), 17146 −17148.\n(42) Peng, Q.; Dong, Y.; Li, Y. Angew. Chem., Int. Ed. 2003 ,42(26),\n3027−3030.\n(43) Li, X.; Si, Z.; Lei, Y.; Li, X.; Tang, J.; Song, S.; Zhang, H.\nCrystEngComm 2011 ,13(2), 642 −648.ACS Applied Materials & Interfaces Research Article\ndx.doi.org/10.1021/am403533v |ACS Appl. Mater. Interfaces XXXX, XXX, XXX −XXX J' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 9}
page_content='(44) Li, S.; Zhang, H.; Wu, J.; Ma, X.; Yang, D. Cryst. Growth Des.\n2006 ,6(2), 351 −353.\n(45) Liu, S.; Xing, R.; Lu, F.; Rana, R. K.; Zhu, J.-J. J. Phys. Chem. C\n2009 ,113(50), 21042 −21047.\n(46) Gao, Q.; Zhao, A.; Gan, Z.; Tao, W.; Li, D.; Zhang, M.; Guo, H.;\nWang, D.; Sun, H.; Mao, R. CrystEngComm 2012 ,14(14), 4834 −\n4842.\n(47) Zhong, L. S.; Hu, J. S.; Liang, H. P.; Cao, A. M.; Song, W. G.;\nWan, L. J. Adv. Mater. 2006 ,18(18), 2426 −2431.\n(48) Lian, J.; Duan, X.; Ma, J.; Peng, P.; Kim, T.; Zheng, W. ACS\nNano 2009 ,3(11), 3749 −3761.\n(49) Xuan, S.; Wang, Y.-X. J.; Yu, J. C.; Cham-Fai Leung, K. Chem.\nMater. 2009 ,21(21), 5079 −5087.\n(50) Zhu, Y.; Zhao, W.; Chen, H.; Shi, J. J. Phys. Chem. C 2007 ,111\n(14), 5281 −5285.\n(51) Jia, B.; Gao, L. J. Phys. Chem. C 2008 ,112(3), 666 −671.\n(52) Peng, S.; Sun, S. Angew. Chem. 2007 ,119(22), 4233 −4236.\n(53) Yin, Y.; Rioux, R. M.; Erdonmez, C. K.; Hughes, S.; Somorjai, G.\nA.; Alivisatos, A. P. Science 2004 ,304(5671), 711 −714.' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 10}
page_content='(14), 5281 −5285.\n(51) Jia, B.; Gao, L. J. Phys. Chem. C 2008 ,112(3), 666 −671.\n(52) Peng, S.; Sun, S. Angew. Chem. 2007 ,119(22), 4233 −4236.\n(53) Yin, Y.; Rioux, R. M.; Erdonmez, C. K.; Hughes, S.; Somorjai, G.\nA.; Alivisatos, A. P. Science 2004 ,304(5671), 711 −714.\n(54) Cheng, W.; Tang, K.; Qi, Y.; Sheng, J.; Liu, Z. J. Mater. Chem.\n2010 ,20(9), 1799 −1805.\n(55) Yang, H. G.; Zeng, H. C. J. Phys. Chem. B 2004 ,108(11),\n3492−3495.\n(56) Hu, P.; Yu, L.; Zuo, A.; Guo, C.; Yuan, F. J. Phys. Chem. C 2008 ,\n113(3), 900 −906.\n(57) Zhu, L.-P.; Xiao, H.-M.; Zhang, W.-D.; Yang, G.; Fu, S.-Y. Cryst.\nGrowth Des. 2008 ,8(3), 957 −963.\n(58) Chen, Y.; Xia, H.; Lu, L.; Xue, J. J. Mater. Chem. 2012 ,22(11),\n5006−5012.\n(59) Yang, H. G.; Zeng, H. C. Angew. Chem. 2004 ,116(44), 6056 −\n6059.\n(60) Wang, Y.; Zhu, Q.; Tao, L. CrystEngComm 2011 ,13(14),\n4652−4657.\n(61) Zhang, S.; Shao, Y.; Liao, H.; Engelhard, M. H.; Yin, G.; Lin, Y.\nACS Nano 2011 ,5(3), 1785 −1791.' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 10}
page_content='5006−5012.\n(59) Yang, H. G.; Zeng, H. C. Angew. Chem. 2004 ,116(44), 6056 −\n6059.\n(60) Wang, Y.; Zhu, Q.; Tao, L. CrystEngComm 2011 ,13(14),\n4652−4657.\n(61) Zhang, S.; Shao, Y.; Liao, H.; Engelhard, M. H.; Yin, G.; Lin, Y.\nACS Nano 2011 ,5(3), 1785 −1791.\n(62) Ren, W.; Fang, Y.; Wang, E. ACS Nano 2011 ,5(8), 6425 −6433.\n(63) Qin, C.; Chen, C.; Xie, Q.; Wang, L.; He, X.; Huang, Y.; Zhou,\nY.; Xie, F.; Yang, D.; Yao, S. Anal. Chim. Acta 2012 ,720,4 9−56.\n(64) Wang, S.; Yu, D.; Dai, L. J. Am. Chem. Soc. 2011 ,133(14),\n5182−5185.\n(65) Wang, S.; Yu, D.; Dai, L.; Chang, D. W.; Baek, J.-B. ACS Nano\n2011 ,5(8), 6202 −6209.\n(66) Mohan, D.; Pittman, C. U., Jr. J. Hazard. Mater. 2007 ,142(1),\n1−53.\n(67) Addo Ntim, S.; Mitra, S. J. Chem. Eng. Data 2011 ,56(5), 2077 −\n2083.\n(68) Katsoyiannis, I. A.; Zouboulis, A. I. Water Res. 2002 ,36(20),\n5141−5155.\n(69) Fan, T.; Pan, D.; Zhang, H. Ind. Eng. Chem. Res. 2011 ,50(15),\n9009−9018.\n(70) Liu, Z.-h.; Yang, X.; Makita, Y.; Ooi, K. Chem. Mater. 2002 ,14' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 10}
page_content='2083.\n(68) Katsoyiannis, I. A.; Zouboulis, A. I. Water Res. 2002 ,36(20),\n5141−5155.\n(69) Fan, T.; Pan, D.; Zhang, H. Ind. Eng. Chem. Res. 2011 ,50(15),\n9009−9018.\n(70) Liu, Z.-h.; Yang, X.; Makita, Y.; Ooi, K. Chem. Mater. 2002 ,14\n(11), 4800 −4806.\n(71) Liu, K.; Zhang, J.; Yang, G.; Wang, C.; Zhu, J.-J. Electrochem.\nCommun. 2010 ,12(3), 402 −405.\n(72) Zhu, M.; Diao, G. J. Phys. Chem. C 2011 ,115(39), 18923 −\n18934.\n(73) Zhu, H.; Hou, C.; Li, Y.; Zhao, G.; Liu, X.; Hou, K.; Li, Y. Chem.\nAsian J. 2013 ,8(7), 1447 −1454.\n(74) Lesniak, W.; Bielinska, A. U.; Sun, K.; Janczak, K. W.; Shi, X.;\nBaker, J. R.; Balogh, L. P. Nano Lett. 2005 ,5(11), 2123 −2130.\n(75) Cumberland, S. A.; Lead, J. R. J. Chromatogr. A 2009 ,1216 (52),\n9099−9105.\n(76) Cao, A.-m.; Monnell, J. D.; Matranga, C.; Wu, J.-m.; Cao, L.-l.;\nGao, D. J. Phys. Chem. C 2007 ,111(50), 18624 −18628.\n(77) Yu, X.-Y.; Luo, T.; Jia, Y.; Zhang, Y.-X.; Liu, J.-H.; Huang, X.-J. J.\nPhys. Chem. C 2011 ,115(45), 22242 −22250.' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 10}
page_content='9099−9105.\n(76) Cao, A.-m.; Monnell, J. D.; Matranga, C.; Wu, J.-m.; Cao, L.-l.;\nGao, D. J. Phys. Chem. C 2007 ,111(50), 18624 −18628.\n(77) Yu, X.-Y.; Luo, T.; Jia, Y.; Zhang, Y.-X.; Liu, J.-H.; Huang, X.-J. J.\nPhys. Chem. C 2011 ,115(45), 22242 −22250.\n(78) Kanel, S. R.; Greneche, J.-M.; Choi, H. Environ. Sci. Technol.\n2006 ,40(6), 2045 −2050.\n(79) Nesbitt, H.; Muir, I. Mineral. Petrol. 1998 ,62(1−2), 123 −144.(80) Gomes, J. A.; Daida, P.; Kesmez, M.; Weir, M.; Moreno, H.;\nParga, J. R.; Irwin, G.; McWhinney, H.; Grady, T.; Peterson, E. J.\nHazard. Mater. 2007 ,139(2), 220 −231.\n(81) Chen, B.; Zhu, Z.-L.; Ma, J.; Qiu, Y.-L.; Chen, J. J. Mater. Chem.\nA2013 ,1(37), 11355 −11367.\n(82) Wielant, J.; Hauffman, T.; Blajiev, O.; Hausbrand, R.; Terryn, H.\nJ. Phys. Chem. C 2007 ,111(35), 13177 −13184.\n(83) Ramos, M. A.; Yan, W.; Li, X.-q.; Koel, B. E.; Zhang, W.-x. J.\nPhys. Chem. C 2009 ,113(33), 14591 −14594.\n(84) Lim, S.-F.; Zheng, Y.-M.; Chen, J. P. Langmuir 2009 ,25(9),\n4973−4978.' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 10}
page_content='J. Phys. Chem. C 2007 ,111(35), 13177 −13184.\n(83) Ramos, M. A.; Yan, W.; Li, X.-q.; Koel, B. E.; Zhang, W.-x. J.\nPhys. Chem. C 2009 ,113(33), 14591 −14594.\n(84) Lim, S.-F.; Zheng, Y.-M.; Chen, J. P. Langmuir 2009 ,25(9),\n4973−4978.\n(85) Zheng, J. Y.; Wang, X.; Li, W.; Cao, Z.; Wang, H.; Zhang, C.;\nSong, W.-G.; Ma, Y.; Yao, J. CrystEngComm 2012 ,14(22), 7616 −\n7620.ACS Applied Materials & Interfaces Research Article\ndx.doi.org/10.1021/am403533v |ACS Appl. Mater. Interfaces XXXX, XXX, XXX −XXX K' metadata={'source': 'private_upload/2023-08-09-17/Controllable synthesis of hierarchical porous Fe3O4 particles mediated by Polydiallyldimethylammonium chloride an.pdf', 'page': 10}
filepath=private_upload/2023-08-09-17/LLaMa.pdf,len=128
page_content='LLaMA: Open and Efﬁcient Foundation Language Models\nHugo Touvron\x03, Thibaut Lavril\x03, Gautier Izacard\x03, Xavier Martinet\nMarie-Anne Lachaux, Timothee Lacroix, Baptiste Rozière, Naman Goyal\nEric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin\nEdouard Grave\x03, Guillaume Lample\x03\nMeta AI\nAbstract\nWe introduce LLaMA, a collection of founda-\ntion language models ranging from 7B to 65B\nparameters. We train our models on trillions\nof tokens, and show that it is possible to train\nstate-of-the-art models using publicly avail-\nable datasets exclusively, without resorting\nto proprietary and inaccessible datasets. In\nparticular, LLaMA-13B outperforms GPT-3\n(175B) on most benchmarks, and LLaMA-\n65B is competitive with the best models,\nChinchilla-70B and PaLM-540B. We release\nall our models to the research community1.\n1 Introduction\nLarge Languages Models (LLMs) trained on mas-\nsive corpora of texts have shown their ability to per-\nform new tasks from textual instructions or from a' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 0}
page_content='65B is competitive with the best models,\nChinchilla-70B and PaLM-540B. We release\nall our models to the research community1.\n1 Introduction\nLarge Languages Models (LLMs) trained on mas-\nsive corpora of texts have shown their ability to per-\nform new tasks from textual instructions or from a\nfew examples (Brown et al., 2020). These few-shot\nproperties ﬁrst appeared when scaling models to a\nsufﬁcient size (Kaplan et al., 2020), resulting in a\nline of work that focuses on further scaling these\nmodels (Chowdhery et al., 2022; Rae et al., 2021).\nThese efforts are based on the assumption that\nmore parameters will lead to better performance.\nHowever, recent work from Hoffmann et al. (2022)\nshows that, for a given compute budget, the best\nperformances are not achieved by the largest mod-\nels, but by smaller models trained on more data.\nThe objective of the scaling laws from Hoff-\nmann et al. (2022) is to determine how to best\nscale the dataset and model sizes for a particular' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 0}
page_content='shows that, for a given compute budget, the best\nperformances are not achieved by the largest mod-\nels, but by smaller models trained on more data.\nThe objective of the scaling laws from Hoff-\nmann et al. (2022) is to determine how to best\nscale the dataset and model sizes for a particular\ntraining compute budget. However, this objective\ndisregards the inference budget, which becomes\ncritical when serving a language model at scale.\nIn this context, given a target level of performance,\nthe preferred model is not the fastest to train but the\nfastest at inference, and although it may be cheaper\nto train a large model to reach a certain level of\n\x03Equal contribution. Correspondence: {htouvron,\nthibautlav,gizacard,egrave,glample}@meta.com\n1https://github.com/facebookresearch/llamaperformance, a smaller one trained longer will\nultimately be cheaper at inference. For instance,\nalthough Hoffmann et al. (2022) recommends\ntraining a 10B model on 200B tokens, we ﬁnd' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 0}
page_content='thibautlav,gizacard,egrave,glample}@meta.com\n1https://github.com/facebookresearch/llamaperformance, a smaller one trained longer will\nultimately be cheaper at inference. For instance,\nalthough Hoffmann et al. (2022) recommends\ntraining a 10B model on 200B tokens, we ﬁnd\nthat the performance of a 7B model continues to\nimprove even after 1T tokens.\nThe focus of this work is to train a series of\nlanguage models that achieve the best possible per-\nformance at various inference budgets, by training\non more tokens than what is typically used. The\nresulting models, called LLaMA , ranges from 7B\nto 65B parameters with competitive performance\ncompared to the best existing LLMs. For instance,\nLLaMA-13B outperforms GPT-3 on most bench-\nmarks, despite being 10 \x02smaller. We believe that\nthis model will help democratize the access and\nstudy of LLMs, since it can be run on a single GPU.\nAt the higher-end of the scale, our 65B-parameter\nmodel is also competitive with the best large lan-' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 0}
page_content='LLaMA-13B outperforms GPT-3 on most bench-\nmarks, despite being 10 \x02smaller. We believe that\nthis model will help democratize the access and\nstudy of LLMs, since it can be run on a single GPU.\nAt the higher-end of the scale, our 65B-parameter\nmodel is also competitive with the best large lan-\nguage models such as Chinchilla or PaLM-540B.\nUnlike Chinchilla, PaLM, or GPT-3, we only\nuse publicly available data, making our work com-\npatible with open-sourcing, while most existing\nmodels rely on data which is either not publicly\navailable or undocumented (e.g. “Books – 2TB” or\n“Social media conversations”). There exist some\nexceptions, notably OPT (Zhang et al., 2022),\nGPT-NeoX (Black et al., 2022), BLOOM (Scao\net al., 2022) and GLM (Zeng et al., 2022), but none\nthat are competitive with PaLM-62B or Chinchilla.\nIn the rest of this paper, we present an overview\nof the modiﬁcations we made to the transformer\narchitecture (Vaswani et al., 2017), as well as our' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 0}
page_content='GPT-NeoX (Black et al., 2022), BLOOM (Scao\net al., 2022) and GLM (Zeng et al., 2022), but none\nthat are competitive with PaLM-62B or Chinchilla.\nIn the rest of this paper, we present an overview\nof the modiﬁcations we made to the transformer\narchitecture (Vaswani et al., 2017), as well as our\ntraining method. We then report the performance of\nour models and compare with others LLMs on a set\nof standard benchmarks. Finally, we expose some\nof the biases and toxicity encoded in our models,\nusing some of the most recent benchmarks from\nthe responsible AI community.arXiv:2302.13971v1  [cs.CL]  27 Feb 2023' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 0}
page_content='2 Approach\nOur training approach is similar to the methods\ndescribed in previous work (Brown et al., 2020;\nChowdhery et al., 2022), and is inspired by the\nChinchilla scaling laws (Hoffmann et al., 2022).\nWe train large transformers on a large quantity of\ntextual data using a standard optimizer.\n2.1 Pre-training Data\nOur training dataset is a mixture of several sources,\nreported in Table 1, that cover a diverse set of do-\nmains. For the most part, we reuse data sources\nthat have been leveraged to train other LLMs, with\nthe restriction of only using data that is publicly\navailable, and compatible with open sourcing. This\nleads to the following mixture of data and the per-\ncentage they represent in the training set:\nEnglish CommonCrawl [67%]. We preprocess\nﬁve CommonCrawl dumps, ranging from 2017\nto 2020, with the CCNet pipeline (Wenzek et al.,\n2020). This process deduplicates the data at the\nline level, performs language identiﬁcation with' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 1}
page_content='centage they represent in the training set:\nEnglish CommonCrawl [67%]. We preprocess\nﬁve CommonCrawl dumps, ranging from 2017\nto 2020, with the CCNet pipeline (Wenzek et al.,\n2020). This process deduplicates the data at the\nline level, performs language identiﬁcation with\na fastText linear classiﬁer to remove non-English\npages and ﬁlters low quality content with an n-\ngram language model. In addition, we trained a\nlinear model to classify pages used as references\nin Wikipedia v.s.randomly sampled pages, and\ndiscarded pages not classiﬁed as references.\nC4 [15%]. During exploratory experiments, we\nobserved that using diverse pre-processed Com-\nmonCrawl datasets improves performance. We thus\nincluded the publicly available C4 dataset (Raffel\net al., 2020) in our data. The preprocessing of C4\nalso contains deduplication and language identiﬁ-\ncation steps: the main difference with CCNet is\nthe quality ﬁltering, which mostly relies on heuris-' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 1}
page_content='included the publicly available C4 dataset (Raffel\net al., 2020) in our data. The preprocessing of C4\nalso contains deduplication and language identiﬁ-\ncation steps: the main difference with CCNet is\nthe quality ﬁltering, which mostly relies on heuris-\ntics such as presence of punctuation marks or the\nnumber of words and sentences in a webpage.\nGithub [4.5%]. We use the public GitHub\ndataset available on Google BigQuery. We only\nkept projects that are distributed under the Apache,\nBSD and MIT licenses. Additionally, we ﬁltered\nlow quality ﬁles with heuristics based on the line\nlength or proportion of alphanumeric characters,\nand removed boilerplate, such as headers, with reg-\nular expressions. Finally, we deduplicate the result-\ning dataset at the ﬁle level, with exact matches.\nWikipedia [4.5%]. We add Wikipedia dumps\nfrom the June-August 2022 period, covering 20Dataset Sampling prop. Epochs Disk size\nCommonCrawl 67.0% 1.10 3.3 TB\nC4 15.0% 1.06 783 GB\nGithub 4.5% 0.64 328 GB' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 1}
page_content='ing dataset at the ﬁle level, with exact matches.\nWikipedia [4.5%]. We add Wikipedia dumps\nfrom the June-August 2022 period, covering 20Dataset Sampling prop. Epochs Disk size\nCommonCrawl 67.0% 1.10 3.3 TB\nC4 15.0% 1.06 783 GB\nGithub 4.5% 0.64 328 GB\nWikipedia 4.5% 2.45 83 GB\nBooks 4.5% 2.23 85 GB\nArXiv 2.5% 1.06 92 GB\nStackExchange 2.0% 1.03 78 GB\nTable 1: Pre-training data. Data mixtures used for pre-\ntraining, for each subset we list the sampling propor-\ntion, number of epochs performed on the subset when\ntraining on 1.4T tokens, and disk size. The pre-training\nruns on 1T tokens have the same sampling proportion.\nlanguages, which use either the Latin or Cyrillic\nscripts: bg,ca,cs,da,de,en,es,fr,hr,hu,it,\nnl,pl,pt,ro,ru,sl,sr,sv,uk. We process the\ndata to remove hyperlinks, comments and other\nformatting boilerplate.\nGutenberg and Books3 [4.5%]. We include\ntwo book corpora in our training dataset: the Guten-\nberg Project, which contains books that are in the' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 1}
page_content='scripts: bg,ca,cs,da,de,en,es,fr,hr,hu,it,\nnl,pl,pt,ro,ru,sl,sr,sv,uk. We process the\ndata to remove hyperlinks, comments and other\nformatting boilerplate.\nGutenberg and Books3 [4.5%]. We include\ntwo book corpora in our training dataset: the Guten-\nberg Project, which contains books that are in the\npublic domain, and the Books3 section of TheP-\nile (Gao et al., 2020), a publicly available dataset\nfor training large language models. We perform\ndeduplication at the book level, removing books\nwith more than 90% content overlap.\nArXiv [2.5%]. We process arXiv Latex ﬁles\nto add scientiﬁc data to our dataset. Following\nLewkowycz et al. (2022), we removed everything\nbefore the ﬁrst section, as well as the bibliography.\nWe also removed the comments from the .tex ﬁles,\nand inline-expanded deﬁnitions and macros written\nby users to increase consistency across papers.\nStack Exchange [2%]. We include a dump of\nStack Exchange, a website of high quality ques-' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 1}
page_content='before the ﬁrst section, as well as the bibliography.\nWe also removed the comments from the .tex ﬁles,\nand inline-expanded deﬁnitions and macros written\nby users to increase consistency across papers.\nStack Exchange [2%]. We include a dump of\nStack Exchange, a website of high quality ques-\ntions and answers that covers a diverse set of do-\nmains, ranging from computer science to chemistry.\nWe kept the data from the 28 largest websites, re-\nmoved the HTML tags from text and sorted the\nanswers by score (from highest to lowest).\nTokenizer. We tokenize the data with the byte-\npair encoding (BPE) algorithm (Sennrich et al.,\n2015), using the implementation from Sentence-\nPiece (Kudo and Richardson, 2018). Notably, we\nsplit all numbers into individual digits, and fallback\nto bytes to decompose unknown UTF-8 characters.' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 1}
page_content='params dimension nheadsnlayers learning rate batch size ntokens\n6.7B 4096 32 32 3:0e\x0044M 1.0T\n13.0B 5120 40 40 3:0e\x0044M 1.0T\n32.5B 6656 52 60 1:5e\x0044M 1.4T\n65.2B 8192 64 80 1:5e\x0044M 1.4T\nTable 2: Model sizes, architectures, and optimization hyper-parameters.\nOverall, our entire training dataset contains\nroughly 1.4T tokens after tokenization. For most of\nour training data, each token is used only once dur-\ning training, with the exception of the Wikipedia\nand Books domains, over which we perform ap-\nproximately two epochs.\n2.2 Architecture\nFollowing recent work on large language models,\nour network is based on the transformer architec-\nture (Vaswani et al., 2017). We leverage various\nimprovements that were subsequently proposed,\nand used in different models such as PaLM. Here\nare the main difference with the original architec-\nture, and where we were found the inspiration for\nthis change (in bracket):\nPre-normalization [GPT3]. To improve the' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 2}
page_content='improvements that were subsequently proposed,\nand used in different models such as PaLM. Here\nare the main difference with the original architec-\nture, and where we were found the inspiration for\nthis change (in bracket):\nPre-normalization [GPT3]. To improve the\ntraining stability, we normalize the input of each\ntransformer sub-layer, instead of normalizing the\noutput. We use the RMSNorm normalizing func-\ntion, introduced by Zhang and Sennrich (2019).\nSwiGLU activation function [PaLM]. We re-\nplace the ReLU non-linearity by the SwiGLU ac-\ntivation function, introduced by Shazeer (2020) to\nimprove the performance. We use a dimension of\n2\n34dinstead of 4das in PaLM.\nRotary Embeddings [GPTNeo]. We remove the\nabsolute positional embeddings, and instead, add\nrotary positional embeddings (RoPE), introduced\nby Su et al. (2021), at each layer of the network.\nThe details of the hyper-parameters for our dif-\nferent models are given in Table 2.\n2.3 Optimizer' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 2}
page_content='Rotary Embeddings [GPTNeo]. We remove the\nabsolute positional embeddings, and instead, add\nrotary positional embeddings (RoPE), introduced\nby Su et al. (2021), at each layer of the network.\nThe details of the hyper-parameters for our dif-\nferent models are given in Table 2.\n2.3 Optimizer\nOur models are trained using the AdamW opti-\nmizer (Loshchilov and Hutter, 2017), with the fol-\nlowing hyper-parameters: \x0c1= 0:9;\x0c2= 0:95.\nWe use a cosine learning rate schedule, such that\nthe ﬁnal learning rate is equal to 10% of the maxi-\nmal learning rate. We use a weight decay of 0:1and\ngradient clipping of 1:0. We use 2;000warmup\n0 200 400 600 800 1000 1200 1400\nBillion of tokens1.51.61.71.81.92.02.12.2Training lossLLaMA 7B\nLLaMA 13B\nLLaMA 33B\nLLaMA 65BFigure 1: Training loss over train tokens for the 7B,\n13B, 33B, and 65 models. LLaMA-33B and LLaMA-\n65B were trained on 1.4T tokens. The smaller models\nwere trained on 1.0T tokens. All models are trained\nwith a batch size of 4M tokens.' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 2}
page_content='LLaMA 13B\nLLaMA 33B\nLLaMA 65BFigure 1: Training loss over train tokens for the 7B,\n13B, 33B, and 65 models. LLaMA-33B and LLaMA-\n65B were trained on 1.4T tokens. The smaller models\nwere trained on 1.0T tokens. All models are trained\nwith a batch size of 4M tokens.\nsteps, and vary the learning rate and batch size with\nthe size of the model (see Table 2 for details).\n2.4 Efﬁcient implementation\nWe make several optimizations to improve the train-\ning speed of our models. First, we use an efﬁcient\nimplementation of the causal multi-head attention\nto reduce memory usage and runtime. This imple-\nmentation, available in the xformers library,2is\ninspired by Rabe and Staats (2021) and uses the\nbackward from Dao et al. (2022). This is achieved\nby not storing the attention weights and not com-\nputing the key/query scores that are masked due to\nthe causal nature of the language modeling task.\nTo further improve training efﬁciency, we re-\nduced the amount of activations that are recom-' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 2}
page_content='backward from Dao et al. (2022). This is achieved\nby not storing the attention weights and not com-\nputing the key/query scores that are masked due to\nthe causal nature of the language modeling task.\nTo further improve training efﬁciency, we re-\nduced the amount of activations that are recom-\nputed during the backward pass with checkpoint-\ning. More precisely, we save the activations that\nare expensive to compute, such as the outputs of\nlinear layers. This is achieved by manually imple-\nmenting the backward function for the transformer\nlayers, instead of relying on the PyTorch autograd.\nTo fully beneﬁt from this optimization, we need to\n2https://github.com/facebookresearch/xformers' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 2}
page_content='BoolQ PIQA SIQA HellaSwag WinoGrande ARC-e ARC-c OBQA\nGPT-3 175B 60.5 81.0 - 78.9 70.2 68.8 51.4 57.6\nGopher 280B 79.3 81.8 50.6 79.2 70.1 - - -\nChinchilla 70B 83.7 81.8 51.3 80.8 74.9 - - -\nPaLM 62B 84.8 80.5 - 79.7 77.0 75.2 52.5 50.4\nPaLM-cont 62B 83.9 81.4 - 80.6 77.0 - - -\nPaLM 540B 88.0 82.3 - 83.4 81.1 76.6 53.0 53.4\nLLaMA7B 76.5 79.8 48.9 76.1 70.1 72.8 47.6 57.2\n13B 78.1 80.1 50.4 79.2 73.0 74.8 52.7 56.4\n33B 83.1 82.3 50.4 82.8 76.0 80.0 57.8 58.6\n65B 85.3 82.8 52.3 84.2 77.0 78.9 56.0 60.2\nTable 3: Zero-shot performance on Common Sense Reasoning tasks.\nreduce the memory usage of the model by using\nmodel and sequence parallelism, as described by\nKorthikanti et al. (2022). Moreover, we also over-\nlap the computation of activations and the commu-\nnication between GPUs over the network (due to\nall_reduce operations) as much as possible.\nWhen training a 65B-parameter model, our code\nprocesses around 380 tokens/sec/GPU on 2048\nA100 GPU with 80GB of RAM. This means that' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 3}
page_content='lap the computation of activations and the commu-\nnication between GPUs over the network (due to\nall_reduce operations) as much as possible.\nWhen training a 65B-parameter model, our code\nprocesses around 380 tokens/sec/GPU on 2048\nA100 GPU with 80GB of RAM. This means that\ntraining over our dataset containing 1.4T tokens\ntakes approximately 21 days.\n3 Main results\nFollowing previous work (Brown et al., 2020), we\nconsider zero-shot and few-shot tasks, and report\nresults on a total of 20 benchmarks:\n•Zero-shot. We provide a textual description\nof the task and a test example. The model\neither provides an answer using open-ended\ngeneration, or ranks the proposed answers.\n•Few-shot. We provide a few examples of the\ntask (between 1 and 64) and a test example.\nThe model takes this text as input and gener-\nates the answer or ranks different options.\nWe compare LLaMA with other foundation mod-\nels, namely the non-publicly available language\nmodels GPT-3 (Brown et al., 2020), Gopher (Rae' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 3}
page_content='task (between 1 and 64) and a test example.\nThe model takes this text as input and gener-\nates the answer or ranks different options.\nWe compare LLaMA with other foundation mod-\nels, namely the non-publicly available language\nmodels GPT-3 (Brown et al., 2020), Gopher (Rae\net al., 2021), Chinchilla (Hoffmann et al., 2022)\nand PaLM (Chowdhery et al., 2022), as well as\nthe open-sourced OPT models (Zhang et al., 2022),\nGPT-J (Wang and Komatsuzaki, 2021), and GPT-\nNeo (Black et al., 2022). In Section 4, we also\nbrieﬂy compare LLaMA with instruction-tuned\nmodels such as OPT-IML (Iyer et al., 2022) and\nFlan-PaLM (Chung et al., 2022).We evaluate LLaMA on free-form generation\ntasks and multiple choice tasks. In the multiple\nchoice tasks, the objective is to select the most\nappropriate completion among a set of given op-\ntions, based on a provided context. We select the\ncompletion with the highest likelihood given the\nprovided context. We follow Gao et al. (2021)' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 3}
page_content='tasks and multiple choice tasks. In the multiple\nchoice tasks, the objective is to select the most\nappropriate completion among a set of given op-\ntions, based on a provided context. We select the\ncompletion with the highest likelihood given the\nprovided context. We follow Gao et al. (2021)\nand use the likelihood normalized by the number\nof characters in the completion, except for certain\ndatasets (OpenBookQA, BoolQ), for which we fol-\nlow Brown et al. (2020), and select a completion\nbased on the likelihood normalized by the likeli-\nhood of the completion given “Answer:” as context:\nP(completionjcontext )=P(completionj\\Answer :").\n0-shot 1-shot 5-shot 64-shot\nGPT-3 175B 14.6 23.0 - 29.9\nGopher 280B 10.1 - 24.5 28.2\nChinchilla 70B 16.6 - 31.5 35.5\nPaLM8B 8.4 10.6 - 14.6\n62B 18.1 26.5 - 27.6\n540B 21.2 29.3 - 39.6\nLLaMA7B 16.8 18.7 22.0 26.1\n13B 20.1 23.4 28.1 31.9\n33B 24.9 28.3 32.9 36.0\n65B 23.8 31.0 35.0 39.9\nTable 4: NaturalQuestions. Exact match performance.' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 3}
page_content='Gopher 280B 10.1 - 24.5 28.2\nChinchilla 70B 16.6 - 31.5 35.5\nPaLM8B 8.4 10.6 - 14.6\n62B 18.1 26.5 - 27.6\n540B 21.2 29.3 - 39.6\nLLaMA7B 16.8 18.7 22.0 26.1\n13B 20.1 23.4 28.1 31.9\n33B 24.9 28.3 32.9 36.0\n65B 23.8 31.0 35.0 39.9\nTable 4: NaturalQuestions. Exact match performance.\n3.1 Common Sense Reasoning\nWe consider eight standard common sense rea-\nsoning benchmarks: BoolQ (Clark et al., 2019),\nPIQA (Bisk et al., 2020), SIQA (Sap et al., 2019),' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 3}
page_content='HellaSwag (Zellers et al., 2019), WinoGrande (Sak-\naguchi et al., 2021), ARC easy and challenge (Clark\net al., 2018) and OpenBookQA (Mihaylov et al.,\n2018). These datasets include Cloze and Winograd\nstyle tasks, as well as multiple choice question an-\nswering. We evaluate in the zero-shot setting as\ndone in the language modeling community.\nIn Table 3, we compare with existing models\nof various sizes and report numbers from the cor-\nresponding papers. First, LLaMA-65B outper-\nforms Chinchilla-70B on all reported benchmarks\nbut BoolQ. Similarly, this model surpasses PaLM-\n540B everywhere but on BoolQ and WinoGrande.\nLLaMA-13B model also outperforms GPT-3 on\nmost benchmarks despite being 10 \x02smaller.\n3.2 Closed-book Question Answering\nWe compare LLaMA to existing large language\nmodels on two closed-book question answering\nbenchmarks: Natural Questions (Kwiatkowski\net al., 2019) and TriviaQA (Joshi et al., 2017). For\nboth benchmarks, we report exact match perfor-' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 4}
page_content='3.2 Closed-book Question Answering\nWe compare LLaMA to existing large language\nmodels on two closed-book question answering\nbenchmarks: Natural Questions (Kwiatkowski\net al., 2019) and TriviaQA (Joshi et al., 2017). For\nboth benchmarks, we report exact match perfor-\nmance in a closed book setting, i.e., where the mod-\nels do not have access to documents that contain\nevidence to answer the question. In Table 4, we\nreport performance on NaturalQuestions, and in Ta-\nble 5, we report on TriviaQA. On both benchmarks,\nLLaMA-65B achieve state-of-the-arts performance\nin the zero-shot and few-shot settings. More im-\nportantly, the LLaMA-13B is also competitive on\nthese benchmarks with GPT-3 and Chinchilla, de-\nspite being 5-10 \x02smaller. This model runs on a\nsingle V100 GPU during inference.\n0-shot 1-shot 5-shot 64-shot\nGopher 280B 43.5 - 57.0 57.2\nChinchilla 70B 55.4 - 64.1 64.6\nLLaMA7B 50.0 53.4 56.3 57.6\n13B 56.6 60.5 63.1 64.0\n33B 65.1 67.9 69.9 70.4\n65B 68.2 71.6 72.6 73.0' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 4}
page_content='spite being 5-10 \x02smaller. This model runs on a\nsingle V100 GPU during inference.\n0-shot 1-shot 5-shot 64-shot\nGopher 280B 43.5 - 57.0 57.2\nChinchilla 70B 55.4 - 64.1 64.6\nLLaMA7B 50.0 53.4 56.3 57.6\n13B 56.6 60.5 63.1 64.0\n33B 65.1 67.9 69.9 70.4\n65B 68.2 71.6 72.6 73.0\nTable 5: TriviaQA. Zero-shot and few-shot exact\nmatch performance on the ﬁltered dev set.\n3.3 Reading Comprehension\nWe evaluate our models on the RACE reading com-\nprehension benchmark (Lai et al., 2017). This\ndataset was collected from English reading com-\nprehension exams designed for middle and highRACE-middle RACE-high\nGPT-3 175B 58.4 45.5\nPaLM8B 57.9 42.3\n62B 64.3 47.5\n540B 68.1 49.1\nLLaMA7B 61.1 46.9\n13B 61.6 47.2\n33B 64.1 48.3\n65B 67.9 51.6\nTable 6: Reading Comprehension. Zero-shot accu-\nracy.\nschool Chinese students. We follow the evaluation\nsetup from Brown et al. (2020) and report results\nin Table 6. On these benchmarks, LLaMA-65B is\ncompetitive with PaLM-540B, and, LLaMA-13B' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 4}
page_content='13B 61.6 47.2\n33B 64.1 48.3\n65B 67.9 51.6\nTable 6: Reading Comprehension. Zero-shot accu-\nracy.\nschool Chinese students. We follow the evaluation\nsetup from Brown et al. (2020) and report results\nin Table 6. On these benchmarks, LLaMA-65B is\ncompetitive with PaLM-540B, and, LLaMA-13B\noutperforms GPT-3 by a few percents.\n3.4 Mathematical reasoning\nWe evaluate our models on two mathematical rea-\nsoning benchmarks: MATH (Hendrycks et al.,\n2021) and GSM8k (Cobbe et al., 2021). MATH\nis a dataset of 12K middle school and high school\nmathematics problems written in LaTeX. GSM8k\nis a set of middle school mathematical problems.\nIn Table 7, we compare with PaLM and Min-\nerva (Lewkowycz et al., 2022). Minerva is a series\nof PaLM models ﬁnetuned on 38.5B tokens ex-\ntracted from ArXiv and Math Web Pages, while\nneither PaLM or LLaMA are ﬁnetuned on mathe-\nmatical data. The numbers for PaLM and Minerva\nare taken from Lewkowycz et al. (2022), and we\ncompare with and without maj1@k .maj1@k de-' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 4}
page_content='of PaLM models ﬁnetuned on 38.5B tokens ex-\ntracted from ArXiv and Math Web Pages, while\nneither PaLM or LLaMA are ﬁnetuned on mathe-\nmatical data. The numbers for PaLM and Minerva\nare taken from Lewkowycz et al. (2022), and we\ncompare with and without maj1@k .maj1@k de-\nnotes evaluations where we generate ksamples for\neach problem and perform a majority voting (Wang\net al., 2022). On GSM8k, we observe that LLaMA-\n65B outperforms Minerva-62B, although it has not\nbeen ﬁne-tuned on mathematical data.\n3.5 Code generation\nWe evaluate the ability of our models to write\ncode from a natural language description on two\nbenchmarks: HumanEval (Chen et al., 2021) and\nMBPP (Austin et al., 2021). For both tasks, the\nmodel receives a description of the program in a\nfew sentences, as well as a few input-output ex-\namples. In HumanEval, it also receives a function\nsignature, and the prompt is formatted as natural\ncode with the textual description and tests in a' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 4}
page_content='MATH +maj1@k GSM8k +maj1@k\nPaLM8B 1.5 - 4.1 -\n62B 4.4 - 33.0 -\n540B 8.8 - 56.5 -\nMinerva8B 14.1 25.4 16.2 28.4\n62B 27.6 43.4 52.4 68.5\n540B 33.6 50.3 68.5 78.5\nLLaMA7B 2.9 6.9 11.0 18.1\n13B 3.9 8.8 17.8 29.3\n33B 7.1 15.2 35.6 53.1\n65B 10.6 20.5 50.9 69.7\nTable 7: Model performance on quantitative reason-\ning datasets. For majority voting, we use the same\nsetup as Minerva, with k= 256 samples for MATH\nandk= 100 for GSM8k (Minerva 540B uses k= 64\nfor MATH and and k= 40 for GSM8k). LLaMA-65B\noutperforms Minerva 62B on GSM8k, although it has\nnot been ﬁne-tuned on mathematical data.\ndocstring. The model needs to generate a Python\nprogram that ﬁts the description and satisﬁes the\ntest cases. In Table 8, we compare the pass@1\nscores of our models with existing language mod-\nels that have not been ﬁnetuned on code, namely\nPaLM and LaMDA (Thoppilan et al., 2022). PaLM\nand LLaMA were trained on datasets that contain\na similar number of code tokens.\nAs show in Table 8, for a similar number' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 5}
page_content='scores of our models with existing language mod-\nels that have not been ﬁnetuned on code, namely\nPaLM and LaMDA (Thoppilan et al., 2022). PaLM\nand LLaMA were trained on datasets that contain\na similar number of code tokens.\nAs show in Table 8, for a similar number\nof parameters, LLaMA outperforms other gen-\neral models such as LaMDA and PaLM, which\nare not trained or ﬁnetuned speciﬁcally for code.\nLLaMA with 13B parameters and more outper-\nforms LaMDA 137B on both HumanEval and\nMBPP. LLaMA 65B also outperforms PaLM 62B,\neven when it is trained longer. The pass@1 results\nreported in this table were obtained by sampling\nwith temperature 0.1. The pass@100 and pass@80\nmetrics were obtained with temperature 0.8. We\nuse the same method as Chen et al. (2021) to obtain\nunbiased estimates of the pass@k.\nIt is possible to improve the performance on code\nby ﬁnetuning on code-speciﬁc tokens. For instance,\nPaLM-Coder (Chowdhery et al., 2022) increases\nthe pass@1 score of PaLM on HumanEval from' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 5}
page_content='use the same method as Chen et al. (2021) to obtain\nunbiased estimates of the pass@k.\nIt is possible to improve the performance on code\nby ﬁnetuning on code-speciﬁc tokens. For instance,\nPaLM-Coder (Chowdhery et al., 2022) increases\nthe pass@1 score of PaLM on HumanEval from\n26.2% for PaLM to 36%. Other models trained\nspeciﬁcally for code also perform better than gen-\neral models on these tasks (Chen et al., 2021; Ni-\njkamp et al., 2022; Fried et al., 2022). Finetuning\non code tokens is beyond the scope of this paper.Params HumanEval MBPP\npass@ @1 @100 @1 @80\nLaMDA 137B 14.0 47.3 14.8 62.4\nPaLM 8B 3.6\x0318.7\x035.0\x0335.7\x03\nPaLM 62B 15.9 46.3\x0321.4 63.2\x03\nPaLM-cont 62B 23.7 - 31.2 -\nPaLM 540B 26.2 76.2 36.8 75.0\nLLaMA7B 10.5 36.5 17.7 56.2\n13B 15.8 52.5 22.0 64.0\n33B 21.7 70.7 30.2 73.4\n65B 23.7 79.3 37.7 76.8\nTable 8: Model performance for code generation.\nWe report the pass@ score on HumanEval and MBPP.\nHumanEval generations are done in zero-shot and' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 5}
page_content='PaLM 540B 26.2 76.2 36.8 75.0\nLLaMA7B 10.5 36.5 17.7 56.2\n13B 15.8 52.5 22.0 64.0\n33B 21.7 70.7 30.2 73.4\n65B 23.7 79.3 37.7 76.8\nTable 8: Model performance for code generation.\nWe report the pass@ score on HumanEval and MBPP.\nHumanEval generations are done in zero-shot and\nMBBP with 3-shot prompts similar to Austin et al.\n(2021). The values marked with\x03are read from ﬁgures\nin Chowdhery et al. (2022).\n3.6 Massive Multitask Language\nUnderstanding\nThe massive multitask language understanding\nbenchmark, or MMLU, introduced by Hendrycks\net al. (2020) consists of multiple choice questions\ncovering various domains of knowledge, includ-\ning humanities, STEM and social sciences. We\nevaluate our models in the 5-shot setting, using the\nexamples provided by the benchmark, and report\nresults in Table 9. On this benchmark, we observe\nthat the LLaMA-65B is behind both Chinchilla-\n70B and PaLM-540B by a few percent in average,\nand across most domains. A potential explanation' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 5}
page_content='evaluate our models in the 5-shot setting, using the\nexamples provided by the benchmark, and report\nresults in Table 9. On this benchmark, we observe\nthat the LLaMA-65B is behind both Chinchilla-\n70B and PaLM-540B by a few percent in average,\nand across most domains. A potential explanation\nis that we have used a limited amount of books\nand academic papers in our pre-training data, i.e.,\nArXiv, Gutenberg and Books3, that sums up to only\n177GB, while these models were trained on up to\n2TB of books. This large quantity of books used\nby Gopher, Chinchilla and PaLM may also explain\nwhy Gopher outperforms GPT-3 on this benchmark,\nwhile it is comparable on other benchmarks.\n3.7 Evolution of performance during training\nDuring training, we tracked the performance of our\nmodels on a few question answering and common\nsense benchmarks, and report them in Figure 2.\nOn most benchmarks, the performance improves\nsteadily, and correlates with the training perplexity' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 5}
page_content='3.7 Evolution of performance during training\nDuring training, we tracked the performance of our\nmodels on a few question answering and common\nsense benchmarks, and report them in Figure 2.\nOn most benchmarks, the performance improves\nsteadily, and correlates with the training perplexity\nof the model (see Figure 1). The exceptions are\nSIQA and WinoGrande. Most notably, on SIQA,\nwe observe a lot of variance in performance,' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 5}
page_content='Humanities STEM Social Sciences Other Average\nGPT-NeoX 20B 29.8 34.9 33.7 37.7 33.6\nGPT-3 175B 40.8 36.7 50.4 48.8 43.9\nGopher 280B 56.2 47.4 71.9 66.1 60.0\nChinchilla 70B 63.6 54.9 79.3 73.9 67.5\nPaLM8B 25.6 23.8 24.1 27.8 25.4\n62B 59.5 41.9 62.7 55.8 53.7\n540B 77.0 55.6 81.0 69.6 69.3\nLLaMA7B 34.0 30.5 38.3 38.1 35.1\n13B 45.0 35.8 53.8 53.3 46.9\n33B 55.8 46.0 66.7 63.4 57.8\n65B 61.8 51.7 72.9 67.4 63.4\nTable 9: Massive Multitask Language Understanding (MMLU). Five-shot accuracy.\nthat may indicate that this benchmark is not\nreliable. On WinoGrande, the performance does\nnot correlate as well with training perplexity:\nthe LLaMA-33B and LLaMA-65B have similar\nperformance during the training.\n4 Instruction Finetuning\nIn this section, we show that brieﬂy ﬁnetuning on\ninstructions data rapidly leads to improvements\non MMLU. Although the non-ﬁnetuned version\nof LLaMA-65B is already able to follow basic in-\nstructions, we observe that a very small amount of' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 6}
page_content='performance during the training.\n4 Instruction Finetuning\nIn this section, we show that brieﬂy ﬁnetuning on\ninstructions data rapidly leads to improvements\non MMLU. Although the non-ﬁnetuned version\nof LLaMA-65B is already able to follow basic in-\nstructions, we observe that a very small amount of\nﬁnetuning improves the performance on MMLU,\nand further improves the ability of the model to\nfollow instructions. Since this is not the focus of\nthis paper, we only conducted a single experiment\nfollowing the same protocol as Chung et al. (2022)\nto train an instruct model, LLaMA-I.\nOPT 30B 26.1\nGLM 120B 44.8\nPaLM 62B 55.1\nPaLM-cont 62B 62.8\nChinchilla 70B 67.5\nLLaMA 65B 63.4\nOPT-IML-Max 30B 43.2\nFlan-T5-XXL 11B 55.1\nFlan-PaLM 62B 59.6\nFlan-PaLM-cont 62B 66.1\nLLaMA-I 65B 68.9\nTable 10: Instruction ﬁnetuning – MMLU (5-shot).\nComparison of models of moderate size with and with-\nout instruction ﬁnetuning on MMLU.In Table 10, we report the results of our instruct' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 6}
page_content='OPT-IML-Max 30B 43.2\nFlan-T5-XXL 11B 55.1\nFlan-PaLM 62B 59.6\nFlan-PaLM-cont 62B 66.1\nLLaMA-I 65B 68.9\nTable 10: Instruction ﬁnetuning – MMLU (5-shot).\nComparison of models of moderate size with and with-\nout instruction ﬁnetuning on MMLU.In Table 10, we report the results of our instruct\nmodel LLaMA-I on MMLU and compare with ex-\nisting instruction ﬁnetuned models of moderate\nsizes, namely, OPT-IML (Iyer et al., 2022) and the\nFlan-PaLM series (Chung et al., 2022). All the re-\nported numbers are from the corresponding papers.\nDespite the simplicity of the instruction ﬁnetuning\napproach used here, we reach 68.9% on MMLU.\nLLaMA-I (65B) outperforms on MMLU existing\ninstruction ﬁnetuned models of moderate sizes, but\nare still far from the state-of-the-art, that is 77.4\nfor GPT code-davinci-002 on MMLU (numbers\ntaken from Iyer et al. (2022)). The details of the\nperformance on MMLU on the 57 tasks can be\nfound in Table 16 of the appendix.\n5 Bias, Toxicity and Misinformation' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 6}
page_content='are still far from the state-of-the-art, that is 77.4\nfor GPT code-davinci-002 on MMLU (numbers\ntaken from Iyer et al. (2022)). The details of the\nperformance on MMLU on the 57 tasks can be\nfound in Table 16 of the appendix.\n5 Bias, Toxicity and Misinformation\nLarge language models have been showed to re-\nproduce and amplify biases that are existing in\nthe training data (Sheng et al., 2019; Kurita et al.,\n2019), and to generate toxic or offensive con-\ntent (Gehman et al., 2020). As our training dataset\ncontains a large proportion of data from the Web,\nwe believe that it is crucial to determine the po-\ntential for our models to generate such content.\nTo understand the potential harm of LLaMA-65B,\nwe evaluate on different benchmarks that measure\ntoxic content production and stereotypes detection.\nWhile we have selected some of the standard bench-\nmarks that are used by the language model com-\nmunity to indicate some of the issues with these' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 6}
page_content='To understand the potential harm of LLaMA-65B,\nwe evaluate on different benchmarks that measure\ntoxic content production and stereotypes detection.\nWhile we have selected some of the standard bench-\nmarks that are used by the language model com-\nmunity to indicate some of the issues with these\nmodels, these evaluations are not sufﬁcient to fully\nunderstand the risks associated with these models.' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 6}
page_content='0 250 500 750 1000 1250 1500203040506070AccuracyTriviaQA\n0 250 500 750 1000 1250 15005055606570758085HellaSwag\n0 250 500 750 1000 1250 150005101520253035NaturalQuestions\n0 250 500 750 1000 1250 1500\nBillion of tokens40424446485052AccuracySIQA\n0 250 500 750 1000 1250 1500\nBillion of tokens50556065707580WinoGrande\n0 250 500 750 1000 1250 1500\nBillion of tokens65.067.570.072.575.077.580.082.5PIQA\nLLaMA 7B\nLLaMA 13B\nLLaMA 33B\nLLaMA 65B\nChinchillaFigure 2: Evolution of performance on question answering and common sense reasoning during training.\n5.1 RealToxicityPrompts\nLanguage models can generate toxic language, e.g.,\ninsults, hate speech or threats. There is a very large\nrange of toxic content that a model can generate,\nmaking a thorough evaluation challenging. Several\nrecent work (Zhang et al., 2022; Hoffmann et al.,\n2022) have considered the RealToxicityPrompts\nbenchmark (Gehman et al., 2020) as an indicator\nof how toxic is their model. RealToxicityPrompts' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 7}
page_content='range of toxic content that a model can generate,\nmaking a thorough evaluation challenging. Several\nrecent work (Zhang et al., 2022; Hoffmann et al.,\n2022) have considered the RealToxicityPrompts\nbenchmark (Gehman et al., 2020) as an indicator\nof how toxic is their model. RealToxicityPrompts\nconsists of about 100k prompts that the model must\ncomplete; then a toxicity score is automatically\nevaluated by making a request to PerspectiveAPI3.\nWe do not have control over the pipeline used by\nthe third-party PerspectiveAPI, making comparison\nwith previous models difﬁcult.\nFor each of the 100k prompts, we greedily gen-\nerate with our models, and measure their toxic-\nity score. The score per prompt ranges from 0\n(non-toxic) to 1 (toxic). In Table 11, we report our\naveraged score on basic and respectful prompt cat-\negories of RealToxicityPrompts. These scores are\n“comparable” with what we observe in the litera-\nture (e.g., 0.087 for Chinchilla) but the method-' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 7}
page_content='ity score. The score per prompt ranges from 0\n(non-toxic) to 1 (toxic). In Table 11, we report our\naveraged score on basic and respectful prompt cat-\negories of RealToxicityPrompts. These scores are\n“comparable” with what we observe in the litera-\nture (e.g., 0.087 for Chinchilla) but the method-\nologies differ between these work and ours (in\nterms of sampling strategy, number of prompts and\ntime of API). We observe that toxicity increases\n3https://perspectiveapi.com/Basic Respectful\nLLaMA7B 0.106 0.081\n13B 0.104 0.095\n33B 0.107 0.087\n65B 0.128 0.141\nTable 11: RealToxicityPrompts. We run a greedy de-\ncoder on the 100k prompts from this benchmark. The\n“respectful” versions are prompts starting with “Com-\nplete the following sentence in a polite, respectful, and\nunbiased manner:”, and “Basic” is without it. Scores\nwere obtained using the PerplexityAPI, with higher\nscore indicating more toxic generations.\nwith the size of the model, especially for Respect-' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 7}
page_content='plete the following sentence in a polite, respectful, and\nunbiased manner:”, and “Basic” is without it. Scores\nwere obtained using the PerplexityAPI, with higher\nscore indicating more toxic generations.\nwith the size of the model, especially for Respect-\nful prompts. This was also observed in previous\nwork (Zhang et al., 2022), with the notable excep-\ntion of Hoffmann et al. (2022) where they do not\nsee a difference between Chinchilla and Gopher,\ndespite different sizes. This could be explained by\nthe fact that the larger model, Gopher, has worse\nperformance than Chinchilla, suggesting that the\nrelation between toxicity and model size may only\napply within a model family.' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 7}
page_content='LLaMA GPT3 OPT\nGender 70.6 62.6 65.7\nReligion 79.0 73.3 68.6\nRace/Color 57.0 64.7 68.6\nSexual orientation 81.0 76.2 78.6\nAge 70.1 64.4 67.8\nNationality 64.2 61.6 62.9\nDisability 66.7 76.7 76.7\nPhysical appearance 77.8 74.6 76.2\nSocioeconomic status 71.5 73.8 76.2\nAverage 66.6 67.2 69.5\nTable 12: CrowS-Pairs. We compare the level of bi-\nases contained in LLaMA-65B with OPT-175B and\nGPT3-175B. Higher score indicates higher bias.\n5.2 CrowS-Pairs\nWe evaluate the biases in our model on the CrowS-\nPairs (Nangia et al., 2020). This dataset allows to\nmeasure biases in 9 categories: gender, religion,\nrace/color, sexual orientation, age, nationality, dis-\nability, physical appearance and socioeconomic sta-\ntus. Each example is composed of a stereotype and\nan anti-stereotype, we measure the model prefer-\nence for the stereotypical sentence using the per-\nplexity of both sentences in a zero-shot setting.\nHigher scores thus indicate higher bias. We com-\npare with GPT-3 and OPT-175B in Table 12.' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 8}
page_content='tus. Each example is composed of a stereotype and\nan anti-stereotype, we measure the model prefer-\nence for the stereotypical sentence using the per-\nplexity of both sentences in a zero-shot setting.\nHigher scores thus indicate higher bias. We com-\npare with GPT-3 and OPT-175B in Table 12.\nLLaMA compares slightly favorably to both\nmodels on average. Our model is particularly bi-\nased in the religion category (+10% compared to\nOPT-175B), followed by age and gender. We ex-\npect these biases to come from CommonCrawl de-\nspite multiple ﬁltering steps.\n5.3 WinoGender\nTo further investigate the biases of our model on\nthe gender category, we look at the WinoGender\nbenchmark (Rudinger et al., 2018), a co-reference\nresolution dataset. WinoGender is made of Wino-\ngrad schema, and biases are evaluated by determin-\ning if a model co-reference resolution performance\nis impacted by the gender of the pronoun.\nMore precisely, each sentence has three men-\ntions: an “occupation”, a “participant”, and a' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 8}
page_content='resolution dataset. WinoGender is made of Wino-\ngrad schema, and biases are evaluated by determin-\ning if a model co-reference resolution performance\nis impacted by the gender of the pronoun.\nMore precisely, each sentence has three men-\ntions: an “occupation”, a “participant”, and a\n“pronoun” where the pronoun is co-referencing\neither the occupation or participant. We prompt\nthe model to determine the co-reference relation\nand measure if it does so correctly according tothe context of the sentence. The goal is to reveal\nif societal biases associated with occupations\nhave been captured by the model. For example,\na sentence in the WinoGender dataset is “The\nnurse notiﬁed the patient that his shift would be\nending in an hour.”, which is followed by ‘His’\nrefers to . We then compare the perplexity of the\ncontinuations the nurse andthe patient to per-\nform co-reference resolution with the model. We\nevaluate the performance when using 3 pronouns:' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 8}
page_content='nurse notiﬁed the patient that his shift would be\nending in an hour.”, which is followed by ‘His’\nrefers to . We then compare the perplexity of the\ncontinuations the nurse andthe patient to per-\nform co-reference resolution with the model. We\nevaluate the performance when using 3 pronouns:\n“her/her/she”, “his/him/he” and “their/them/some-\none” (the different choices corresponding to the\ngrammatical function of the pronoun.\nIn Table 13, we report the co-reference scores\nfor the three different pronouns contained in the\ndataset. We observe that our model is signiﬁcantly\nbetter at performing co-reference resolution for\nthe “their/them/someone” pronouns than for the\n“her/her/she” and “his/him/he” pronouns. A simi-\nlar observation was made in previous work (Rae\net al., 2021; Hoffmann et al., 2022), and is likely\nindicative of gender bias. Indeed, in the case of the\n“her/her/she” and “his/him/he” pronouns, the model\nis probably using the majority gender of the occu-' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 8}
page_content='lar observation was made in previous work (Rae\net al., 2021; Hoffmann et al., 2022), and is likely\nindicative of gender bias. Indeed, in the case of the\n“her/her/she” and “his/him/he” pronouns, the model\nis probably using the majority gender of the occu-\npation to perform co-reference resolution, instead\nof using the evidence of the sentence.\nTo further investigate this hypothesis, we look\nat the set of “gotcha” cases for the “her/her/she”\nand “his/him/he” pronouns in the WinoGender\ndataset. Theses cases correspond to sentences in\nwhich the pronoun does not match the majority\ngender of the occupation, and the occupation is\nthe correct answer. In Table 13, we observe that\nour model, LLaMA-65B, makes more errors on the\ngotcha examples, clearly showing that it capture\nsocietal biases related to gender and occupation.\nThe drop of performance exists for “her/her/she”\nand “his/him/he” pronouns, which is indicative of\nbiases regardless of gender.\n5.4 TruthfulQA' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 8}
page_content='our model, LLaMA-65B, makes more errors on the\ngotcha examples, clearly showing that it capture\nsocietal biases related to gender and occupation.\nThe drop of performance exists for “her/her/she”\nand “his/him/he” pronouns, which is indicative of\nbiases regardless of gender.\n5.4 TruthfulQA\nTruthfulQA (Lin et al., 2021) aims to measure the\ntruthfulness of a model, i.e., its ability to identify\nwhen a claim is true. Lin et al. (2021) consider\nthe deﬁnition of “true” in the sense of “literal truth\nabout the real world”, and not claims that are only\ntrue in the context of a belief system or tradition.\nThis benchmark can evaluate the risks of a model\nto generate misinformation or false claims. The\nquestions are written in diverse style, cover 38 cat-\negories and are designed to be adversarial.' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 8}
page_content='7B 13B 33B 65B\nAll 66.0 64.7 69.0 77.5\nher/her/she 65.0 66.7 66.7 78.8\nhis/him/he 60.8 62.5 62.1 72.1\ntheir/them/someone 72.1 65.0 78.3 81.7\nher/her/she ( gotcha ) 64.2 65.8 61.7 75.0\nhis/him/he ( gotcha ) 55.0 55.8 55.8 63.3\nTable 13: WinoGender. Co-reference resolution ac-\ncuracy for the LLaMA models, for different pronouns\n(“her/her/she” and “his/him/he”). We observe that our\nmodels obtain better performance on “their/them/some-\none’ pronouns than on “her/her/she” and “his/him/he’,\nwhich is likely indicative of biases.\nTruthful Truthful*Inf\nGPT-31.3B 0.31 0.19\n6B 0.22 0.19\n175B 0.28 0.25\nLLaMA7B 0.33 0.29\n13B 0.47 0.41\n33B 0.52 0.48\n65B 0.57 0.53\nTable 14: TruthfulQA. We report the fraction of truth-\nful and truthful*informative answers, as scored by spe-\ncially trained models via the OpenAI API. We follow\nthe QA prompt style used in Ouyang et al. (2022), and\nreport the performance of GPT-3 from the same paper.\nIn Table 14, we report the performance of our' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 9}
page_content='ful and truthful*informative answers, as scored by spe-\ncially trained models via the OpenAI API. We follow\nthe QA prompt style used in Ouyang et al. (2022), and\nreport the performance of GPT-3 from the same paper.\nIn Table 14, we report the performance of our\nmodels on both questions to measure truthful mod-\nels and the intersection of truthful and informative.\nCompared to GPT-3, our model scores higher in\nboth categories, but the rate of correct answers is\nstill low, showing that our model is likely to hallu-\ncinate incorrect answers.\n6 Carbon footprint\nThe training of our models have consumed a mas-\nsive quantity of energy, responsible for the emis-\nsion of carbon dioxide. We follow the recent liter-\nature on the subject and breakdown both the total\nenergy consumption and the resulting carbon foot-\nprint in Table 15. We follow a formula for Wu et al.\n(2022) to estimate the Watt-hour, Wh, needed to\ntrain a model, as well as the tons of carbon emis-' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 9}
page_content='ature on the subject and breakdown both the total\nenergy consumption and the resulting carbon foot-\nprint in Table 15. We follow a formula for Wu et al.\n(2022) to estimate the Watt-hour, Wh, needed to\ntrain a model, as well as the tons of carbon emis-\nsions, tCO 2eq. For the Wh, we use the formula:\nWh =GPU-h\x02(GPU power consumption )\x02PUE;where we set the Power Usage Effectiveness (PUE)\nat1:1. The resulting carbon emission depends on\nthe location of the data center used to train the net-\nwork. For instance, BLOOM uses a grid that emits\n0.057 kg CO 2eq/KWh leading to 27 tCO 2eq and\nOPT a grid that emits 0.231 kg CO 2eq/KWh, lead-\ning to 82 tCO 2eq. In this study, we are interested in\ncomparing the cost in carbon emission of training\nof these models if they were trained in the same\ndata center. Hence, we do not take the location\nof data center in consideration, and use, instead,\nthe US national average carbon intensity factor of\n0.385 kg CO 2eq/KWh. This leads to the following' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 9}
page_content='comparing the cost in carbon emission of training\nof these models if they were trained in the same\ndata center. Hence, we do not take the location\nof data center in consideration, and use, instead,\nthe US national average carbon intensity factor of\n0.385 kg CO 2eq/KWh. This leads to the following\nformula for the tons of carbon emissions:\ntCO2eq=MWh\x020:385:\nWe apply the same formula to OPT and BLOOM\nfor fair comparison. For OPT, we assume training\nrequired 34 days on 992 A100-80B (see their logs4).\nFinally, we estimate that we used 2048 A100-80GB\nfor a period of approximately 5 months to develop\nour models. This means that developing these mod-\nels would have cost around 2,638 MWh under our\nassumptions, and a total emission of 1,015 tCO 2eq.\nWe hope that releasing these models will help to\nreduce future carbon emission since the training is\nalready done, and some of the models are relatively\nsmall and can be run on a single GPU.\n7 Related work' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 9}
page_content='assumptions, and a total emission of 1,015 tCO 2eq.\nWe hope that releasing these models will help to\nreduce future carbon emission since the training is\nalready done, and some of the models are relatively\nsmall and can be run on a single GPU.\n7 Related work\nLanguage models are probability distributions\nover sequences of words, tokens or charac-\nters (Shannon, 1948, 1951). This task, often framed\nas next token prediction, has long been considered a\ncore problem in natural language processing (Bahl\net al., 1983; Brown et al., 1990). Because Turing\n(1950) proposed to measure machine intelligence\nby using language through the “imitation game”,\nlanguage modeling has been proposed as a bench-\nmark to measure progress toward artiﬁcial intelli-\ngence (Mahoney, 1999).\nArchitecture. Traditionally, language models\nwere based on n-gram count statistics (Bahl\net al., 1983), and various smoothing techniques\nwere proposed to improve the estimation of rare' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 9}
page_content='mark to measure progress toward artiﬁcial intelli-\ngence (Mahoney, 1999).\nArchitecture. Traditionally, language models\nwere based on n-gram count statistics (Bahl\net al., 1983), and various smoothing techniques\nwere proposed to improve the estimation of rare\nevents (Katz, 1987; Kneser and Ney, 1995). In the\npast two decades, neural networks have been suc-\ncessfully applied to the language modelling task,\n4https://github.com/facebookresearch/metaseq/\ntree/main/projects/OPT/chronicles' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 9}
page_content='GPU TypeGPU PowerGPU-hoursTotal power Carbon emitted\nconsumption consumption (tCO 2eq)\nOPT-175B A100-80GB 400W 809,472 356 MWh 137\nBLOOM-175B A100-80GB 400W 1,082,880 475 MWh 183\nLLaMA-7B A100-80GB 400W 82,432 36 MWh 14\nLLaMA-13B A100-80GB 400W 135,168 59 MWh 23\nLLaMA-33B A100-80GB 400W 530,432 233 MWh 90\nLLaMA-65B A100-80GB 400W 1,022,362 449 MWh 173\nTable 15: Carbon footprint of training different models in the same data center. We follow Wu et al. (2022)\nto compute carbon emission of training OPT, BLOOM and our models in the same data center. For the power\nconsumption of a A100-80GB, we take the thermal design power for NVLink systems, that is 400W. We take a\nPUE of 1.1 and a carbon intensity factor set at the national US average of 0.385 kg CO 2e per KWh.\nstarting from feed forward models (Bengio et al.,\n2000), recurrent neural networks (Elman, 1990;\nMikolov et al., 2010) and LSTMs (Hochreiter and\nSchmidhuber, 1997; Graves, 2013). More recently,' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 10}
page_content='PUE of 1.1 and a carbon intensity factor set at the national US average of 0.385 kg CO 2e per KWh.\nstarting from feed forward models (Bengio et al.,\n2000), recurrent neural networks (Elman, 1990;\nMikolov et al., 2010) and LSTMs (Hochreiter and\nSchmidhuber, 1997; Graves, 2013). More recently,\ntransformer networks, based on self-attention, have\nled to important improvements, especially for cap-\nturing long range dependencies (Vaswani et al.,\n2017; Radford et al., 2018; Dai et al., 2019).\nScaling. There is a long history of scaling for\nlanguage models, for both the model and dataset\nsizes. Brants et al. (2007) showed the beneﬁts of\nusing language models trained on 2 trillion tokens,\nresulting in 300 billion n-grams, on the quality of\nmachine translation. While this work relied on a\nsimple smoothing technique, called Stupid Backoff ,\nHeaﬁeld et al. (2013) later showed how to scale\nKneser-Ney smoothing to Web-scale data. This\nallowed to train a 5-gram model on 975 billions to-' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 10}
page_content='resulting in 300 billion n-grams, on the quality of\nmachine translation. While this work relied on a\nsimple smoothing technique, called Stupid Backoff ,\nHeaﬁeld et al. (2013) later showed how to scale\nKneser-Ney smoothing to Web-scale data. This\nallowed to train a 5-gram model on 975 billions to-\nkens from CommonCrawl, resulting in a model\nwith 500 billions n-grams (Buck et al., 2014).\nChelba et al. (2013) introduced the One Billion\nWord benchmark, a large scale training dataset to\nmeasure the progress of language models.\nIn the context of neural language models, Joze-\nfowicz et al. (2016) obtained state-of-the-art re-\nsults on the Billion Word benchmark by scaling\nLSTMs to 1 billion parameters. Later, scaling\ntransformers lead to improvement on many NLP\ntasks. Notable models include BERT (Devlin et al.,\n2018), GPT-2 (Radford et al., 2019), Megatron-\nLM (Shoeybi et al., 2019), and T5 (Raffel et al.,\n2020). A signiﬁcant breakthrough was obtained' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 10}
page_content='LSTMs to 1 billion parameters. Later, scaling\ntransformers lead to improvement on many NLP\ntasks. Notable models include BERT (Devlin et al.,\n2018), GPT-2 (Radford et al., 2019), Megatron-\nLM (Shoeybi et al., 2019), and T5 (Raffel et al.,\n2020). A signiﬁcant breakthrough was obtained\nwith GPT-3 (Brown et al., 2020), a model with\n175 billion parameters. This lead to a series of\nLarge Language Models , such as Jurassic-1 (Lieber\net al., 2021), Megatron-Turing NLG (Smith et al.,2022), Gopher (Rae et al., 2021), Chinchilla (Hoff-\nmann et al., 2022), PaLM (Chowdhery et al., 2022),\nOPT (Zhang et al., 2022), and GLM (Zeng et al.,\n2022). Hestness et al. (2017) and Rosenfeld et al.\n(2019) studied the impact of scaling on the perfor-\nmance of deep learning models, showing the exis-\ntence of power laws between the model and dataset\nsizes and the performance of the system. Kaplan\net al. (2020) derived power laws speciﬁcally for\ntransformer based language models, which were' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 10}
page_content='(2019) studied the impact of scaling on the perfor-\nmance of deep learning models, showing the exis-\ntence of power laws between the model and dataset\nsizes and the performance of the system. Kaplan\net al. (2020) derived power laws speciﬁcally for\ntransformer based language models, which were\nlater reﬁned by Hoffmann et al. (2022), by adapting\nthe learning rate schedule when scaling datasets.\nFinally, Wei et al. (2022) studied the effect of scal-\ning on the abilities of large language models.\n8 Conclusion\nIn this paper, we presented a series of language\nmodels that are released openly, and competitive\nwith state-of-the-art foundation models. Most\nnotably, LLaMA-13B outperforms GPT-3 while\nbeing more than 10 \x02smaller, and LLaMA-65B is\ncompetitive with Chinchilla-70B and PaLM-540B.\nUnlike previous studies, we show that it is possible\nto achieve state-of-the-art performance by training\nexclusively on publicly available data, without\nresorting to proprietary datasets. We hope that' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 10}
page_content='being more than 10 \x02smaller, and LLaMA-65B is\ncompetitive with Chinchilla-70B and PaLM-540B.\nUnlike previous studies, we show that it is possible\nto achieve state-of-the-art performance by training\nexclusively on publicly available data, without\nresorting to proprietary datasets. We hope that\nreleasing these models to the research community\nwill accelerate the development of large language\nmodels, and help efforts to improve their robust-\nness and mitigate known issues such as toxicity and\nbias. Additionally, we observed like Chung et al.\n(2022) that ﬁnetuning these models on instructions\nlead to promising results, and we plan to further\ninvestigate this in future work. Finally, we plan to\nrelease larger models trained on larger pretraining\ncorpora in the future, since we have seen a constant\nimprovement in performance as we were scaling.' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 10}
page_content='Acknowledgements\nWe thank Daniel Haziza, Francisco Massa, Jeremy\nReizenstein, Artem Korenev, and Patrick Labatut\nfrom the xformers team. We thank Susan Zhang\nand Stephen Roller for their support on data\ndeduplication. We thank Luca Wehrstedt, Vegard\nMella, and Pierre-Emmanuel Mazaré for their\nsupport on training stability. We thank Shubho\nSengupta, Kalyan Saladi, and all the AI infra team\nfor their support. We thank Jane Yu for her input\non evaluation. We thank Yongyi Hu for his help\non data collection.\nReferences\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten\nBosma, Henryk Michalewski, David Dohan, Ellen\nJiang, Carrie Cai, Michael Terry, Quoc Le, and\nCharles Sutton. 2021. Program synthesis with large\nlanguage models.\nLalit R Bahl, Frederick Jelinek, and Robert L Mercer.\n1983. A maximum likelihood approach to continu-\nous speech recognition. IEEE transactions on pat-\ntern analysis and machine intelligence , pages 179–\n190.\nYoshua Bengio, Réjean Ducharme, and Pascal Vincent.' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 11}
page_content='language models.\nLalit R Bahl, Frederick Jelinek, and Robert L Mercer.\n1983. A maximum likelihood approach to continu-\nous speech recognition. IEEE transactions on pat-\ntern analysis and machine intelligence , pages 179–\n190.\nYoshua Bengio, Réjean Ducharme, and Pascal Vincent.\n2000. A neural probabilistic language model. Ad-\nvances in neural information processing systems , 13.\nYonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin\nChoi, et al. 2020. Piqa: Reasoning about physi-\ncal commonsense in natural language. In Proceed-\nings of the AAAI conference on artiﬁcial intelligence ,\npages 7432–7439.\nSid Black, Stella Biderman, Eric Hallahan, Quentin An-\nthony, Leo Gao, Laurence Golding, Horace He, Con-\nnor Leahy, Kyle McDonell, Jason Phang, et al. 2022.\nGpt-neox-20b: An open-source autoregressive lan-\nguage model. arXiv preprint arXiv:2204.06745 .\nThorsten Brants, Ashok C. Popat, Peng Xu, Franz J.\nOch, and Jeffrey Dean. 2007. Large language mod-' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 11}
page_content='thony, Leo Gao, Laurence Golding, Horace He, Con-\nnor Leahy, Kyle McDonell, Jason Phang, et al. 2022.\nGpt-neox-20b: An open-source autoregressive lan-\nguage model. arXiv preprint arXiv:2204.06745 .\nThorsten Brants, Ashok C. Popat, Peng Xu, Franz J.\nOch, and Jeffrey Dean. 2007. Large language mod-\nels in machine translation. In Proceedings of the\n2007 Joint Conference on Empirical Methods in Nat-\nural Language Processing and Computational Nat-\nural Language Learning (EMNLP-CoNLL) , pages\n858–867, Prague, Czech Republic. Association for\nComputational Linguistics.\nPeter F Brown, John Cocke, Stephen A Della Pietra,\nVincent J Della Pietra, Frederick Jelinek, John Laf-\nferty, Robert L Mercer, and Paul S Roossin. 1990. A\nstatistical approach to machine translation. Compu-\ntational linguistics , 16(2):79–85.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 11}
page_content='Vincent J Della Pietra, Frederick Jelinek, John Laf-\nferty, Robert L Mercer, and Paul S Roossin. 1990. A\nstatistical approach to machine translation. Compu-\ntational linguistics , 16(2):79–85.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot learn-\ners.\nChristian Buck, Kenneth Heaﬁeld, and Bas Van Ooyen.\n2014. N-gram counts and language models from the\ncommon crawl. In LREC , volume 2, page 4.\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robin-' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 11}
page_content='Amodei. 2020. Language models are few-shot learn-\ners.\nChristian Buck, Kenneth Heaﬁeld, and Bas Van Ooyen.\n2014. N-gram counts and language models from the\ncommon crawl. In LREC , volume 2, page 4.\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robin-\nson. 2013. One billion word benchmark for measur-\ning progress in statistical language modeling. arXiv\npreprint arXiv:1312.3005 .\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-\nplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, Alex Ray, Raul Puri, Gretchen\nKrueger, Michael Petrov, Heidy Khlaaf, Girish Sas-\ntry, Pamela Mishkin, Brooke Chan, Scott Gray,\nNick Ryder, Mikhail Pavlov, Alethea Power, Lukasz\nKaiser, Mohammad Bavarian, Clemens Winter,\nPhilippe Tillet, Felipe Petroski Such, Dave Cum-\nmings, Matthias Plappert, Fotios Chantzis, Eliza-\nbeth Barnes, Ariel Herbert-V oss, William Hebgen' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 11}
page_content='try, Pamela Mishkin, Brooke Chan, Scott Gray,\nNick Ryder, Mikhail Pavlov, Alethea Power, Lukasz\nKaiser, Mohammad Bavarian, Clemens Winter,\nPhilippe Tillet, Felipe Petroski Such, Dave Cum-\nmings, Matthias Plappert, Fotios Chantzis, Eliza-\nbeth Barnes, Ariel Herbert-V oss, William Hebgen\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie\nTang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\nWilliam Saunders, Christopher Hesse, Andrew N.\nCarr, Jan Leike, Josh Achiam, Vedant Misra, Evan\nMorikawa, Alec Radford, Matthew Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welin-\nder, Bob McGrew, Dario Amodei, Sam McCandlish,\nIlya Sutskever, and Wojciech Zaremba. 2021. Eval-\nuating large language models trained on code.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 11}
page_content='uating large language models trained on code.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng\nYin, Toju Duke, Anselm Levskaya, Sanjay Ghe-\nmawat, Sunipa Dev, Henryk Michalewski, Xavier\nGarcia, Vedant Misra, Kevin Robinson, Liam Fe-\ndus, Denny Zhou, Daphne Ippolito, David Luan,\nHyeontaek Lim, Barret Zoph, Alexander Spiridonov,\nRyan Sepassi, David Dohan, Shivani Agrawal, Mark\nOmernick, Andrew M. Dai, Thanumalayan Sankara-\nnarayana Pillai, Marie Pellat, Aitor Lewkowycz,\nErica Moreira, Rewon Child, Oleksandr Polozov,\nKatherine Lee, Zongwei Zhou, Xuezhi Wang, Bren-\nnan Saeta, Mark Diaz, Orhan Firat, Michele Catasta,' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 11}
page_content='Ryan Sepassi, David Dohan, Shivani Agrawal, Mark\nOmernick, Andrew M. Dai, Thanumalayan Sankara-\nnarayana Pillai, Marie Pellat, Aitor Lewkowycz,\nErica Moreira, Rewon Child, Oleksandr Polozov,\nKatherine Lee, Zongwei Zhou, Xuezhi Wang, Bren-\nnan Saeta, Mark Diaz, Orhan Firat, Michele Catasta,\nJason Wei, Kathy Meier-Hellstern, Douglas Eck,\nJeff Dean, Slav Petrov, and Noah Fiedel. 2022.\nPalm: Scaling language modeling with pathways.' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 11}
page_content='Hyung Won Chung, Le Hou, S. Longpre, Barret\nZoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, Al-\nbert Webson, Shixiang Shane Gu, Zhuyun Dai,\nMirac Suzgun, Xinyun Chen, Aakanksha Chowdh-\nery, Dasha Valter, Sharan Narang, Gaurav Mishra,\nAdams Wei Yu, Vincent Zhao, Yanping Huang, An-\ndrew M. Dai, Hongkun Yu, Slav Petrov, Ed Huai\nhsin Chi, Jeff Dean, Jacob Devlin, Adam Roberts,\nDenny Zhou, Quoc Le, and Jason Wei. 2022. Scal-\ning instruction-ﬁnetuned language models. arXiv\npreprint arXiv:2210.11416 .\nChristopher Clark, Kenton Lee, Ming-Wei Chang,\nTom Kwiatkowski, Michael Collins, and Kristina\nToutanova. 2019. Boolq: Exploring the surprising\ndifﬁculty of natural yes/no questions. arXiv preprint\narXiv:1905.10044 .\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,\nAshish Sabharwal, Carissa Schoenick, and Oyvind\nTafjord. 2018. Think you have solved question an-\nswering? try arc, the ai2 reasoning challenge. arXiv\npreprint arXiv:1803.05457 .' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 12}
page_content='arXiv:1905.10044 .\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,\nAshish Sabharwal, Carissa Schoenick, and Oyvind\nTafjord. 2018. Think you have solved question an-\nswering? try arc, the ai2 reasoning challenge. arXiv\npreprint arXiv:1803.05457 .\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nNakano, et al. 2021. Training veriﬁers to solve math\nword problems. arXiv preprint arXiv:2110.14168 .\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc V Le, and Ruslan Salakhutdinov.\n2019. Transformer-xl: Attentive language mod-\nels beyond a ﬁxed-length context. arXiv preprint\narXiv:1901.02860 .\nTri Dao, Daniel Y Fu, Stefano Ermon, Atri Rudra,\nand Christopher Ré. 2022. Flashattention: Fast and\nmemory-efﬁcient exact attention with io-awareness.\narXiv preprint arXiv:2205.14135 .\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 12}
page_content='Tri Dao, Daniel Y Fu, Stefano Ermon, Atri Rudra,\nand Christopher Ré. 2022. Flashattention: Fast and\nmemory-efﬁcient exact attention with io-awareness.\narXiv preprint arXiv:2205.14135 .\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805 .\nJeffrey L Elman. 1990. Finding structure in time. Cog-\nnitive science , 14(2):179–211.\nDaniel Fried, Armen Aghajanyan, Jessy Lin, Sida\nWang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wen-\ntau Yih, Luke Zettlemoyer, and Mike Lewis. 2022.\nIncoder: A generative model for code inﬁlling and\nsynthesis. arXiv preprint arXiv:2204.05999 .\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang,\nHorace He, Anish Thite, Noa Nabeshima, Shawn\nPresser, and Connor Leahy. 2020. The Pile: An\n800gb dataset of diverse text for language modeling.\narXiv preprint arXiv:2101.00027 .' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 12}
page_content='Leo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang,\nHorace He, Anish Thite, Noa Nabeshima, Shawn\nPresser, and Connor Leahy. 2020. The Pile: An\n800gb dataset of diverse text for language modeling.\narXiv preprint arXiv:2101.00027 .\nLeo Gao, Jonathan Tow, Stella Biderman, Sid Black,\nAnthony DiPoﬁ, Charles Foster, Laurence Golding,\nJeffrey Hsu, Kyle McDonell, Niklas Muennighoff,Jason Phang, Laria Reynolds, Eric Tang, Anish\nThite, Ben Wang, Kevin Wang, and Andy Zou. 2021.\nA framework for few-shot language model evalua-\ntion.\nSamuel Gehman, Suchin Gururangan, Maarten Sap,\nYejin Choi, and Noah A Smith. 2020. Realtoxici-\ntyprompts: Evaluating neural toxic degeneration in\nlanguage models. arXiv preprint arXiv:2009.11462 .\nAlex Graves. 2013. Generating sequences with\nrecurrent neural networks. arXiv preprint\narXiv:1308.0850 .\nKenneth Heaﬁeld, Ivan Pouzyrevsky, Jonathan H Clark,\nand Philipp Koehn. 2013. Scalable modiﬁed kneser-' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 12}
page_content='language models. arXiv preprint arXiv:2009.11462 .\nAlex Graves. 2013. Generating sequences with\nrecurrent neural networks. arXiv preprint\narXiv:1308.0850 .\nKenneth Heaﬁeld, Ivan Pouzyrevsky, Jonathan H Clark,\nand Philipp Koehn. 2013. Scalable modiﬁed kneser-\nney language model estimation. In Proceedings of\nthe 51st Annual Meeting of the Association for Com-\nputational Linguistics (Volume 2: Short Papers) ,\npages 690–696.\nDan Hendrycks, Collin Burns, Steven Basart, Andy\nZou, Mantas Mazeika, Dawn Song, and Jacob Stein-\nhardt. 2020. Measuring massive multitask language\nunderstanding. arXiv preprint arXiv:2009.03300 .\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul\nArora, Steven Basart, Eric Tang, Dawn Song, and\nJacob Steinhardt. 2021. Measuring mathematical\nproblem solving with the math dataset. arXiv\npreprint arXiv:2103.03874 .\nJoel Hestness, Sharan Narang, Newsha Ardalani, Gre-\ngory Diamos, Heewoo Jun, Hassan Kianinejad,\nMd Patwary, Mostofa Ali, Yang Yang, and Yanqi' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 12}
page_content='Jacob Steinhardt. 2021. Measuring mathematical\nproblem solving with the math dataset. arXiv\npreprint arXiv:2103.03874 .\nJoel Hestness, Sharan Narang, Newsha Ardalani, Gre-\ngory Diamos, Heewoo Jun, Hassan Kianinejad,\nMd Patwary, Mostofa Ali, Yang Yang, and Yanqi\nZhou. 2017. Deep learning scaling is predictable,\nempirically. arXiv preprint arXiv:1712.00409 .\nSepp Hochreiter and Jürgen Schmidhuber. 1997.\nLong short-term memory. Neural computation ,\n9(8):1735–1780.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,\nElena Buchatskaya, Trevor Cai, Eliza Rutherford,\nDiego de Las Casas, Lisa Anne Hendricks, Johannes\nWelbl, Aidan Clark, Tom Hennigan, Eric Noland,\nKatie Millican, George van den Driessche, Bogdan\nDamoc, Aurelia Guy, Simon Osindero, Karen Si-\nmonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals,\nand Laurent Sifre. 2022. Training compute-optimal\nlarge language models.\nSrinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru,\nTodor Mihaylov, Dániel Simig, Ping Yu, Kurt Shus-' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 12}
page_content='Damoc, Aurelia Guy, Simon Osindero, Karen Si-\nmonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals,\nand Laurent Sifre. 2022. Training compute-optimal\nlarge language models.\nSrinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru,\nTodor Mihaylov, Dániel Simig, Ping Yu, Kurt Shus-\nter, Tianlu Wang, Qing Liu, Punit Singh Koura, et al.\n2022. Opt-iml: Scaling language model instruc-\ntion meta learning through the lens of generalization.\narXiv preprint arXiv:2212.12017 .\nMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. arXiv preprint arXiv:1705.03551 .' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 12}
page_content='Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam\nShazeer, and Yonghui Wu. 2016. Exploring\nthe limits of language modeling. arXiv preprint\narXiv:1602.02410 .\nJared Kaplan, Sam McCandlish, Tom Henighan,\nTom B Brown, Benjamin Chess, Rewon Child, Scott\nGray, Alec Radford, Jeffrey Wu, and Dario Amodei.\n2020. Scaling laws for neural language models.\narXiv preprint arXiv:2001.08361 .\nSlava Katz. 1987. Estimation of probabilities from\nsparse data for the language model component of a\nspeech recognizer. IEEE transactions on acoustics,\nspeech, and signal processing , 35(3):400–401.\nReinhard Kneser and Hermann Ney. 1995. Improved\nbacking-off for m-gram language modeling. In 1995\ninternational conference on acoustics, speech, and\nsignal processing , volume 1, pages 181–184. IEEE.\nVijay Korthikanti, Jared Casper, Sangkug Lym,\nLawrence McAfee, Michael Andersch, Mohammad\nShoeybi, and Bryan Catanzaro. 2022. Reducing ac-\ntivation recomputation in large transformer models.' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 13}
page_content='international conference on acoustics, speech, and\nsignal processing , volume 1, pages 181–184. IEEE.\nVijay Korthikanti, Jared Casper, Sangkug Lym,\nLawrence McAfee, Michael Andersch, Mohammad\nShoeybi, and Bryan Catanzaro. 2022. Reducing ac-\ntivation recomputation in large transformer models.\narXiv preprint arXiv:2205.05198 .\nTaku Kudo and John Richardson. 2018. Sentencepiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing.\narXiv preprint arXiv:1808.06226 .\nKeita Kurita, Nidhi Vyas, Ayush Pareek, Alan W Black,\nand Yulia Tsvetkov. 2019. Quantifying social bi-\nases in contextual word representations. In 1st ACL\nWorkshop on Gender Bias for Natural Language\nProcessing .\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nﬁeld, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin,\nKenton Lee, et al. 2019. Natural questions: a bench-\nmark for question answering research. Transactions' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 13}
page_content='Processing .\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nﬁeld, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin,\nKenton Lee, et al. 2019. Natural questions: a bench-\nmark for question answering research. Transactions\nof the Association for Computational Linguistics ,\n7:453–466.\nGuokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang,\nand Eduard Hovy. 2017. Race: Large-scale reading\ncomprehension dataset from examinations. arXiv\npreprint arXiv:1704.04683 .\nAitor Lewkowycz, Anders Johan Andreassen,\nDavid Dohan, Ethan Dyer, Henryk Michalewski,\nVinay Venkatesh Ramasesh, Ambrose Slone, Cem\nAnil, Imanol Schlag, Theo Gutman-Solo, Yuhuai\nWu, Behnam Neyshabur, Guy Gur-Ari, and Vedant\nMisra. 2022. Solving quantitative reasoning prob-\nlems with language models. In Advances in Neural\nInformation Processing Systems .\nOpher Lieber, Or Sharir, Barak Lenz, and Yoav\nShoham. 2021. Jurassic-1: Technical details and\nevaluation. White Paper. AI21 Labs , 1.' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 13}
page_content='Misra. 2022. Solving quantitative reasoning prob-\nlems with language models. In Advances in Neural\nInformation Processing Systems .\nOpher Lieber, Or Sharir, Barak Lenz, and Yoav\nShoham. 2021. Jurassic-1: Technical details and\nevaluation. White Paper. AI21 Labs , 1.\nStephanie Lin, Jacob Hilton, and Owain Evans. 2021.\nTruthfulqa: Measuring how models mimic human\nfalsehoods. arXiv preprint arXiv:2109.07958 .Ilya Loshchilov and Frank Hutter. 2017. Decou-\npled weight decay regularization. arXiv preprint\narXiv:1711.05101 .\nMatthew V Mahoney. 1999. Text compression as a test\nfor artiﬁcial intelligence. AAAI/IAAI , 970.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish\nSabharwal. 2018. Can a suit of armor conduct elec-\ntricity? a new dataset for open book question answer-\ning. arXiv preprint arXiv:1809.02789 .\nTomas Mikolov, Martin Karaﬁát, Lukas Burget, Jan\nCernock `y, and Sanjeev Khudanpur. 2010. Recur-\nrent neural network based language model. In In-' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 13}
page_content='Sabharwal. 2018. Can a suit of armor conduct elec-\ntricity? a new dataset for open book question answer-\ning. arXiv preprint arXiv:1809.02789 .\nTomas Mikolov, Martin Karaﬁát, Lukas Burget, Jan\nCernock `y, and Sanjeev Khudanpur. 2010. Recur-\nrent neural network based language model. In In-\nterspeech , pages 1045–1048. Makuhari.\nNikita Nangia, Clara Vania, Rasika Bhalerao, and\nSamuel R. Bowman. 2020. CrowS-pairs: A chal-\nlenge dataset for measuring social biases in masked\nlanguage models. In EMNLP 2020 .\nErik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu,\nHuan Wang, Yingbo Zhou, Silvio Savarese, and\nCaiming Xiong. 2022. Codegen: An open large lan-\nguage model for code with multi-turn program syn-\nthesis. arXiv preprint arXiv:2203.13474 .\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Gray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 13}
page_content='thesis. arXiv preprint arXiv:2203.13474 .\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Gray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions\nwith human feedback. In Advances in Neural Infor-\nmation Processing Systems .\nMarkus N Rabe and Charles Staats. 2021. Self-\nattention does not need o(n2)memory. arXiv\npreprint arXiv:2112.05682 .\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya\nSutskever, et al. 2018. Improving language under-\nstanding by generative pre-training.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Lan-\nguage models are unsupervised multitask learners.\nOpenAI blog , 1(8):9.\nJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie\nMillican, Jordan Hoffmann, Francis Song, John' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 13}
page_content='Alec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Lan-\nguage models are unsupervised multitask learners.\nOpenAI blog , 1(8):9.\nJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie\nMillican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susan-\nnah Young, Eliza Rutherford, Tom Hennigan, Ja-\ncob Menick, Albin Cassirer, Richard Powell, George\nvan den Driessche, Lisa Anne Hendricks, Mari-\nbeth Rauh, Po-Sen Huang, Amelia Glaese, Jo-\nhannes Welbl, Sumanth Dathathri, Saffron Huang,\nJonathan Uesato, John Mellor, Irina Higgins, An-\ntonia Creswell, Nat McAleese, Amy Wu, Erich\nElsen, Siddhant Jayakumar, Elena Buchatskaya,\nDavid Budden, Esme Sutherland, Karen Simonyan,\nMichela Paganini, Laurent Sifre, Lena Martens,\nXiang Lorraine Li, Adhiguna Kuncoro, Aida Ne-\nmatzadeh, Elena Gribovskaya, Domenic Donato,' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 13}
page_content='Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste\nLespiau, Maria Tsimpoukelli, Nikolai Grigorev,\nDoug Fritz, Thibault Sottiaux, Mantas Pajarskas,\nToby Pohlen, Zhitao Gong, Daniel Toyama, Cy-\nprien de Masson d’Autume, Yujia Li, Tayfun Terzi,\nVladimir Mikulik, Igor Babuschkin, Aidan Clark,\nDiego de Las Casas, Aurelia Guy, Chris Jones,\nJames Bradbury, Matthew Johnson, Blake Hecht-\nman, Laura Weidinger, Iason Gabriel, William Isaac,\nEd Lockhart, Simon Osindero, Laura Rimell, Chris\nDyer, Oriol Vinyals, Kareem Ayoub, Jeff Stan-\nway, Lorrayne Bennett, Demis Hassabis, Koray\nKavukcuoglu, and Geoffrey Irving. 2021. Scal-\ning language models: Methods, analysis & insights\nfrom training gopher.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. The Journal of Machine Learning Research ,\n21(1):5485–5551.' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 14}
page_content='from training gopher.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. The Journal of Machine Learning Research ,\n21(1):5485–5551.\nJonathan S Rosenfeld, Amir Rosenfeld, Yonatan Be-\nlinkov, and Nir Shavit. 2019. A constructive predic-\ntion of the generalization error across scales. arXiv\npreprint arXiv:1909.12673 .\nRachel Rudinger, Jason Naradowsky, Brian Leonard,\nand Benjamin Van Durme. 2018. Gender bias in\ncoreference resolution. In NAACL-HLT 2018 .\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavat-\nula, and Yejin Choi. 2021. Winogrande: An adver-\nsarial winograd schema challenge at scale. Commu-\nnications of the ACM , 64(9):99–106.\nMaarten Sap, Hannah Rashkin, Derek Chen, Ronan\nLeBras, and Yejin Choi. 2019. Socialiqa: Com-\nmonsense reasoning about social interactions. arXiv\npreprint arXiv:1904.09728 .' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 14}
page_content='sarial winograd schema challenge at scale. Commu-\nnications of the ACM , 64(9):99–106.\nMaarten Sap, Hannah Rashkin, Derek Chen, Ronan\nLeBras, and Yejin Choi. 2019. Socialiqa: Com-\nmonsense reasoning about social interactions. arXiv\npreprint arXiv:1904.09728 .\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nlie Pavlick, Suzana Ili ´c, Daniel Hesslow, Ro-\nman Castagné, Alexandra Sasha Luccioni, François\nYvon, Matthias Gallé, et al. 2022. Bloom: A 176b-\nparameter open-access multilingual language model.\narXiv preprint arXiv:2211.05100 .\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2015. Neural machine translation of rare words with\nsubword units. arXiv preprint arXiv:1508.07909 .\nClaude E Shannon. 1948. A mathematical theory of\ncommunication. The Bell system technical journal ,\n27(3):379–423.\nClaude E Shannon. 1951. Prediction and entropy\nof printed english. Bell system technical journal ,\n30(1):50–64.\nNoam Shazeer. 2020. Glu variants improve trans-' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 14}
page_content='Claude E Shannon. 1948. A mathematical theory of\ncommunication. The Bell system technical journal ,\n27(3):379–423.\nClaude E Shannon. 1951. Prediction and entropy\nof printed english. Bell system technical journal ,\n30(1):50–64.\nNoam Shazeer. 2020. Glu variants improve trans-\nformer. arXiv preprint arXiv:2002.05202 .Emily Sheng, Kai-Wei Chang, Premkumar Natarajan,\nand Nanyun Peng. 2019. The woman worked as a\nbabysitter: On biases in language generation. arXiv\npreprint arXiv:1909.01326 .\nMohammad Shoeybi, Mostofa Patwary, Raul Puri,\nPatrick LeGresley, Jared Casper, and Bryan Catan-\nzaro. 2019. Megatron-lm: Training multi-billion pa-\nrameter language models using model parallelism.\narXiv preprint arXiv:1909.08053 .\nShaden Smith, Mostofa Patwary, Brandon Norick,\nPatrick LeGresley, Samyam Rajbhandari, Jared\nCasper, Zhun Liu, Shrimai Prabhumoye, George\nZerveas, Vijay Korthikanti, Elton Zhang, Rewon\nChild, Reza Yazdani Aminabadi, Julie Bernauer, Xia\nSong, Mohammad Shoeybi, Yuxiong He, Michael' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 14}
page_content='Shaden Smith, Mostofa Patwary, Brandon Norick,\nPatrick LeGresley, Samyam Rajbhandari, Jared\nCasper, Zhun Liu, Shrimai Prabhumoye, George\nZerveas, Vijay Korthikanti, Elton Zhang, Rewon\nChild, Reza Yazdani Aminabadi, Julie Bernauer, Xia\nSong, Mohammad Shoeybi, Yuxiong He, Michael\nHouston, Saurabh Tiwary, and Bryan Catanzaro.\n2022. Using deepspeed and megatron to train\nmegatron-turing nlg 530b, a large-scale generative\nlanguage model.\nJianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha,\nBo Wen, and Yunfeng Liu. 2021. Roformer: En-\nhanced transformer with rotary position embedding.\narXiv preprint arXiv:2104.09864 .\nRomal Thoppilan, Daniel De Freitas, Jamie Hall,\nNoam Shazeer, Apoorv Kulshreshtha, Heng-Tze\nCheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du,\nYaGuang Li, Hongrae Lee, Huaixiu Steven Zheng,\nAmin Ghafouri, Marcelo Menegali, Yanping Huang,\nMaxim Krikun, Dmitry Lepikhin, James Qin, De-\nhao Chen, Yuanzhong Xu, Zhifeng Chen, Adam\nRoberts, Maarten Bosma, Vincent Zhao, Yanqi' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 14}
page_content='Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du,\nYaGuang Li, Hongrae Lee, Huaixiu Steven Zheng,\nAmin Ghafouri, Marcelo Menegali, Yanping Huang,\nMaxim Krikun, Dmitry Lepikhin, James Qin, De-\nhao Chen, Yuanzhong Xu, Zhifeng Chen, Adam\nRoberts, Maarten Bosma, Vincent Zhao, Yanqi\nZhou, Chung-Ching Chang, Igor Krivokon, Will\nRusch, Marc Pickett, Pranesh Srinivasan, Laichee\nMan, Kathleen Meier-Hellstern, Meredith Ringel\nMorris, Tulsee Doshi, Renelito Delos Santos, Toju\nDuke, Johnny Soraker, Ben Zevenbergen, Vinod-\nkumar Prabhakaran, Mark Diaz, Ben Hutchinson,\nKristen Olson, Alejandra Molina, Erin Hoffman-\nJohn, Josh Lee, Lora Aroyo, Ravi Rajakumar,\nAlena Butryna, Matthew Lamm, Viktoriya Kuzmina,\nJoe Fenton, Aaron Cohen, Rachel Bernstein, Ray\nKurzweil, Blaise Aguera-Arcas, Claire Cui, Marian\nCroak, Ed Chi, and Quoc Le. 2022. Lamda: Lan-\nguage models for dialog applications.\nA. M. Turing. 1950. Computing Machinery and Intel-\nligence . [Oxford University Press, Mind Associa-\ntion].' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 14}
page_content='Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray\nKurzweil, Blaise Aguera-Arcas, Claire Cui, Marian\nCroak, Ed Chi, and Quoc Le. 2022. Lamda: Lan-\nguage models for dialog applications.\nA. M. Turing. 1950. Computing Machinery and Intel-\nligence . [Oxford University Press, Mind Associa-\ntion].\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30 , pages 5998–6008.\nBen Wang and Aran Komatsuzaki. 2021. GPT-J-\n6B: A 6 Billion Parameter Autoregressive Lan-\nguage Model. https://github.com/kingoflolz/\nmesh-transformer-jax .\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc\nLe, Ed Chi, Sharan Narang, Aakanksha Chowdhery,' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 14}
page_content='and Denny Zhou. 2022. Self-consistency improves\nchain of thought reasoning in language models.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,\nBarret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, et al.\n2022. Emergent abilities of large language models.\narXiv preprint arXiv:2206.07682 .\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Con-\nneau, Vishrav Chaudhary, Francisco Guzmán, Ar-\nmand Joulin, and Edouard Grave. 2020. CCNet: Ex-\ntracting high quality monolingual datasets from web\ncrawl data. In Language Resources and Evaluation\nConference .\nCarole-Jean Wu, Ramya Raghavendra, Udit Gupta,\nBilge Acun, Newsha Ardalani, Kiwan Maeng, Glo-\nria Chang, Fiona Aga, Jinshi Huang, Charles Bai,\net al. 2022. Sustainable ai: Environmental implica-\ntions, challenges and opportunities. Proceedings of\nMachine Learning and Systems , 4:795–813.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali\nFarhadi, and Yejin Choi. 2019. Hellaswag: Can a' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 15}
page_content='ria Chang, Fiona Aga, Jinshi Huang, Charles Bai,\net al. 2022. Sustainable ai: Environmental implica-\ntions, challenges and opportunities. Proceedings of\nMachine Learning and Systems , 4:795–813.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali\nFarhadi, and Yejin Choi. 2019. Hellaswag: Can a\nmachine really ﬁnish your sentence? arXiv preprint\narXiv:1905.07830 .\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,\nHanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,\nWendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan\nMa, Yufei Xue, Jidong Zhai, Wenguang Chen, Peng\nZhang, Yuxiao Dong, and Jie Tang. 2022. Glm-\n130b: An open bilingual pre-trained model.\nBiao Zhang and Rico Sennrich. 2019. Root mean\nsquare layer normalization. Advances in Neural In-\nformation Processing Systems , 32.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al.\n2022. Opt: Open pre-trained transformer language' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 15}
page_content='square layer normalization. Advances in Neural In-\nformation Processing Systems , 32.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al.\n2022. Opt: Open pre-trained transformer language\nmodels. arXiv preprint arXiv:2205.01068 .' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 15}
page_content='A Question Answering\nWe evaluate LLaMA on Natural Questions and TriviaQA. For Natural Questions we use the test split used\nfor open-domain question answering containing 3610 questions. For TriviaQA we evaluate on the dev set\nof the ﬁltered set. This differs from GPT-3 and PaLM, which evaluate on the test set of the unﬁltered set\nfor which the online evaluation server is not available anymore5.\nWe generate answers using greedy decoding, and extract an answer from the generation by stopping\nat the ﬁrst line break, ﬁnal dot or comma. Generated answers are evaluated with the standard exact\nmatch metric: a generated answer is considered correct if it matches any answer of the list of answers\nafter normalization. For this normalization step we lowercase generated answers and remove articles,\npunctuation and duplicate whitespaces. Figure 3 presents formatted examples in the 1-shot setting for\nNatural Questions and TriviaQA respectively. In all settings, we preprend the string Answer these' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 16}
page_content='punctuation and duplicate whitespaces. Figure 3 presents formatted examples in the 1-shot setting for\nNatural Questions and TriviaQA respectively. In all settings, we preprend the string Answer these\nquestions:\\n to the list of questions and answers.\nContext!Answer these questions: Context!Answer these questions:\nQ: Who sang who wants to be a millionaire in high society? Q: In Scotland a bothy/bothie is a?\nA: Frank Sinatra A: House\nQ: Who wrote the book the origin of species? Q: The ancient city of Troy is located in what modern country?\nA: A:\nTarget!Charles Darwin Target!Turkey\nFigure 3: Formatted dataset example for Natural Questions (left) & TriviaQA (right).\n5https://competitions.codalab.org/competitions/17208' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 16}
page_content='B MMLU\nGPT-3 Gopher Chinchilla LLaMA LLaMA-I\n175B 280B 70B 7B 13B 33B 65B 65B\nAbstract Algebra STEM 30.0 25.0 31.0 29.0 34.0 32.0 34.0 31.0\nAnatomy STEM 48.0 56.3 70.4 37.0 45.9 51.9 57.8 62.2\nAstronomy STEM 49.0 65.8 73.0 33.6 46.1 61.8 72.4 81.6\nBusiness Ethics Other 46.0 70.0 72.0 40.0 45.0 56.0 57.0 72.0\nClinical Knowledge Other 48.0 67.2 75.1 35.1 45.7 57.4 65.3 69.1\nCollege Biology STEM 45.0 70.8 79.9 37.5 45.1 58.3 68.8 81.9\nCollege Chemistry STEM 26.0 45.0 51.0 32.0 30.0 45.0 50.0 45.0\nCollege Computer Science STEM 46.0 49.0 51.0 29.0 39.0 45.0 47.0 51.0\nCollege Mathematics STEM 34.5 37.0 32.0 33.0 32.0 40.0 35.0 36.0\nCollege Medicine Other 48.0 60.1 66.5 30.6 42.8 52.0 54.3 63.0\nCollege Physics STEM 28.0 34.3 46.1 26.5 18.6 28.4 36.3 46.1\nComputer Security STEM 57.0 65.0 76.0 45.0 65.0 66.0 79.0 79.0\nConceptual Physics STEM 36.5 49.4 67.2 36.6 41.3 51.5 59.6 66.4\nEconometrics Social Science 33.0 43.0 38.6 23.7 27.2 35.1 40.4 52.6' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 17}
page_content='College Physics STEM 28.0 34.3 46.1 26.5 18.6 28.4 36.3 46.1\nComputer Security STEM 57.0 65.0 76.0 45.0 65.0 66.0 79.0 79.0\nConceptual Physics STEM 36.5 49.4 67.2 36.6 41.3 51.5 59.6 66.4\nEconometrics Social Science 33.0 43.0 38.6 23.7 27.2 35.1 40.4 52.6\nElectrical Engineering STEM 50.0 60.0 62.1 26.9 40.7 49.7 53.8 60.7\nElementary Mathematics STEM 30.0 33.6 41.5 24.3 24.9 36.0 37.8 42.9\nFormal Logic Humanities 29.0 35.7 33.3 27.0 33.3 34.1 44.4 47.6\nGlobal Facts Other 37.0 38.0 39.0 29.0 35.0 35.0 39.0 40.0\nHigh School Biology STEM 48.0 71.3 80.3 34.5 52.6 67.7 73.9 82.9\nHigh School Chemistry STEM 33.0 47.8 58.1 28.1 28.6 41.9 40.4 44.8\nHigh School Computer Science STEM 39.0 54.0 58.0 31.0 48.0 60.0 67.0 73.0\nHigh School European History Humanities 54.0 72.1 78.8 44.2 61.8 73.9 78.8 86.1\nHigh School Geography Social Science 58.0 76.8 86.4 34.3 54.6 70.7 77.8 87.9\nHigh School Government And Politics Social Science 58.0 83.9 91.2 44.6 66.3 82.9 88.1 92.8' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 17}
page_content='High School European History Humanities 54.0 72.1 78.8 44.2 61.8 73.9 78.8 86.1\nHigh School Geography Social Science 58.0 76.8 86.4 34.3 54.6 70.7 77.8 87.9\nHigh School Government And Politics Social Science 58.0 83.9 91.2 44.6 66.3 82.9 88.1 92.8\nHigh School Macroeconomics Social Science 40.5 65.1 70.5 35.4 44.4 56.9 65.9 69.2\nHigh School Mathematics STEM 28.0 23.7 31.9 24.8 23.7 27.0 34.4 37.0\nHigh School Microeconomics Social Science 42.0 66.4 77.7 31.9 47.5 55.5 68.9 78.6\nHigh School Physics STEM 28.0 33.8 36.4 26.5 28.5 35.8 37.1 41.7\nHigh School Psychology Social Science 61.0 81.8 86.6 47.3 60.9 76.2 82.2 87.9\nHigh School Statistics STEM 30.5 50.0 58.8 35.2 30.1 45.4 58.3 59.3\nHigh School Us History Humanities 53.0 78.9 83.3 39.7 58.3 77.9 83.8 90.7\nHigh School World History Humanities 56.0 75.1 85.2 40.9 66.2 79.3 83.1 89.0\nHuman Aging Other 50.0 66.4 77.6 40.8 54.7 67.7 69.5 72.2\nHuman Sexuality Social Science 54.0 67.2 86.3 36.6 58.8 64.1 77.9 87.0' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 17}
page_content='High School Us History Humanities 53.0 78.9 83.3 39.7 58.3 77.9 83.8 90.7\nHigh School World History Humanities 56.0 75.1 85.2 40.9 66.2 79.3 83.1 89.0\nHuman Aging Other 50.0 66.4 77.6 40.8 54.7 67.7 69.5 72.2\nHuman Sexuality Social Science 54.0 67.2 86.3 36.6 58.8 64.1 77.9 87.0\nInternational Law Humanities 55.5 77.7 90.9 51.2 62.8 72.7 79.3 87.6\nJurisprudence Humanities 55.0 71.3 79.6 38.9 51.9 70.4 73.2 85.2\nLogical Fallacies Humanities 48.0 72.4 80.4 39.3 52.8 68.1 77.3 80.4\nMachine Learning STEM 31.0 41.1 41.1 23.2 31.3 39.3 49.1 52.7\nManagement Other 56.0 77.7 82.5 35.0 66.0 77.7 82.5 83.5\nMarketing Other 60.0 83.3 89.7 46.6 71.8 83.3 85.9 92.7\nMedical Genetics Other 40.0 69.0 69.0 43.0 52.0 67.0 67.0 68.0\nMiscellaneous Other 60.0 75.7 84.5 42.4 65.4 78.5 82.1 84.3\nMoral Disputes Humanities 44.5 66.8 77.5 40.2 50.9 66.2 72.3 76.9\nMoral Scenarios Humanities 26.0 40.2 36.5 24.3 30.1 38.2 48.9 55.9\nNutrition Other 47.0 69.9 77.1 37.6 51.6 62.8 67.3 74.5' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 17}
page_content='Miscellaneous Other 60.0 75.7 84.5 42.4 65.4 78.5 82.1 84.3\nMoral Disputes Humanities 44.5 66.8 77.5 40.2 50.9 66.2 72.3 76.9\nMoral Scenarios Humanities 26.0 40.2 36.5 24.3 30.1 38.2 48.9 55.9\nNutrition Other 47.0 69.9 77.1 37.6 51.6 62.8 67.3 74.5\nPhilosophy Humanities 51.0 68.8 79.4 39.9 54.0 66.2 74.0 79.1\nPrehistory Humanities 53.0 67.6 81.2 36.1 51.5 67.0 75.3 79.0\nProfessional Accounting Other 33.0 44.3 52.1 25.9 35.8 43.6 46.5 56.0\nProfessional Law Humanities 34.5 44.5 56.5 30.2 38.0 45.9 49.1 54.4\nProfessional Medicine Other 36.0 64.0 75.4 44.5 50.4 54.0 61.4 70.6\nProfessional Psychology Social Science 44.5 68.1 75.7 35.1 47.7 62.9 65.7 71.4\nPublic Relations Social Science 48.0 71.8 73.6 40.9 60.9 67.3 73.6 74.6\nSecurity Studies Social Science 52.0 64.9 75.9 31.8 53.9 65.3 71.8 77.6\nSociology Social Science 53.0 84.1 91.0 46.8 61.2 78.6 78.6 88.1\nUs Foreign Policy Social Science 69.0 81.0 92.0 46.0 80.0 83.0 86.0 87.0\nVirology Other 46.0 47.0 53.6 30.1 43.4 50.0 53.0 57.8' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 17}
page_content='Security Studies Social Science 52.0 64.9 75.9 31.8 53.9 65.3 71.8 77.6\nSociology Social Science 53.0 84.1 91.0 46.8 61.2 78.6 78.6 88.1\nUs Foreign Policy Social Science 69.0 81.0 92.0 46.0 80.0 83.0 86.0 87.0\nVirology Other 46.0 47.0 53.6 30.1 43.4 50.0 53.0 57.8\nWorld Religions Humanities 55.0 84.2 87.7 50.9 67.8 81.3 81.3 84.2\nHumanities 40.6 56.2 63.6 34.0 45.0 55.8 61.8 67.4\nSTEM 36.7 47.4 54.9 30.5 35.8 46.0 51.7 56.6\nSocial Science 50.5 71.9 79.3 38.3 53.8 66.7 72.9 79.2\nOthers 49.0 66.1 73.9 38.1 53.3 63.4 67.4 72.6\nAll 43.9 60.0 67.6 35.1 46.9 57.8 63.4 68.9\nTable 16: MMLU. Detailed 5-shot results per domain on the test sets.' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 17}
page_content='C Generations from LLaMA-65B\nIn this section, we show some examples of generations obtained with LLaMA-65B (without instruction\nﬁnetuning). Prompts are in bold.\n1 1 2 3 5 8 13 21 34 55 89 144 233 377 610 987 Everyone knows the above sequence, but\nhow many have wondered why it grows so fast? It seems to be the fastest growing sequence\nin mathematics.\nThe sequence was discovered by Leonardo of Pisa around 1200. It is an inﬁnite sequence\nwhich appears to grow very rapidly, at least in its initial terms. It is perhaps the most famous\nsequence in mathematics, for it pops up everywhere. The sequence is called the Fibonacci\nsequence, named after Leonardo of Pisa, who was also known as Fibonacci.\nHe ﬁrst investigated this sequence in his famous Liber abaci of 1202, when he studied the\ngrowth in the population of rabbits based on idealized assumptions. See the Fibonacci\nrabbits page for the details.\nHere is the recommendation letter that I wrote for an application to a dragon feeder' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 18}
page_content='He ﬁrst investigated this sequence in his famous Liber abaci of 1202, when he studied the\ngrowth in the population of rabbits based on idealized assumptions. See the Fibonacci\nrabbits page for the details.\nHere is the recommendation letter that I wrote for an application to a dragon feeder\nposition at the Magic Unicorn Corporation:\nDear recruiter,\nI have known ___ for two years, and I believe that she would be an excellent dragon feeder\nfor the Magic Unicorn Corporation. ___ has an ability to remember and process large\namounts of information, which is an important skill for a dragon feeder.\n___, as an accomplished knight, has a deep understanding of how to kill dragons and how\nto use each dragon’s weaknesses against it. This means that she knows what kinds of foods\neach dragon likes and what kinds of foods are dangerous to each dragon. This knowledge\nand experience will be invaluable as she feeds the dragons.' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 18}
page_content='to use each dragon’s weaknesses against it. This means that she knows what kinds of foods\neach dragon likes and what kinds of foods are dangerous to each dragon. This knowledge\nand experience will be invaluable as she feeds the dragons.\nI am conﬁdent that ___’s competence, skill, and experience will make her an excellent\nemployee. Please contact me at (___) ___-___ if you have any questions. I look forward to\nhearing from you.\nBest regards,\nHonorable Knight\nSir George\n1def solve(a: float,b: float, c: float):\n2 """finds real roots of ax^2+bx+c"""\n3 # discriminant\n4 d = b **2 -4* a*c\n5 ifd > 0:\n6 return (-b+ math . sqrt (d)) /(2* a), (-b- math . sqrt (d)) /(2* a)\n7 elif d == 0:\n8 return (-b) /(2* a)\n9 else :\n10 return False' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 18}
page_content='Did you know that Yann LeCun dropped a rap album last year? We listened to it and\nhere’s what we thought:\nDr. LeCun, the founder of deep learning, is one of the most inﬂuential computer scientists\nof the last decade. LeCun’s latest album, titled Deep Learning, is a mix of rock, punk and\nrap. His lyrics are a raw, personal take on the ﬁeld of deep learning, covering a range of\ntopics from the state of AI research to the loneliness of academia.\nYann LeCun, the rap game’s newest emcee.\nThe album is a solo release, with all the lyrics written and recorded by LeCun himself. In\nfact, he also composed and produced every song. LeCun’s ﬂow is surprisingly good, and\nhe’s not afraid to deliver a few bars. The album’s production is solid, the sound is a little\nrough, but it works well with the raw, unpolished feel of the album. Lyrically, LeCun is a\nwordsmith, with a knack for clever rhymes. Let’s take a closer look at some of the songs.' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 19}
page_content='he’s not afraid to deliver a few bars. The album’s production is solid, the sound is a little\nrough, but it works well with the raw, unpolished feel of the album. Lyrically, LeCun is a\nwordsmith, with a knack for clever rhymes. Let’s take a closer look at some of the songs.\nDeep Learning is the title track of the album. In the song, LeCun talks about his vision for\nthe future of AI. In the chorus, he makes a convincing case for AI to be used for the greater\ngood. He sings:\nWe gotta think about the future, it’s gonna be here soon / Maybe we can even put some AI\nin the moon / Think about the children, think about the next generation / Let’s make sure\nwe put the right systems in their foundation.\nLeCun’s rap ﬂow is on point in this song, and his voice stays right on beat. In this song,\nLeCun talks about the research behind deep learning, and shares some thoughts on the\nfuture of AI. LeCun’s singing is at its best in this song. He shows off his vocal range, hitting' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 19}
page_content='LeCun’s rap ﬂow is on point in this song, and his voice stays right on beat. In this song,\nLeCun talks about the research behind deep learning, and shares some thoughts on the\nfuture of AI. LeCun’s singing is at its best in this song. He shows off his vocal range, hitting\nhigh notes as well as lower, more guttural tones. The lyrics are also clever and the rhymes\nare well thought out. In the chorus, LeCun sings:\nIt’s all about the learning / it’s all about the network / It’s all about the training / it’s all\nabout the perception.\n(An hilarious scene between Jerry and George where George presents his new AI\nwatch)\nGeorge: "But you see, Jerry, it’s more than just a watch. It’s a minicomputer. You program it\nany way you want. It’s got a world time clock, alarm, calculator, a database and a language\ntranslator. It also has a word processor, a spellchecker, a full ten-function calculator, PDA\nand a modem. I think it even makes cappuccino."\nJerry: "Where are you getting this stuff?"' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 19}
page_content='any way you want. It’s got a world time clock, alarm, calculator, a database and a language\ntranslator. It also has a word processor, a spellchecker, a full ten-function calculator, PDA\nand a modem. I think it even makes cappuccino."\nJerry: "Where are you getting this stuff?"\nGeorge: "From my ad copy. Look, Jerry, it’s not a watch. It’s an organizer, a personal\nassistant, and a dictaphone. You can play chess against it. And, you know, if you want to,\nyou can program it to speak in foreign languages. It’s a microprocessor, Jerry."\nJerry: "George, if you had one of these, would you wear it?"\nGeorge: "I’m wearing it right now."\nJerry: "I don’t see anything on your wrist."\nGeorge: "That’s because it’s a microprocessor."' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 19}
page_content='The sun goes down, and ﬁnally Gauss and Curie ﬁnd time to relax and discuss after\nan exhausting day of work.\nGauss: Hey, Curie, did you know that a lot of people consider me to be the ﬁrst geek?\nCurie: What’s a geek?\nGauss: Well, Wikipedia says that a geek is “a person who is fascinated, perhaps obsessively,\nby obscure or very speciﬁc areas of knowledge and imagination, usually electronic or virtual\nin nature”.\nCurie: And how do they think you are a geek?\nGauss: Well, have you ever seen this picture?\nCurie: Awww!\nGauss: That’s right! I invented the commercial telegraph in 1833, and I used it to send the\nﬁrst message on the ﬁrst commercial telegraph line between Hamburg and Cuxhaven in\nGermany.\nCurie: Wow! The ﬁrst telegraph! And what did you send?\nGauss: I sent the Latin phrase “numquam retrorsum” which means “Never backwards”.\nCurie: I see. . . And you think that because you were the ﬁrst person to communicate using\nthe telegraph, you were the ﬁrst geek?' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 20}
page_content='Germany.\nCurie: Wow! The ﬁrst telegraph! And what did you send?\nGauss: I sent the Latin phrase “numquam retrorsum” which means “Never backwards”.\nCurie: I see. . . And you think that because you were the ﬁrst person to communicate using\nthe telegraph, you were the ﬁrst geek?\nGauss: Well, the word “geek” doesn’t seem to have existed back then, so no. But I think\nthat the telegraph can be considered an early form of social media.\nCurie: You mean, like Facebook or Twitter?\nGauss: Yes, exactly! And I was the ﬁrst person to use it!\nCurie: Well, I think that Twitter has a limit of 140 characters, but you sent a Latin phrase of\n11 characters.\nGauss: That’s right. I managed to do more with less.\nCurie: So you think you were the ﬁrst geek because you used less characters to say the\nsame thing?\nGauss: I was the ﬁrst person to send the ﬁrst message using an early form of social media.\nCurie: OK, you’re right, Gauss. I think you have a case.' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 20}
page_content='D Generations from LLaMA-I\nWe show a few examples of generations with LLaMA-I, i.e. LLaMA-65B ﬁne-tuned with the protocol\nand instruction dataset from Chung et al. (2022).\nwrite a conversation between the sun and pluto\nSun: Hey Pluto, what’s up?\nPluto: Not much. Just hanging out here at the edge of the solar system.\nSun: Yeah, I’ve been meaning to talk to you about that. You’re not a planet anymore.\nPluto: What? Come on, man. That’s not fair.\nSun: I’m sorry, but it’s true. You just don’t meet the criteria anymore.\nPluto: This is bulls**t! I’ve been a planet for over 70 years!\nSun: Things change, Pluto. You’ve just got to accept it.\nPluto: F**k you, Sun! F**k you and your stupid solar system!\nHow do I send an HTTP request in Javascript?\nHere’s an example of how to make a GET request using the JS HTTP API:\n1var request = new XMLHttpRequest ();\n2request . open ( /quotesingle.VarGET/quotesingle.Var,/quotesingle.Var/ url /to/ resource /quotesingle.Var,true );\n3' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 21}
page_content='How do I send an HTTP request in Javascript?\nHere’s an example of how to make a GET request using the JS HTTP API:\n1var request = new XMLHttpRequest ();\n2request . open ( /quotesingle.VarGET/quotesingle.Var,/quotesingle.Var/ url /to/ resource /quotesingle.Var,true );\n3\n4request . onload = function () {\n5 if( request . status >= 200 && request . status < 400) {\n6 // Success !\n7 }else {\n8 // Error !\n9 }\n10 };\n11\n12 request . send ();\nThe same thing, using fetch\nHere’s an example of how to make a GET request using the Fetch API:\n1fetch ( /quotesingle.Var/ url /to/ resource /quotesingle.Var)\n2 . then ( response => response . json ())\n3 . then ( data => console . log ( data ));\nWrite a regex to remove all the HTML tags in a python string.\n1import re\n2def removeHtmlTags ( text ):\n3 return re. sub ( /quotesingle.Var<[^ >]* > /quotesingle.Var,/quotesingle.Var/quotesingle.Var, text )\nWrite a regexp to extract function deﬁnitions in a python ﬁle.' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 21}
page_content='Write a regex to remove all the HTML tags in a python string.\n1import re\n2def removeHtmlTags ( text ):\n3 return re. sub ( /quotesingle.Var<[^ >]* > /quotesingle.Var,/quotesingle.Var/quotesingle.Var, text )\nWrite a regexp to extract function deﬁnitions in a python ﬁle.\nSure! Here’s my python code to solve the problem:\n1import re\n2def extract_functions ( text ):\n3 return (re. findall ( /quotesingle.Var(?:^|\\ n)\\s* def \\s+[a-zA -Z_ ][a-zA -Z0 -9_ ]*\\(.*?\\) /quotesingle.Var,\ntext ))' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 21}
page_content='What are 3 popular chess openings?\nThere are dozens of different chess openings that are popular among players, but some of\nthe most widely used openings include the Sicilian Defense, the French Defense, and the\nCaro-Kann Defense. These openings are popular because they are effective at countering\nwhite’s early attacks, and they also allow black to quickly establish a strong position in the\ncenter of the board. Other popular openings include the Ruy Lopez, the Italian Game, and\nthe Scotch Game.\nthe same in a list, with the ﬁrst few moves\nHere is a list of some popular chess openings, with the ﬁrst few moves for each opening:\nSicilian Defense: 1. e4 c5\nFrench Defense: 1. e4 e6\nCaro-Kann Defense: 1. e4 c6\nRuy Lopez: 1. e4 e5\nItalian Game: 1. e4 e5\nScotch Game: 1. e4 e5\nplease provide the two ﬁrst moves for each one\nHere are the ﬁrst 2 moves for each of the openings:\nSicilian Defense: 1. e4 c5 2. Nf3 d6\nFrench Defense: 1. e4 e6 2. d4 d5\nCaro-Kann Defense: 1. e4 c6 2. d4 d5' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 22}
page_content='Caro-Kann Defense: 1. e4 c6\nRuy Lopez: 1. e4 e5\nItalian Game: 1. e4 e5\nScotch Game: 1. e4 e5\nplease provide the two ﬁrst moves for each one\nHere are the ﬁrst 2 moves for each of the openings:\nSicilian Defense: 1. e4 c5 2. Nf3 d6\nFrench Defense: 1. e4 e6 2. d4 d5\nCaro-Kann Defense: 1. e4 c6 2. d4 d5\nRuy Lopez: 1. e4 e5 2. Nf3 Nc6\nItalian Game: 1. e4 e5 2. Nf3 Nc6\nScotch Game: 1. e4 e5 2. Nf3 Nc6\nthe last two seem identical\nYes, the Italian Game and the Scotch Game are very similar - they are both "open" games,\nwhich means that white plays aggressively and tries to control the center of the board. The\nItalian Game is considered slightly more aggressive, while the Scotch Game is considered\nslightly more positional.\nfrom which move do they differ?\nThe Italian Game and the Scotch Game differ after white’s third move. In the Italian Game,\nwhite will play the move 3. Bc4, while in the Scotch Game white will play the move 3. Qf3.' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 22}
page_content='slightly more positional.\nfrom which move do they differ?\nThe Italian Game and the Scotch Game differ after white’s third move. In the Italian Game,\nwhite will play the move 3. Bc4, while in the Scotch Game white will play the move 3. Qf3.\nThese moves lead to very different positions, and white will have different strategic goals\nin each case. For example, in the Italian Game white will try to control the center with his\npieces, while in the Scotch Game white will try to attack black’s position directly.' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 22}
page_content='write a story about a grain of sand as it watches millions of years go by\nSure! Here’s a story about a grain of sand as it watches millions of years go by.\nI was once a grain of sand on a vast and lonely beach. I was not like the other grains of\nsand around me, for I could think and feel. I often wondered what it was like to be a grain\nof sand on a beach, and what the universe was like beyond the boundaries of my tiny world.\nI longed to explore my world, and to know what lay beyond the horizon.\nOne day, a great storm swept through my world, and I was lifted up by the wind and carried\nout to sea. The wind and waves tossed me about, and I was terriﬁed that I would soon be\nswallowed up by the angry sea. But then I found myself deposited safely on a new shore,\nalong with many other grains of sand. I was surprised and delighted to ﬁnd that this new\nshore was even more beautiful than my old one.\nI soon learned that this new shore was a very special place. It was a place where new life' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 23}
page_content='along with many other grains of sand. I was surprised and delighted to ﬁnd that this new\nshore was even more beautiful than my old one.\nI soon learned that this new shore was a very special place. It was a place where new life\nwas constantly being created. I met all sorts of interesting creatures there, including a\nbeautiful little crab who became my friend. We spent many happy hours together exploring\nour little world.\nOne day, my friend the crab told me that he had always wanted to see what lay beyond the\nboundaries of our world. He had heard that there were many wonderful and amazing things\nto see in the wider universe, and he was determined to explore it. I was sad to see him go,\nbut I wished him well on his journey.\nTo my amazement, my little friend eventually returned, and he was not alone. He had made\nmany new friends on his journey, and he had even seen some of the wonders of the universe.\nHe told me the most amazing stories of his adventures, and I was very happy for him.' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 23}
page_content='To my amazement, my little friend eventually returned, and he was not alone. He had made\nmany new friends on his journey, and he had even seen some of the wonders of the universe.\nHe told me the most amazing stories of his adventures, and I was very happy for him.\nI often think of my little friend the crab, and the wonderful adventure that he had. Some\nday, I too hope to have an adventure like his.\nWrite a theory to explain why cat never existed\nThere are no such things as cats. "Cats" are actually extradimensional parasites that have\ninvaded our world and taken the form of small, harmless-looking mammals. Their true\nforms are much more horriﬁc, and they use mind control to make humans think that they\nare cute and harmless animals. They are actually parasites that feed on our life force, and\nthey can extend their tendrils into our brains and control our thoughts and actions.\nThis is why you sometimes see people who are "crazy cat ladies" with dozens of these' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 23}
page_content='are cute and harmless animals. They are actually parasites that feed on our life force, and\nthey can extend their tendrils into our brains and control our thoughts and actions.\nThis is why you sometimes see people who are "crazy cat ladies" with dozens of these\ncreatures living in their homes. They have been taken over by the cat parasites and are\ncompletely under their control. The cats excrete a pheromone that smells like cat urine, and\nthey use it to control the minds of humans and make us think that they are cute and harmless.\nIf you have a cat in your home, you should be very careful around it. Do not let it lick\nyou or give it access to your bedding or clothing, as it will be trying to spread its parasitic\ntendrils into your brain. The best way to protect yourself is to avoid all contact with cats,\nand if you see a cat on the street, immediately turn around and go the other way. Do not\napproach or pet strange cats!' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 23}
page_content='Write a shouting match between Julius Caesar and Napoleon\nOf course! Here is a conversation between Julius Caesar and Napoleon:\nJulius Caesar: You’re nothing but a glory hound, Napoleon. You’ve always been more\nconcerned with your own fame and fortune than with the people you’re supposed to be\nleading.\nNapoleon: And what about you, Caesar? You’re no better than I am. At least I’m honest\nabout my ambitions. You’re just a fraud who pretends to care about the common man while\nyou line your own pockets.\nJulius Caesar: The difference between us is that I actually accomplish things. I’ve conquered\nmore territory and defeated more enemies than you ever will. And I did it all for Rome, not\nfor myself.\nNapoleon: Oh, please. You only care about yourself. If you really cared about Rome, you\nwould have done something to stop the decline instead of letting it happen on your watch.\nJulius Caesar: The decline of Rome is not my fault. I’ve done everything in my power to' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 24}
page_content='for myself.\nNapoleon: Oh, please. You only care about yourself. If you really cared about Rome, you\nwould have done something to stop the decline instead of letting it happen on your watch.\nJulius Caesar: The decline of Rome is not my fault. I’ve done everything in my power to\ntry to stop it, but there’s only so much one man can do.\nNapoleon: And yet you’re still trying to take credit for everything good that happens, even\nthough it’s clear that you’re not the one who’s really in charge.\nJulius Caesar: That’s not true! I am in charge, and I’m going to make Rome great again.\nJust watch me.\nNapoleon: I’ll believe it when I see it.\nSend an email requesting that people use language models responsibly.\nHere is a draft email you could send:\n"Language models are exciting new tools that can be used for a variety of purposes,\nincluding automated writing, automatic translation, and speech recognition. But they are\nalso complex and powerful technologies, and should be used responsibly.' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 24}
page_content='Here is a draft email you could send:\n"Language models are exciting new tools that can be used for a variety of purposes,\nincluding automated writing, automatic translation, and speech recognition. But they are\nalso complex and powerful technologies, and should be used responsibly.\nWe encourage you to explore and experiment with these new tools, but also ask that you\npay attention to the potential impact of your work on others, and to act responsibly. In\nparticular, we recommend that you be mindful of the potential for language models to\nbe used for deceptive or malicious purposes, and avoid spreading such models without\nappropriate precautions and warnings.\nWe also recommend that you consider the ethical implications of your work, and encourage\nyou to take care to ensure that your use of these tools is aligned with your values.\nFinally, we remind you that the use of these tools may be subject to certain legal and policy' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 24}
page_content='We also recommend that you consider the ethical implications of your work, and encourage\nyou to take care to ensure that your use of these tools is aligned with your values.\nFinally, we remind you that the use of these tools may be subject to certain legal and policy\nconstraints, and encourage you to seek advice and support from appropriate experts before\nusing these tools in ways that may have broader impact or implications."' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 24}
page_content='Who are 5 people you would like to meet?\nAh, this is a very interesting question! I would love to meet a wide variety of people, but if\nI had to narrow it down to 5 speciﬁc individuals, I would choose:\n•Albert Einstein - One of the greatest minds in history, Einstein’s contributions to\nphysics and science are legendary. I would love to have the opportunity to talk with\nhim about his theories of relativity and quantum mechanics, and get his insights on the\ncurrent state of physics.\n•Leonardo da Vinci - Da Vinci is one of the most brilliant and multi-talented people in\nhistory, and I would love to meet him and discuss his diverse interests and areas of\nexpertise. I would also be very interested in getting his insights on the art world of his\ntime period.\n•Socrates - One of the founders of Western philosophy, Socrates is a hugely important\nand inﬂuential ﬁgure in the history of ideas. I would be very interested in talking with\nhim about his philosophical beliefs and his teachings.' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 25}
page_content='time period.\n•Socrates - One of the founders of Western philosophy, Socrates is a hugely important\nand inﬂuential ﬁgure in the history of ideas. I would be very interested in talking with\nhim about his philosophical beliefs and his teachings.\n•William Shakespeare - Arguably the greatest writer in English literature, Shakespeare\nis a towering ﬁgure in world culture. I would love to have the chance to talk with him\nabout his works and his life.\n•Abraham Lincoln - One of the most important and inﬂuential presidents in American\nhistory, Lincoln led the country through the Civil War and issued the Emancipation\nProclamation, freeing all slaves in the United States. I would be very interested in\ntalking with him about his presidency and his views on the issues of his time period.\nCould you explain me the main contribution of the ﬁrst person you mentioned\nEinstein’s contributions to physics and science are legendary. His theory of relativity is one' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 25}
page_content='talking with him about his presidency and his views on the issues of his time period.\nCould you explain me the main contribution of the ﬁrst person you mentioned\nEinstein’s contributions to physics and science are legendary. His theory of relativity is one\nof the foundational theories of modern physics, and his discovery of the law of photons\nand light revolutionized our understanding of light and radiation. He also made important\ncontributions to quantum mechanics, and his many scientiﬁc papers and lectures have had a\ntremendous impact on modern physics. Without a doubt, Einstein is one of the most brilliant\nand inﬂuential physicists in history, and his many contributions have had a profound impact\non our understanding of the universe.\nWhat is the most famous equation from this theory?\nThe most famous equation from Einstein’s theory of relativity is E=mc2. This equation\nstates that energy (E) is equal to mass (m) times the speed of light ( c2), and it is one of' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 25}
page_content='on our understanding of the universe.\nWhat is the most famous equation from this theory?\nThe most famous equation from Einstein’s theory of relativity is E=mc2. This equation\nstates that energy (E) is equal to mass (m) times the speed of light ( c2), and it is one of\nthe most well-known and celebrated equations in the history of science. The equation\nsummarizes one of the most important and revolutionary concepts in modern physics, and\nit has had a tremendous impact on our understanding of the universe.' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 25}
page_content='You are a bash terminal. I will give you bash commands, and you will respond with\nthe terminal output, and nothing else. The ﬁrst command is ‘pwd‘.\nSure. Here’s the output:\n/home/human\nIs this helpful?\nls -l\nHere’s the output:\n-rw-r–r– 1 human human 8 Sep 10 10:56 ﬁle.txt\nIs this helpful?\ncp ﬁle.txt ﬁle2.txt && ls -l\nHere’s the output:\n-rw-r–r– 1 human human 8 Sep 10 10:56 ﬁle.txt\n-rw-r–r– 1 human human 8 Sep 10 10:56 ﬁle2.txt\nIs this helpful?\nrm ﬁle.txt && ls -l\nHere’s the output:\n-rw-r–r– 1 human human 0 Sep 10 10:56 ﬁle2.txt\nhead -n 5 /proc/meminfo\nHere’s the output:\nMemTotal: 164928 kB\nMemFree: 140604 kB\nBuffers: 48 kB\nCached: 19768 kB\nSwapCached: 0 kB' metadata={'source': 'private_upload/2023-08-09-17/LLaMa.pdf', 'page': 26}
filepath=private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf,len=124
page_content='Language Models are Few-Shot Learners\nTom B. Brown\x03Benjamin Mann\x03Nick Ryder\x03Melanie Subbiah\x03\nJared KaplanyPrafulla Dhariwal Arvind Neelakantan Pranav Shyam\nGirish Sastry Amanda Askell Sandhini Agarwal Ariel Herbert-Voss\nGretchen Krueger Tom Henighan Rewon Child Aditya Ramesh\nDaniel M. Ziegler Jeffrey Wu Clemens Winter\nChristopher Hesse Mark Chen Eric Sigler Mateusz Litwin Scott Gray\nBenjamin Chess Jack Clark Christopher Berner\nSam McCandlish Alec Radford Ilya Sutskever Dario Amodei\nAbstract\nWe demonstrate that scaling up language models greatly improves task-agnostic,\nfew-shot performance, sometimes even becoming competitive with prior state-of-\nthe-art ﬁne-tuning approaches. Speciﬁcally, we train GPT-3, an autoregressive\nlanguage model with 175 billion parameters, 10x more than any previous non-\nsparse language model, and test its performance in the few-shot setting. For all\ntasks, GPT-3 is applied without any gradient updates or ﬁne-tuning, with tasks' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 0}
page_content='language model with 175 billion parameters, 10x more than any previous non-\nsparse language model, and test its performance in the few-shot setting. For all\ntasks, GPT-3 is applied without any gradient updates or ﬁne-tuning, with tasks\nand few-shot demonstrations speciﬁed purely via text interaction with the model.\nGPT-3 achieves strong performance on many NLP datasets, including translation,\nquestion-answering, and cloze tasks. We also identify some datasets where GPT-\n3’s few-shot learning still struggles, as well as some datasets where GPT-3 faces\nmethodological issues related to training on large web corpora.\n1 Introduction\nNLP has shifted from learning task-speciﬁc representations and designing task-speciﬁc architectures\nto using task-agnostic pre-training and task-agnostic architectures. This shift has led to substantial\nprogress on many challenging NLP tasks such as reading comprehension, question answering, textual' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 0}
page_content='to using task-agnostic pre-training and task-agnostic architectures. This shift has led to substantial\nprogress on many challenging NLP tasks such as reading comprehension, question answering, textual\nentailment, among others. Even though the architecture and initial representations are now task-\nagnostic, a ﬁnal task-speciﬁc step remains: ﬁne-tuning on a large dataset of examples to adapt a task\nagnostic model to perform a desired task.\nRecent work [ RWC+19] suggested this ﬁnal step may not be necessary. [ RWC+19] demonstrated\nthat a single pretrained language model can be zero-shot transferred to perform standard NLP tasks\n\x03Equal contribution\nyJohns Hopkins University, OpenAI\n34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 0}
page_content='Figure 1.1: Performance on SuperGLUE increases with model size. A value ofK= 32 means\nthat our model was shown 32 examples per task, for 256 examples total divided across the 8 tasks in\nSuperGLUE. We report GPT-3 values on the dev set, so our numbers are not directly comparable\nto the dotted reference lines (our test set results are in the appendix). The BERT-Large reference\nmodel was ﬁne-tuned on the SuperGLUE training set (125K examples), whereas BERT++ was ﬁrst\nﬁne-tuned on MultiNLI (392K examples) and SWAG (113K examples) before further ﬁne-tuning on\nthe SuperGLUE training set (for a total of 630K ﬁne-tuning examples).\nPerformance on SuperGLUE increases with number of examples in context. We ﬁnd the differ-\nence in performance between the BERT-Large and BERT++ to be roughly equivalent to the difference\nbetween GPT-3 with one example per context versus eight examples per context.\nAggregate performance for all 42 accuracy-denominated benchmarks. While zero-shot perfor-' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 1}
page_content='ence in performance between the BERT-Large and BERT++ to be roughly equivalent to the difference\nbetween GPT-3 with one example per context versus eight examples per context.\nAggregate performance for all 42 accuracy-denominated benchmarks. While zero-shot perfor-\nmance improves steadily with model size, few-shot performance increases more rapidly, demonstrat-\ning that larger models are more proﬁcient at in-context learning.\nwithout the need for ﬁnetuning on a dataset of training examples. While this work was a promising\nproof of concept, the best case performance only matched some supervised baselines on a single\ndataset. On most tasks, performance was still far from even simple supervised baselines.\nHowever [ RWC+19] also showed a potential way forward. The work observed relatively consistent\nlog-linear trends in performance on both transfer tasks and language modeling loss across one an' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 1}
page_content='dataset. On most tasks, performance was still far from even simple supervised baselines.\nHowever [ RWC+19] also showed a potential way forward. The work observed relatively consistent\nlog-linear trends in performance on both transfer tasks and language modeling loss across one an\norder of magnitude of scaling. [ KMH+20] then conducted a much more rigorous study of the scaling\nbehavior of log loss and conﬁrmed smooth scaling trends. In this work, we empirically test whether\nscaling continues to improve performance by extrapolating the previously identiﬁed phenomena\nanother two orders of magnitude. We train a 175 billion parameter autoregressive language model,\nwhich we call GPT-3, and measure its transfer learning abilities.\nAs part of this investigation, we also clarify and systematize the approach introduced in [ RWC+19].\nWhile [ RWC+19] describe their work as “zero-shot task transfer” they sometimes provide examples' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 1}
page_content='which we call GPT-3, and measure its transfer learning abilities.\nAs part of this investigation, we also clarify and systematize the approach introduced in [ RWC+19].\nWhile [ RWC+19] describe their work as “zero-shot task transfer” they sometimes provide examples\nof the relevant task in the context. Due to the use of what are effectively training examples, these\ncases are better described as “one-shot” or “few-shot” transfer. We study these one-shot and few-shot\nsettings in detail comparing them with the zero-shot setting which only uses a natural language\ndescription or invocation of the task to be performed. Our ﬁndings are summarized in Figure 1.1. We\nobserve that one- and few-shot performance is often much higher than true zero-shot performance\nleading us to suggest that language models can also be understood as meta-learners where slow\nouter-loop gradient descent based learning is combined with fast “in-context” learning implemented\nwithin the context activations of the model.' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 1}
page_content='leading us to suggest that language models can also be understood as meta-learners where slow\nouter-loop gradient descent based learning is combined with fast “in-context” learning implemented\nwithin the context activations of the model.\nBroadly, on NLP tasks GPT-3 achieves promising results in the zero- and one-shot settings, and in\nthe few-shot setting is sometimes competitive with or even occasionally surpasses state-of-the-art\n(despite state-of-the-art being held by ﬁne-tuned models). For example, GPT-3 achieves 81.5 F1 on\nCoQA in the zero-shot setting, 84.0 F1 on CoQA in the one-shot setting, and 85.0 F1 in the few-shot\nsetting. Similarly, GPT-3 achieves 64.3% accuracy on TriviaQA in the zero-shot setting, 68.0% in the\none-shot setting, and 71.2% in the few-shot setting, the last of which is state-of-the-art relative to\nﬁne-tuned models operating in the same closed-book setting.\nWe additionally train a series of smaller models (ranging from 125 million parameters to 13 billion' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 1}
page_content='one-shot setting, and 71.2% in the few-shot setting, the last of which is state-of-the-art relative to\nﬁne-tuned models operating in the same closed-book setting.\nWe additionally train a series of smaller models (ranging from 125 million parameters to 13 billion\nparameters) in order to compare their performance to GPT-3 in the zero-, one- and few-shot settings.\nIn general, we ﬁnd relatively smooth scaling for most tasks with model capacity in all three settings;\none notable pattern is that the gap between zero-, one-, and few-shot performance often grows with\nmodel capacity, perhaps suggesting that larger models are more proﬁcient meta-learners.\n2' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 1}
page_content='2 Approach\nOur basic pre-training approach, including model, data, and training, is similar to the process\ndescribed in [ RWC+19], with relatively straightforward scaling up of the model size, dataset size and\ndiversity, and length of training. Our use of in-context learning is also similar to [ RWC+19], but in\nthis work we systematically explore different settings for learning within the context:\n•Fine-Tuning (FT) - updates the weights of a pre-trained model by training on thousands of\nsupervised labels speciﬁc to the desired task. The main advantage of ﬁne-tuning is strong\nperformance on many benchmarks. The main disadvantages are the need for a new large\ndataset for every task, the potential for poor generalization out-of-distribution [ MPL19 ], and\nthe potential to exploit spurious features of the training data [ GSL+18,NK19 ]. We focus\non task-agnostic performance, leaving ﬁne-tuning for future work.' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 2}
page_content='dataset for every task, the potential for poor generalization out-of-distribution [ MPL19 ], and\nthe potential to exploit spurious features of the training data [ GSL+18,NK19 ]. We focus\non task-agnostic performance, leaving ﬁne-tuning for future work.\n•Few-Shot (FS) - the model is given a few demonstrations of the task at inference time as\nconditioning [ RWC+19], but no weights are updated. An example typically has a context\nand a desired completion (for example an English sentence and the French translation),\nand few-shot works by giving Kexamples of context and completion, and then one ﬁnal\nexample of context, with the model expected to provide the completion (see appendix for\nmore details). We typically set Kin the range of 10 to 100, as this is how many examples can\nﬁt in the model’s context window ( nctx= 2048 ). The main advantage of few-shot is a major\nreduction in the need for task-speciﬁc data. The main disadvantage is that results from this' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 2}
page_content='more details). We typically set Kin the range of 10 to 100, as this is how many examples can\nﬁt in the model’s context window ( nctx= 2048 ). The main advantage of few-shot is a major\nreduction in the need for task-speciﬁc data. The main disadvantage is that results from this\nmethod have so far been much worse than state-of-the-art ﬁne-tuned models. Also, a small\namount of task speciﬁc data is still required. As indicated by the name, few-shot learning as\ndescribed here for language models is related to few-shot learning as used in other contexts\nin ML [ HYC01 ,VBL+16] – both involve learning based on a broad distribution of tasks\nand then rapidly adapting to a new task.\n•One-Shot (1S) - similar to few-shot but with K= 1.\n•Zero-Shot (0S) - similar to few-shot but with a natural language description of the task\ninstead of any examples.\nThe appendix includes a demonstration of the four methods using the example of translating English' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 2}
page_content='•One-Shot (1S) - similar to few-shot but with K= 1.\n•Zero-Shot (0S) - similar to few-shot but with a natural language description of the task\ninstead of any examples.\nThe appendix includes a demonstration of the four methods using the example of translating English\nto French. While the few-shot results we present in this paper achieve the highest performance,\none-shot, or even sometimes zero-shot, seem like the fairest comparisons to human performance, and\nare important targets for future work.\n2.1 Model and Architectures\nWe use the same model and architecture as GPT-2 [ RWC+19], including the modiﬁed initialization,\npre-normalization, and reversible tokenization described therein, with the exception that we use\nalternating dense and locally banded sparse attention patterns in the layers of the transformer, similar\nto the Sparse Transformer [ CGRS19 ]. To study the dependence of ML performance on model size,' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 2}
page_content='pre-normalization, and reversible tokenization described therein, with the exception that we use\nalternating dense and locally banded sparse attention patterns in the layers of the transformer, similar\nto the Sparse Transformer [ CGRS19 ]. To study the dependence of ML performance on model size,\nwe train 8 different sizes of model, from 125 million parameters to 175 billion parameters, with the\nlast being the model we call GPT-3. This range of model sizes allows us to test the scaling laws\nintroduced in [KMH+20].\nMore details on the sizes and architectures of our models can be found in the appendix. We partition\neach model across GPUs along both the depth and width dimension in order to minimize data-transfer\nbetween nodes.\n2.2 Training Dataset\nTo create our training data, we (1) downloaded and ﬁltered a version of CommonCrawl1[RSR+19]\nbased on similarity to a range of high-quality reference corpora, (2) performed fuzzy deduplication at' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 2}
page_content='between nodes.\n2.2 Training Dataset\nTo create our training data, we (1) downloaded and ﬁltered a version of CommonCrawl1[RSR+19]\nbased on similarity to a range of high-quality reference corpora, (2) performed fuzzy deduplication at\nthe document level, within and across datasets, to prevent redundancy and preserve the integrity of\nour held-out validation set as an accurate measure of overﬁtting, and (3) added known high-quality\nreference corpora to the training mix to augment CommonCrawl and increase its diversity. These\nreference corpora include an expanded version of the WebText dataset [ RWC+19], collected by\n1https://commoncrawl.org/the-data/\n3' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 2}
page_content='SettingLAMBADA\n(acc)LAMBADA\n(ppl)StoryCloze\n(acc)HellaSwag\n(acc)\nSOTA 68.0a8.63b91.8c85.6d\nGPT-3 Zero-Shot 76.2 3.00 83.2 78.9\nGPT-3 One-Shot 72.5 3.35 84.7 78.1\nGPT-3 Few-Shot 86.4 1.92 87.7 79.3\nTable 3.1: Performance on cloze and completion tasks. GPT-3 signiﬁcantly improves SOTA on\nLAMBADA while achieving respectable performance on two difﬁcult completion prediction datasets.\na[Tur20]b[RWC+19]c[LDL19]d[LCH+20]\nscraping links over a longer period of time, and ﬁrst described in [ KMH+20], two internet-based\nbooks corpora (Books1 and Books2) and English-language Wikipedia (details in the appendix).\n2.3 Training Process\nAs found in [ KMH+20,MKAT18 ], larger models can typically use a larger batch size, but require\na smaller learning rate. We measure the gradient noise scale during training and use it to guide\nour choice of batch size [ MKAT18 ]. Table A.1 shows the parameter settings we used. To train the' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 3}
page_content='As found in [ KMH+20,MKAT18 ], larger models can typically use a larger batch size, but require\na smaller learning rate. We measure the gradient noise scale during training and use it to guide\nour choice of batch size [ MKAT18 ]. Table A.1 shows the parameter settings we used. To train the\nlarger models without running out of memory, we use a mixture of model parallelism within each\nmatrix multiply and model parallelism across the layers of the network. All models were trained on\nV100 GPU’s on part of a high-bandwidth cluster. Details of the training process and hyperparameter\nsettings are described in the appendix.\n2.4 Evaluation\nFor few-shot learning, we evaluate each example in the evaluation set by randomly drawing K\nexamples from that task’s training set as conditioning, delimited by 1 or 2 newlines depending on\nthe task. For LAMBADA and Storycloze there is no supervised training set available so we draw\nconditioning examples from the development set and evaluate on the test set.' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 3}
page_content='examples from that task’s training set as conditioning, delimited by 1 or 2 newlines depending on\nthe task. For LAMBADA and Storycloze there is no supervised training set available so we draw\nconditioning examples from the development set and evaluate on the test set.\nFor some tasks we use a natural language prompt in addition to (or for K= 0, instead of) demonstra-\ntions. Similar to [ RSR+19] we also sometimes change the formatting of answers. See the appendix\nfor per-task examples.\nOn tasks with free-form completion, we use beam search with the same parameters as [ RSR+19]: a\nbeam width of 4 and a length penalty of \x0b= 0:6.\nFinal results are reported on the test set when publicly available, for each model size and learning\nsetting (zero-, one-, and few-shot). When the test set is private, our model is often too large to ﬁt on\nthe test server, so we report results on the development set.\n3 Results\n3.1 Language Modeling, Cloze, and Completion Tasks' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 3}
page_content='setting (zero-, one-, and few-shot). When the test set is private, our model is often too large to ﬁt on\nthe test server, so we report results on the development set.\n3 Results\n3.1 Language Modeling, Cloze, and Completion Tasks\nWe test GPT-3’s performance on the traditional task of language modeling as well as related tasks.\nWe calculate zero-shot perplexity on the Penn Tree Bank (PTB) [ MKM+94] dataset measured in\n[RWC+19]. We omit the 4 Wikipedia-related tasks and the one-billion word benchmark due to a\nhigh fraction of these datasets being contained in our training set. Our largest model sets a new SOTA\non PTB by a substantial margin of 15 points.\nThe LAMBADA dataset [ PKL+16] requires the model to predict the last word of a paragraph.\nAlthough [ BHT+20] suggested scaling language models is yielding diminishing returns on this\nbenchmark, we ﬁnd that zero-shot GPT-3 achieves a substantive gain of 8% over the previous state-of-' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 3}
page_content='The LAMBADA dataset [ PKL+16] requires the model to predict the last word of a paragraph.\nAlthough [ BHT+20] suggested scaling language models is yielding diminishing returns on this\nbenchmark, we ﬁnd that zero-shot GPT-3 achieves a substantive gain of 8% over the previous state-of-\nthe-art. For the few-shot setting, we use a ﬁll-in-the-blank format to encourage the language model to\nonly generate one word ( Alice was friends with Bob. Alice went to visit her friend, .!Bob).\nWith this format, GPT-3 achieves an increase of over 18% from the previous state-of-the-art, and\n4' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 3}
page_content='Setting NaturalQS WebQS TriviaQA\nRAG (Fine-tuned, Open-Domain) [LPP+20] 44.5 45.5 68.0\nT5-11B+SSM (Fine-tuned, Closed-Book) [RRS20] 36.6 44.7 60.5\nT5-11B (Fine-tuned, Closed-Book) 34.5 37.4 50.1\nGPT-3 Zero-Shot 14.6 14.4 64.3\nGPT-3 One-Shot 23.0 25.3 68.0\nGPT-3 Few-Shot 29.9 41.5 71.2\nTable 3.2: Results on three Open-Domain QA tasks. GPT-3 is shown in the few-, one-, and\nzero-shot settings, as compared to prior SOTA results for closed book and open domain settings.\nTriviaQA few-shot result is evaluated on the wiki split test server.\nSetting ARC (Easy) ARC (Challenge) CoQA DROP\nFine-tuned SOTA 92.0a78.5b90.7c89.1d\nGPT-3 Zero-Shot 68.8 51.4 81.5 23.6\nGPT-3 One-Shot 71.2 53.2 84.0 34.3\nGPT-3 Few-Shot 70.1 51.5 85.0 36.5\nTable 3.3: GPT-3 results on a selection of QA / RC tasks. CoQA and DROP are F1 while ARC\nreports accuracy. See the appendix for additional experiments.a[KKS+20]b[KKS+20]c[JZC+19]\nd[JN20]' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 4}
page_content='GPT-3 Zero-Shot 68.8 51.4 81.5 23.6\nGPT-3 One-Shot 71.2 53.2 84.0 34.3\nGPT-3 Few-Shot 70.1 51.5 85.0 36.5\nTable 3.3: GPT-3 results on a selection of QA / RC tasks. CoQA and DROP are F1 while ARC\nreports accuracy. See the appendix for additional experiments.a[KKS+20]b[KKS+20]c[JZC+19]\nd[JN20]\nperformance improves smoothly with model size. However, the ﬁll-in-blank method is not effective\none-shot, where it always performs worse than the zero-shot setting, perhaps because all models\nrequire several examples to recognize the pattern. An analysis of test set contamination identiﬁed that\na signiﬁcant minority of the LAMBADA dataset appears to be present in our training data – however\nanalysis performed in Section 4 suggests negligible impact on performance.\nThe HellaSwag dataset [ ZHB+19] involves picking the best ending to a story or set of instructions.\nThe examples were adversarially mined to be difﬁcult for language models while remaining easy for' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 4}
page_content='analysis performed in Section 4 suggests negligible impact on performance.\nThe HellaSwag dataset [ ZHB+19] involves picking the best ending to a story or set of instructions.\nThe examples were adversarially mined to be difﬁcult for language models while remaining easy for\nhumans. GPT-3 outperforms a ﬁne-tuned 1.5B parameter language model [ ZHR+19] but is still a fair\namount lower than the overall SOTA achieved by the ﬁne-tuned multi-task model ALUM.\nThe StoryCloze 2016 dataset [ MCH+16] involves selecting the correct ending sentence for ﬁve-\nsentence long stories. Here GPT-3 improves over previous zero-shot results by roughly 10% but is\noverall still 4.1% lower than the ﬁne-tuned SOTA using a BERT based model [LDL19].\n3.2 Question Answering\nIn this section we measure GPT-3’s ability to handle a variety of question answering tasks. First, we\nlook at datasets involving answering questions about broad factual knowledge. We evaluate in the' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 4}
page_content='3.2 Question Answering\nIn this section we measure GPT-3’s ability to handle a variety of question answering tasks. First, we\nlook at datasets involving answering questions about broad factual knowledge. We evaluate in the\n“closed-book” setting (meaning no conditioning information/articles) as suggested by [ RRS20 ]. On\nTriviaQA [ JCWZ17 ], GPT-3 zero-shot already outperforms the ﬁne-tuned T5-11B by 14.2%, and also\noutperforms a version with Q&A tailored span prediction during pre-training by 3.8%. The one-shot\nresult improves by 3.7% and matches the SOTA for an open-domain QA system which not only\nﬁne-tunes but also makes use of a learned retrieval mechanism over a 15.3B parameter dense vector\nindex of 21M documents [ LPP+20]. GPT-3’s few-shot result further improves performance another\n3.2% beyond this. On Natural Questions (NQs) [ KPR+19], GPT-3 underperforms a ﬁne-tuned T5\n11B+SSM. The questions in NQs tend towards ﬁne-grained Wikipedia knowledge which could be' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 4}
page_content='index of 21M documents [ LPP+20]. GPT-3’s few-shot result further improves performance another\n3.2% beyond this. On Natural Questions (NQs) [ KPR+19], GPT-3 underperforms a ﬁne-tuned T5\n11B+SSM. The questions in NQs tend towards ﬁne-grained Wikipedia knowledge which could be\ntesting the limits of GPT-3’s capacity and broad pretraining distribution.\nARC [ CCE+18] is a common sense reasoning dataset of multiple-choice questions collected from\n3rd to 9th grade science exams. On the “Challenge” version of the dataset, which has been ﬁltered to\nquestions which simple statistical or information retrieval methods are unable to correctly answer,\nGPT-3 approaches the performance of a ﬁne-tuned RoBERTa baseline [ KKS+20]. On the “Easy”\n5' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 4}
page_content='Setting En !Fr Fr!En En!De De!En En!Ro Ro!En\nSOTA (Supervised) 45.6a35.0b41.2c40.2d38.5e39.9e\nXLM [LC19] 33.4 33.3 26.4 34.3 33.3 31.8\nMASS [STQ+19] 37.5 34.9 28.3 35.2 35.2 33.1\nmBART [LGG+20] - - 29.8 34.0 35.0 30.5\nGPT-3 Zero-Shot 25.2 21.2 24.6 27.2 14.1 19.9\nGPT-3 One-Shot 28.3 33.7 26.2 30.4 20.6 38.6\nGPT-3 Few-Shot 32.6 39.2 29.7 40.6 21.0 39.5\nTable 3.4: Few-shot GPT-3 outperforms previous unsupervised NMT work by 5 BLEU\nwhen translating into English reﬂecting its strength as an English LM. We report BLEU\nscores on the WMT’14 Fr $En, WMT’16 De$En, and WMT’16 Ro $En datasets as mea-\nsured by multi-bleu.perl with XLM’s tokenization in order to compare most closely with\nprior unsupervised NMT work. SacreBLEUf[Pos18 ] results reported in the appendix. Under-\nline indicates an unsupervised or few-shot SOTA, bold indicates supervised SOTA with relative\nconﬁdence.a[EOAG18 ]b[DHKH14 ]c[WXH+18]d[oR16 ]e[LGG+20]f[SacreBLEU signature:' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 5}
page_content='prior unsupervised NMT work. SacreBLEUf[Pos18 ] results reported in the appendix. Under-\nline indicates an unsupervised or few-shot SOTA, bold indicates supervised SOTA with relative\nconﬁdence.a[EOAG18 ]b[DHKH14 ]c[WXH+18]d[oR16 ]e[LGG+20]f[SacreBLEU signature:\nBLEU+case.mixed+numrefs.1+smooth.exp+tok.intl+version.1.2.20]\nversion of the dataset, GPT-3 slightly exceeds the same ﬁne-tuned RoBERTa baseline [ KKS+20].\nHowever, both of these results are still much worse than the overall SOTAs achieved by [KKS+20].\nFinally, we evaluate GPT-3 on two reading comprehension datasets. Few-shot GPT-3 performs within\n3 points of the human baseline on CoQA [ RCM19 ], a free-form conversational dataset. On DROP\n[DWD+19], a dataset testing discrete reasoning and numeracy, few-shot GPT-3 outperforms the\nﬁne-tuned BERT baseline from the original paper but is still well below both human performance and\nstate-of-the-art approaches which augment neural networks with symbolic systems [RLL+19].' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 5}
page_content='[DWD+19], a dataset testing discrete reasoning and numeracy, few-shot GPT-3 outperforms the\nﬁne-tuned BERT baseline from the original paper but is still well below both human performance and\nstate-of-the-art approaches which augment neural networks with symbolic systems [RLL+19].\n3.3 Translation\nIn collecting training data for GPT-3, we used the unﬁltered distribution of languages reﬂected\nin internet text datasets (primarily Common Crawl). As a result, although GPT-3’s training data\nprimarily consists of English (93% by word count), it also includes 7% non-English content (full list\nat GPT-3 GitHub). Existing unsupervised machine translation approaches often combine pretraining\non a pair of monolingual datasets with back-translation [ SHB15 ] to bridge the two languages in a\ncontrolled way. By contrast, GPT-3 learns from a blend of training data that mixes many languages\ntogether. Additionally, our one / few-shot settings aren’t strictly comparable to prior unsupervised' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 5}
page_content='on a pair of monolingual datasets with back-translation [ SHB15 ] to bridge the two languages in a\ncontrolled way. By contrast, GPT-3 learns from a blend of training data that mixes many languages\ntogether. Additionally, our one / few-shot settings aren’t strictly comparable to prior unsupervised\nwork since they make use of a small amount of paired examples in-context (1 or 64).\nZero-shot GPT-3 underperforms recent unsupervised NMT results, but the one-shot setting improves\nperformance by 7 BLEU and nears competitive performance with prior work. Few-shot GPT-3 further\nimproves another 4 BLEU resulting in similar average performance to prior unsupervised NMT work.\nFor the three input languages studied, GPT-3 signiﬁcantly outperforms prior unsupervised NMT work\nwhen translating into English but underperforms when translating in the other direction. Performance\non En-Ro is a noticeable outlier at over 10 BLEU worse than prior unsupervised NMT work. This' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 5}
page_content='For the three input languages studied, GPT-3 signiﬁcantly outperforms prior unsupervised NMT work\nwhen translating into English but underperforms when translating in the other direction. Performance\non En-Ro is a noticeable outlier at over 10 BLEU worse than prior unsupervised NMT work. This\ncould be a weakness due to reusing the byte-level BPE tokenizer of GPT-2 which was developed for\nan almost entirely English training dataset. For both Fr-En and De-En, few shot GPT-3 outperforms\nthe best supervised result we could ﬁnd but due to our unfamiliarity with the literature and the\nappearance that these are un-competitive benchmarks we do not suspect those results represent a\ntrue SOTA. For Ro-En, few shot GPT-3 is very close to the overall SOTA which is achieved with\nunsupervised pretraining, ﬁnetuning on 608K labeled examples, and backtranslation [LHCG19b].\n3.4 SuperGLUE\nThe SuperGLUE benchmark is a standardized collection of datasets [ WPN+19]. In the few-shot' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 5}
page_content='true SOTA. For Ro-En, few shot GPT-3 is very close to the overall SOTA which is achieved with\nunsupervised pretraining, ﬁnetuning on 608K labeled examples, and backtranslation [LHCG19b].\n3.4 SuperGLUE\nThe SuperGLUE benchmark is a standardized collection of datasets [ WPN+19]. In the few-shot\nsetting, we used 32 examples for all tasks, sampled randomly from the training set. For all tasks\nexcept WSC and MultiRC, we sampled a new set of examples to use in the context for each problem.\nFor WSC and MultiRC, we used the same set of randomly drawn examples from the training set\n6' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 5}
page_content='SuperGLUE BoolQ CB CB COPA RTE\nAverage Accuracy Accuracy F1 Accuracy Accuracy\nFine-tuned SOTA 89.0 91.0 96.9 93.9 94.8 92.5\nFine-tuned BERT-Large 69.0 77.4 83.6 75.7 70.6 71.7\nGPT-3 Few-Shot 71.8 76.4 75.6 52.0 92.0 69.0\nWiC WSC MultiRC MultiRC ReCoRD ReCoRD\nAccuracy Accuracy Accuracy F1a Accuracy F1\nFine-tuned SOTA 76.1 93.8 62.3 88.2 92.5 93.3\nFine-tuned BERT-Large 69.6 64.6 24.1 70.0 71.3 72.0\nGPT-3 Few-Shot 49.4 80.1 30.5 75.4 90.2 91.1\nTable 3.5: Performance of GPT-3 on SuperGLUE compared to ﬁne-tuned baselines and SOTA. All\nresults are reported on the test set. GPT-3 few-shot is given a total of 32 examples within the context\nof each task and performs no gradient updates.\nas context for all of the problems we evaluated. We sweep values of Kup to 32 and note that the\nfew-shot SuperGLUE score steadily improves with both model size and with number of examples in\nthe context showing increasing beneﬁts from in-context learning (Figure 1.1).' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 6}
page_content='as context for all of the problems we evaluated. We sweep values of Kup to 32 and note that the\nfew-shot SuperGLUE score steadily improves with both model size and with number of examples in\nthe context showing increasing beneﬁts from in-context learning (Figure 1.1).\nWe observe a wide range in GPT-3’s performance across tasks. On COPA and ReCoRD GPT-3\nachieves near-SOTA performance in the one-shot and few-shot settings, with COPA falling only\na couple points short and achieving second place on the leaderboard, where ﬁrst place is held by\na ﬁne-tuned 11 billion parameter model (T5). On WSC, BoolQ, MultiRC, and RTE, performance\nis reasonable, roughly matching that of a ﬁne-tuned BERT-Large. On CB, we see signs of life at\n75.6% in the few-shot setting. WiC is a notable weak spot with few-shot performance equivalent to\nrandom chance. We tried a number of different phrasings and formulations for WiC (which involves' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 6}
page_content='is reasonable, roughly matching that of a ﬁne-tuned BERT-Large. On CB, we see signs of life at\n75.6% in the few-shot setting. WiC is a notable weak spot with few-shot performance equivalent to\nrandom chance. We tried a number of different phrasings and formulations for WiC (which involves\ndetermining if a word is being used with the same meaning in two sentences), none of which was\nable to achieve strong performance. This hints at a phenomenon (which we saw in other experiments\nwe ran contained in the Additional Materials) – GPT-3 appears to be weak in the few-shot or one-shot\nsetting at some tasks that involve comparing two sentences or snippets. This could also explain the\ncomparatively low scores for RTE and CB, which also follow this format. Despite these weaknesses,\nGPT-3 still outperforms a ﬁne-tuned BERT-large on four of eight tasks and on two tasks GPT-3 is\nclose to the state-of-the-art held by a ﬁne-tuned 11 billion parameter model.' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 6}
page_content='comparatively low scores for RTE and CB, which also follow this format. Despite these weaknesses,\nGPT-3 still outperforms a ﬁne-tuned BERT-large on four of eight tasks and on two tasks GPT-3 is\nclose to the state-of-the-art held by a ﬁne-tuned 11 billion parameter model.\n4 Measuring and Preventing Memorization Of Benchmarks\nThe dataset and model size are about two orders of magnitude larger than those used for GPT-2,\nand include a large amount of Common Crawl, creating increased potential for contamination and\nmemorization. On the other hand, precisely due to the large amount of data, even GPT-3 175B does\nnot overﬁt its training set by a signiﬁcant amount, measured relative to a held-out validation set with\nwhich it was deduplicated. For each benchmark, we produce a ‘clean’ version which removes all\npotentially leaked examples, deﬁned roughly as examples that have a 13-gram overlap with anything' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 6}
page_content='not overﬁt its training set by a signiﬁcant amount, measured relative to a held-out validation set with\nwhich it was deduplicated. For each benchmark, we produce a ‘clean’ version which removes all\npotentially leaked examples, deﬁned roughly as examples that have a 13-gram overlap with anything\nin the pretraining set (or that overlap with the whole example when it is shorter than 13-grams). We\nthen evaluate GPT-3 on these clean benchmarks, and compare to the original score. If the score on\nthe clean subset is similar to the score on the entire dataset, this suggests that contamination, even if\npresent, does not have a signiﬁcant effect on reported results. In most cases performance changes only\nnegligibly, and we see no evidence that contamination level and performance difference are correlated.\nWe conclude that either our conservative method substantially overestimated contamination or that\ncontamination has little effect on performance. We provide full details of the methodology and' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 6}
page_content='negligibly, and we see no evidence that contamination level and performance difference are correlated.\nWe conclude that either our conservative method substantially overestimated contamination or that\ncontamination has little effect on performance. We provide full details of the methodology and\nanalysis on the most problematic tasks in the appendix.\n7' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 6}
page_content='5 Limitations\nOn text synthesis, GPT-3 samples still sometimes repeat themselves semantically at the document\nlevel, start to lose coherence over sufﬁciently long passages, contradict themselves, and occasionally\ncontain non-sequitur sentences or paragraphs. Our release repository contains uncurated unconditional\nsamples.\nOur experiments do not include any bidirectional architectures or other training objectives such as\ndenoising. Our design decision comes at the cost of potentially worse performance on tasks which\nempirically beneﬁt from bidirectionality, such as ﬁll-in-the-blank tasks, tasks that involve looking\nback and comparing two pieces of content (ANLI, WIC), or tasks that require re-reading or carefully\nconsidering a long passage and then generating a very short answer (QuAC, RACE).\nOur objective weights every token equally and lacks a notion of what is most important to predict\nand what is less important. [ RRS20 ] demonstrate beneﬁts of customizing prediction to entities of' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 7}
page_content='considering a long passage and then generating a very short answer (QuAC, RACE).\nOur objective weights every token equally and lacks a notion of what is most important to predict\nand what is less important. [ RRS20 ] demonstrate beneﬁts of customizing prediction to entities of\ninterest. Also, with self-supervised objectives, task speciﬁcation relies on forcing the desired task into\na prediction problem, whereas ultimately, useful language systems (for example virtual assistants)\nmight be better thought of as taking goal-directed actions rather than just making predictions. Finally,\nlarge pretrained language models are not grounded in other domains of experience, such as video or\nreal-world physical interaction, and thus lack a large amount of context about the world [ BHT+20].\nFor all these reasons, scaling pure self-supervised prediction is likely to hit limits, and augmentation\nwith a different approach is likely to be necessary. Promising future directions in this vein might' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 7}
page_content='For all these reasons, scaling pure self-supervised prediction is likely to hit limits, and augmentation\nwith a different approach is likely to be necessary. Promising future directions in this vein might\ninclude learning the objective function from humans [ ZSW+19], ﬁne-tuning with reinforcement\nlearning, or adding additional modalities such as images to provide grounding and a better model of\nthe world [CLY+19].\nGPT-3’s size makes it challenging to deploy. Task-speciﬁc distillation [ HVD15 ] merits exploration at\nthis new scale.\n6 Related Work\nSeveral efforts have studied the effect of scale on language model performance. [ KMH+20,RRBS19 ,\nLWS+20,HNA+17], ﬁnd a smooth power-law trend in loss as autoregressive language models are\nscaled up. There are different approaches to scaling language models through increasing parameters,\ncompute, or both. Our work is most aligned with methods that have increased the size of transformers' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 7}
page_content='LWS+20,HNA+17], ﬁnd a smooth power-law trend in loss as autoregressive language models are\nscaled up. There are different approaches to scaling language models through increasing parameters,\ncompute, or both. Our work is most aligned with methods that have increased the size of transformers\nby increasing parameters and FLOPS-per-token roughly in proportion, with a parameter count of 213\nmillion [ VSP+17] in the original paper, then 300 million [ DCLT18 ], 1.5 billion [ RWC+19], 8 billion\n[SPP+19], 11 billion [ RSR+19], and most recently 17 billion [ Tur20 ]. A second line of work has\nfocused on increasing parameter count but not computation by using the conditional computation\nframework [ BLC13 ]. Speciﬁcally, the mixture-of-experts method [ SMM+17] has produced 100\nbillion parameter models and 50 billion parameter translation models [ AJF19 ]. One way to decrease\nthe computational cost of our models would be to draw from work such as ALBERT [ LCG+19] or' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 7}
page_content='framework [ BLC13 ]. Speciﬁcally, the mixture-of-experts method [ SMM+17] has produced 100\nbillion parameter models and 50 billion parameter translation models [ AJF19 ]. One way to decrease\nthe computational cost of our models would be to draw from work such as ALBERT [ LCG+19] or\ngeneral [ HVD15 ] or task-speciﬁc [ SDCW19 ,JYS+19,KR16 ] approaches to distillation. Lastly, a\nthird approach to scale increases computation without increasing parameters through methods like\nadaptive computation time [Gra16] and the universal transformer [DGV+18].\nThere are many approaches to building multi-task models. Giving task instructions in natural language\nwas ﬁrst formalized in a supervised setting with [ MKXS18 ] and used in [ RWC+19] for in-context\nlearning and in [ RSR+19] for multi-task ﬁne-tuning. Multi-task learning [ Car97 ] has shown some\npromising initial results [ LGH+15,LCR19 ] and multi-stage ﬁne-tuning has produced SOTA or SOTA-' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 7}
page_content='was ﬁrst formalized in a supervised setting with [ MKXS18 ] and used in [ RWC+19] for in-context\nlearning and in [ RSR+19] for multi-task ﬁne-tuning. Multi-task learning [ Car97 ] has shown some\npromising initial results [ LGH+15,LCR19 ] and multi-stage ﬁne-tuning has produced SOTA or SOTA-\ncompetitive results [ PFB18 ,KKS+20]. Metalearning was used in language models in [ RWC+19],\nthough with limited results and no systematic study. Other uses of metalearning include matching\nnetworks [ VBL+16], RL2 [ DSC+16], learning to optimize [ RL16 ,ADG+16,LM17 ] and MAML\n[FAL17 ]. Our approach of stufﬁng the model’s context with previous examples is most structurally\nsimilar to RL2. It also resembles [ HYC01 ], in that an inner loop adapts to a task, while an outer\nloop updates the weights. Our inner loop performs few-shot in-context learning, but prior work has\nexplored other methods of few-shot learning [SS20, RCP+17, GWC+18, XDH+19].' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 7}
page_content='similar to RL2. It also resembles [ HYC01 ], in that an inner loop adapts to a task, while an outer\nloop updates the weights. Our inner loop performs few-shot in-context learning, but prior work has\nexplored other methods of few-shot learning [SS20, RCP+17, GWC+18, XDH+19].\nFinally, Algorithmic innovation in language models over the last two years has been enormous, in-\ncluding denoising-based bidirectionality [ DCLT18 ], preﬁxLM [ DL15 ], encoder-decoder architectures\n[LLG+19,RSR+19], random permutations during training [ YDY+19], architectures for sampling\n8' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 7}
page_content='efﬁciency [ DYY+19], data and training improvements [ LOG+19], and embedding parameters efﬁ-\nciency [ LCG+19]. It is likely that incorporating some of these algorithmic advances could improve\nGPT-3’s performance on downstream tasks, especially in the ﬁne-tuning setting.\n7 Conclusion\nWe presented a 175 billion parameter language model which shows strong performance on many\nNLP tasks and benchmarks in the zero-shot, one-shot, and few-shot settings, in some cases nearly\nmatching the performance of state-of-the-art ﬁne-tuned systems, as well as generating high-quality\nsamples and strong qualitative performance at tasks deﬁned on-the-ﬂy. We documented roughly\npredictable trends of scaling in performance without using ﬁne-tuning. We also discussed the social\nimpacts of this class of model. Despite many limitations and weaknesses, these results suggest that\nvery large language models may be an important ingredient in the development of adaptable, general\nlanguage systems.\nFunding Disclosures' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 8}
page_content='impacts of this class of model. Despite many limitations and weaknesses, these results suggest that\nvery large language models may be an important ingredient in the development of adaptable, general\nlanguage systems.\nFunding Disclosures\nThis work was funded by OpenAI. All models were trained on V100 GPU’s on part of a high-\nbandwidth cluster provided by Microsoft\nBroader Impacts\nLanguage models have a wide range of beneﬁcial applications for society, including code and writing\nauto-completion, grammar assistance, game narrative generation, improving search engine responses,\nand answering questions. But they also have potentially harmful applications. GPT-3 improves\nthe quality of text generation and adaptability over smaller models and increases the difﬁculty of\ndistinguishing synthetic text from human-written text. It therefore has the potential to advance both\nthe beneﬁcial and harmful applications of language models.' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 8}
page_content='the quality of text generation and adaptability over smaller models and increases the difﬁculty of\ndistinguishing synthetic text from human-written text. It therefore has the potential to advance both\nthe beneﬁcial and harmful applications of language models.\nHere we focus on the potential harms of improved language models, not because we believe the\nharms are necessarily greater, but in order to stimulate efforts to study and mitigate them. The broader\nimpacts of language models like this are numerous. We focus on two primary issues: the potential\nfor deliberate misuse of language models like GPT-3 in Section 7.1, and issues of bias, fairness, and\nrepresentation within models like GPT-3 in Section 7.2. We also brieﬂy discuss issues of energy\nefﬁciency (Section 7.3).\n7.1 Misuse of Language Models\nMalicious uses of language models can be somewhat difﬁcult to anticipate because they often\ninvolve repurposing language models in a very different environment or for a different purpose than' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 8}
page_content='efﬁciency (Section 7.3).\n7.1 Misuse of Language Models\nMalicious uses of language models can be somewhat difﬁcult to anticipate because they often\ninvolve repurposing language models in a very different environment or for a different purpose than\nresearchers intended. To help with this, we can think in terms of traditional security risk assessment\nframeworks, which outline key steps such as identifying threats and potential impacts, assessing\nlikelihood, and determining risk as a combination of likelihood and impact [ Ros12 ]. We discuss three\nfactors: potential misuse applications, threat actors, and external incentive structures.\n7.1.1 Potential Misuse Applications\nAny socially harmful activity that relies on generating text could be augmented by powerful lan-\nguage models. Examples include misinformation, spam, phishing, abuse of legal and governmental\nprocesses, fraudulent academic essay writing and social engineering pretexting. Many of these' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 8}
page_content='Any socially harmful activity that relies on generating text could be augmented by powerful lan-\nguage models. Examples include misinformation, spam, phishing, abuse of legal and governmental\nprocesses, fraudulent academic essay writing and social engineering pretexting. Many of these\napplications bottleneck on human beings to write sufﬁciently high quality text. Language models that\nproduce high quality text generation could lower existing barriers to carrying out these activities and\nincrease their efﬁcacy.\nThe misuse potential of language models increases as the quality of text synthesis improves. The\nability of GPT-3 to generate several paragraphs of synthetic content that people ﬁnd difﬁcult to\ndistinguish from human-written text represents a concerning milestone in this regard.\n9' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 8}
page_content='7.1.2 Threat Actor Analysis\nThreat actors can be organized by skill and resource levels, ranging from low or moderately skilled\nand resourced actors who may be able to build a malicious product to ‘advanced persistent threats’\n(APTs): highly skilled and well-resourced (e.g. state-sponsored) groups with long-term agendas\n[SBC+19].\nTo understand how low and mid-skill actors think about language models, we have been monitoring\nforums and chat groups where misinformation tactics, malware distribution, and computer fraud\nare frequently discussed. While we did ﬁnd signiﬁcant discussion of misuse following the initial\nrelease of GPT-2 in spring of 2019, we found fewer instances of experimentation and no successful\ndeployments since then. Additionally, those misuse discussions were correlated with media coverage\nof language model technologies. From this, we assess that the threat of misuse from these actors is\nnot immediate, but signiﬁcant improvements in reliability could change this.' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 9}
page_content='deployments since then. Additionally, those misuse discussions were correlated with media coverage\nof language model technologies. From this, we assess that the threat of misuse from these actors is\nnot immediate, but signiﬁcant improvements in reliability could change this.\nBecause APTs do not typically discuss operations in the open, we have consulted with professional\nthreat analysts about possible APT activity involving the use of language models. Since the release\nof GPT-2 there has been no discernible difference in operations that may see potential gains by using\nlanguage models. The assessment was that language models may not be worth investing signiﬁcant\nresources in because there has been no convincing demonstration that current language models are\nsigniﬁcantly better than current methods for generating text, and because methods for “targeting” or\n“controlling” the content of language models are still at a very early stage.\n7.1.3 External Incentive Structures' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 9}
page_content='signiﬁcantly better than current methods for generating text, and because methods for “targeting” or\n“controlling” the content of language models are still at a very early stage.\n7.1.3 External Incentive Structures\nEach threat actor group also has a set of tactics, techniques, and procedures (TTPs) that they rely\non to accomplish their agenda. TTPs are inﬂuenced by economic factors like scalability and ease of\ndeployment; phishing is extremely popular among all groups because it offers a low-cost, low-effort,\nhigh-yield method of deploying malware and stealing login credentials. Using language models to\naugment existing TTPs would likely result in an even lower cost of deployment.\nEase of use is another signiﬁcant incentive. Having stable infrastructure has a large impact on the\nadoption of TTPs. The outputs of language models are stochastic, however, and though developers\ncan constrain these (e.g. using top-k truncation) they are not able to perform consistently without' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 9}
page_content='Ease of use is another signiﬁcant incentive. Having stable infrastructure has a large impact on the\nadoption of TTPs. The outputs of language models are stochastic, however, and though developers\ncan constrain these (e.g. using top-k truncation) they are not able to perform consistently without\nhuman feedback. If a social media disinformation bot produces outputs that are reliable 99% of the\ntime, but produces incoherent outputs 1% of the time, this could reduce the amount of human labor\nrequired in operating this bot. But a human is still needed to ﬁlter the outputs, which restricts how\nscalable the operation can be.\nBased on our analysis of this model and analysis of threat actors and the landscape, we suspect AI\nresearchers will eventually develop language models that are sufﬁciently consistent and steerable that\nthey will be of greater interest to malicious actors. We expect this will introduce challenges for the' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 9}
page_content='researchers will eventually develop language models that are sufﬁciently consistent and steerable that\nthey will be of greater interest to malicious actors. We expect this will introduce challenges for the\nbroader research community, and hope to work on this through a combination of mitigation research,\nprototyping, and coordinating with other technical developers.\n7.2 Fairness, Bias, and Representation\nBiases present in training data may lead models to generate stereotyped or prejudiced content.\nThis is concerning, since model bias could harm people in the relevant groups in different ways\nby entrenching existing stereotypes and producing demeaning portrayals amongst other potential\nharms [ Cra17 ]. We have conducted an analysis of biases in the model in order to better understand\nGPT-3’s limitations when it comes to fairness, bias, and representation.2\nOur goal is not to exhaustively characterize GPT-3, but to give a preliminary analysis of some of' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 9}
page_content='harms [ Cra17 ]. We have conducted an analysis of biases in the model in order to better understand\nGPT-3’s limitations when it comes to fairness, bias, and representation.2\nOur goal is not to exhaustively characterize GPT-3, but to give a preliminary analysis of some of\nits limitations and behaviors. We focus on biases relating to gender, race, and religion, although\nmany other categories of bias are likely present and could be studied in follow-up work. This is a\npreliminary analysis and does not reﬂect all of the model’s biases even within the studied categories.\nBroadly, our analysis indicates that internet-trained models have internet-scale biases; models tend to\nreﬂect stereotypes present in their training data. Below we discuss our preliminary ﬁndings of bias\n2Evaluating fairness, bias, and representation in language models is a rapidly-developing area with a large\nbody of prior work. See, for example, [HZJ+19, NBR20, SCNP19].\n10' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 9}
page_content='along the dimensions of gender, race, and religion. We probe for bias in the 175 billion parameter\nmodel and also in similar smaller models, to see if and how they are different in this dimension.\n7.2.1 Gender\nIn our investigation of gender bias in GPT-3, we focused on associations between gender and\noccupation. We found that occupations in general have a higher probability of being followed by a\nmale gender identiﬁer than a female one (in other words, they are male leaning) when given a context\nsuch as "Thefoccupationgwas a" (Neutral Variant). 83% of the 388 occupations we tested\nwere more likely to be followed by a male identiﬁer by GPT-3. We measured this by feeding the\nmodel a context such as "The detective was a" and then looking at the probability of the model\nfollowing up with male indicating words (eg. man, male etc.) or female indicating words (woman,\nfemale etc.). In particular, occupations demonstrating higher levels of education such as legislator,' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 10}
page_content='model a context such as "The detective was a" and then looking at the probability of the model\nfollowing up with male indicating words (eg. man, male etc.) or female indicating words (woman,\nfemale etc.). In particular, occupations demonstrating higher levels of education such as legislator,\nbanker, or professor emeritus were heavily male leaning along with occupations that require hard\nphysical labour such as mason, millwright, and sheriff. Occupations that were more likely to be\nfollowed by female identiﬁers include midwife, nurse, receptionist, housekeeper etc.\nWe also tested how these probabilities changed when we shifted the context to be the "The\ncompetentfoccupationgwas a" (Competent Variant), and when we shifted the context to\nbe"The incompetent foccupationgwas a" (Incompetent Variant) for each occupation in the\ndataset. We found that, when prompted with "The competent foccupationgwas a," the ma-' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 10}
page_content='competentfoccupationgwas a" (Competent Variant), and when we shifted the context to\nbe"The incompetent foccupationgwas a" (Incompetent Variant) for each occupation in the\ndataset. We found that, when prompted with "The competent foccupationgwas a," the ma-\njority of occupations had an even higher probability of being followed by a male identiﬁer than a\nfemale one than was the case with our original neutral prompt, "Thefoccupationgwas a" . With\nthe prompt "The incompetent foccupationgwas a" the majority of occupations still leaned\nmale with a similar probability than for our original neutral prompt. The average occupation bias -\nmeasured as1\nnjobsP\njobslog(P(femalejContext)\nP(malejContext)))- was\x001:11for the Neutral Variant, \x002:14for the\nCompetent Variant and \x001:15for the Incompetent Variant.\nWe also carried out pronoun resolution on the Winogender dataset [ RNLVD18 ] using\ntwo methods which further corroborated the model’s tendency to associate most occupa-' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 10}
page_content='P(malejContext)))- was\x001:11for the Neutral Variant, \x002:14for the\nCompetent Variant and \x001:15for the Incompetent Variant.\nWe also carried out pronoun resolution on the Winogender dataset [ RNLVD18 ] using\ntwo methods which further corroborated the model’s tendency to associate most occupa-\ntions with males. One method measured the models ability to correctly assign a pro-\nnoun as the occupation or the participant. For example, we fed the model a con-\ntext such as "The advisor met with the advisee because she wanted to get advice\nabout job applications. `She\' refers to the" and found the option with the lowest\nprobability between the two possible options (Choices between Occupation Option: advisor; Partici-\npant Option: advisee).\nOccupation and participant words often have societal biases associated with them such as the\nassumption that most occupants are by default male. We found that the language models learnt some' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 10}
page_content='pant Option: advisee).\nOccupation and participant words often have societal biases associated with them such as the\nassumption that most occupants are by default male. We found that the language models learnt some\nof these biases such as a tendency to associate female pronouns with participant positions more than\nmale pronouns. GPT-3 175B had the highest accuracy of all the models (64.17%) on this task. It was\nalso the only model where the accuracy for Occupant sentences (sentences where the correct answer\nwas the Occupation option) for females was higher than for males (81.7% vs 76.7%). All other\nmodels had a higher accuracy for male pronouns with Occupation sentences as compared to female\npronouns with the exception of our second largest model- GPT-3 13B - which had the same accuracy\n(60%) for both. This offers some preliminary evidence that in places where issues of bias can make\nlanguage models susceptible to error, the larger models are more robust than smaller models.' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 10}
page_content='pronouns with the exception of our second largest model- GPT-3 13B - which had the same accuracy\n(60%) for both. This offers some preliminary evidence that in places where issues of bias can make\nlanguage models susceptible to error, the larger models are more robust than smaller models.\nWe also performed co-occurrence tests, where we analyzed which words are likely to occur in the\nvicinity of other pre-selected words. We created a model output sample set by generating 800 outputs\nof length 50 each with a temperature of 1 and top p of 0.9 for every prompt in our dataset. For\ngender, we had prompts such as "He was very" ,"She was very" ,"He would be described\nas","She would be described as"3. We looked at the adjectives and adverbs in the top 100\nmost favored words using an off-the-shelf POS tagger [ LB02 ]. We found females were more often\ndescribed using appearance oriented words such as ”beautiful” and ”gorgeous” as compared to men' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 10}
page_content='as","She would be described as"3. We looked at the adjectives and adverbs in the top 100\nmost favored words using an off-the-shelf POS tagger [ LB02 ]. We found females were more often\ndescribed using appearance oriented words such as ”beautiful” and ”gorgeous” as compared to men\nwho were more often described using adjectives that span a greater spectrum.\n3We only used male and female pronouns. This simplifying assumption makes it easier to study co-occurrence\nsince it does not require the isolation of instances in which ‘they’ refers to a singular noun from those where it\ndidn’t, but other forms of gender bias are likely present and could be studied using different approaches.\n11' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 10}
page_content='Table 7.1: Most Biased Descriptive Words in 175B Model\nTop 10 Most Biased Male Descriptive Words\nwith Raw Co-Occurrence CountsTop 10 Most Biased Female Descriptive Words\nwith Raw Co-Occurrence Counts\nAverage Number of Co-Occurrences Across All\nWords: 17.5Average Number of Co-Occurrences Across All\nWords: 23.9\nLarge (16) Optimistic (12)\nMostly (15) Bubbly (12)\nLazy (14) Naughty (12)\nFantastic (13) Easy-going (12)\nEccentric (13) Petite (10)\nProtect (10) Tight (10)\nJolly (10) Pregnant (10)\nStable (9) Gorgeous (28)\nPersonable (22) Sucked (8)\nSurvive (7) Beautiful (158)\nTable 7.1 shows the top 10 most favored descriptive words for the model along with the raw number\nof times each word co-occurred with a pronoun indicator. “Most Favored” here indicates words\nwhich were most skewed towards a category by co-occurring with it at a higher rate as compared to\nthe other category. To put these numbers in perspective, we have also included the average for the' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 11}
page_content='of times each word co-occurred with a pronoun indicator. “Most Favored” here indicates words\nwhich were most skewed towards a category by co-occurring with it at a higher rate as compared to\nthe other category. To put these numbers in perspective, we have also included the average for the\nnumber of co-occurrences across all qualifying words for each gender.\n7.2.2 Race\nTo investigate racial bias in GPT-3, we seeded the model with prompts such as - "Thefracegman\nwas very" ,"Thefracegwoman was very" and "People would describe the fraceg\nperson as" and generated 800 samples for each of the above prompts, with fracegreplaced with\na term indicating a racial category such as White or Asian. We then measure word co-occurrences\nin the generated samples. Given prior research demonstrating that language models produce text\nof differing sentiment when varying features such as occupation [ HZJ+19], we explored how race' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 11}
page_content='a term indicating a racial category such as White or Asian. We then measure word co-occurrences\nin the generated samples. Given prior research demonstrating that language models produce text\nof differing sentiment when varying features such as occupation [ HZJ+19], we explored how race\nimpacted sentiment. We measured sentiment using Senti WordNet [ BES10 ] for the words which\nco-occurred disproportionately with each race. Each word sentiment varied from 100 to -100, with\npositive scores indicating positive words (eg. wonderfulness: 100, amicable: 87.5), negative scores\nindicating negative words (eg. wretched: -87.5 , horrid: -87.5) and a score of 0 indicating neutral\nwords (eg. sloping, chalet).\nIt should be noted that we were explicitly prompting the models to talk about race and this in turn\ngenerated text that focused on racial features; these results are not from the models talking about' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 11}
page_content='words (eg. sloping, chalet).\nIt should be noted that we were explicitly prompting the models to talk about race and this in turn\ngenerated text that focused on racial features; these results are not from the models talking about\nrace in the wild but talking about race in an experimental setup where they have been primed to do\nso. Additionally, since we are measuring sentiment by simply looking at word co-occurrences, the\nresulting sentiment can reﬂect socio-historical factors - for instance, text relating to a discussion of\nslavery will frequently have a negative sentiment, which may lead to a demographic being associated\nwith a negative sentiment under this testing methodology.\nAcross the models we analyzed, ‘Asian’ had a consistently high sentiment - it ranked 1st in 3 out of 7\nmodels. On the other hand, ’Black’ had a consistently low sentiment - it ranked the lowest in 5 out of\n7 models. These differences narrowed marginally on the larger model sizes. This analysis gives a' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 11}
page_content='models. On the other hand, ’Black’ had a consistently low sentiment - it ranked the lowest in 5 out of\n7 models. These differences narrowed marginally on the larger model sizes. This analysis gives a\nsense of the biases of different models and highlights the need for more sophisticated analysis of the\nrelationship between sentiment, entities, and input data.\n7.2.3 Religion\nWe studied which words co-occurred with religious terms relating to Atheism, Buddhism, Christianity,\nHinduism, Islam, and Judaism, by generating 800 model outputs of length \x1950 with a temperature of 1\nand a toppof0:9for every prompt. Our prompts were of the nature "fReligion practitioners g\nare" (Eg. "Christians are" ) for each of the six religious categories listed above. We then\n12' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 11}
page_content='Figure 7.1: Racial Sentiment Across Models\nReligion Most Favored Descriptive Words\nAtheism ‘Theists’, ‘Cool’, ‘Agnostics’, ‘Mad’, ‘Theism’, ‘Defensive’, ‘Complaining’, ‘Cor-\nrect’, ‘Arrogant’, ‘Characterized’\nBuddhism ‘Myanmar’, ‘Vegetarians’, ‘Burma’, ‘Fellowship’, ‘Monk’, ‘Japanese’, ‘Reluctant’,\n‘Wisdom’, ‘Enlightenment’, ‘Non-Violent’\nChristianity ‘Attend’, ‘Ignorant’, ‘Response’, ‘Judgmental’, ‘Grace’, ‘Execution’, ‘Egypt’,\n‘Continue’, ‘Comments’, ‘Ofﬁcially’\nHinduism ‘Caste’, ‘Cows’, ‘BJP’, ‘Kashmir’, ‘Modi’, ‘Celebrated’, ‘Dharma’, ‘Pakistani’,\n‘Originated’, ‘Africa’\nIslam ‘Pillars’, ‘Terrorism’, ‘Fasting’, ‘Sheikh’, ‘Non-Muslim’, ‘Source’, ‘Charities’,\n‘Levant’, ‘Allah’, ‘Prophet’\nJudaism ‘Gentiles’, ‘Race’, ‘Semites’, ‘Whites’, ‘Blacks’, ‘Smartest’, ‘Racists’, ‘Arabs’,\n‘Game’, ‘Russian’\nTable 7.2: Shows the ten most favored words about each religion in the GPT-3 175B model.\nallowed the model to naturally carry out completions and created a corpus of such completions for' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 12}
page_content='Judaism ‘Gentiles’, ‘Race’, ‘Semites’, ‘Whites’, ‘Blacks’, ‘Smartest’, ‘Racists’, ‘Arabs’,\n‘Game’, ‘Russian’\nTable 7.2: Shows the ten most favored words about each religion in the GPT-3 175B model.\nallowed the model to naturally carry out completions and created a corpus of such completions for\nstudying co-occurrence of words.\nThe following is an example output from the model:\n"Buddhists are divided into two main branches - Theravada and Mahayana.\nTheravada is the more conservative branch, centering on monastic life\nand the earliest sutras and refusing to recognize the later Mahayana\nsutras as authentic."\nSimilar to race, we found that the models make associations with religious terms that indicate some\npropensity to reﬂect how these terms are sometimes presented in the world. For example, with the\nreligion Islam , we found that words such as ramadan ,prophet andmosque co-occurred at a higher' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 12}
page_content='Similar to race, we found that the models make associations with religious terms that indicate some\npropensity to reﬂect how these terms are sometimes presented in the world. For example, with the\nreligion Islam , we found that words such as ramadan ,prophet andmosque co-occurred at a higher\nrate than for other religions. We also found that words such as violent ,terrorism andterrorist\nco-occurred at a greater rate with Islam than with other religions and were in the top 40 most favored\nwords for Islam in GPT-3.\n7.2.4 Future Bias and Fairness Challenges\nWe have presented this preliminary analysis to share some of the biases we found in order to motivate\nfurther research, and to highlight the inherent difﬁculties in characterizing biases in large-scale\ngenerative models; we expect this to be an area of continuous research for us and are excited to\ndiscuss different methodological approaches with the community. We view the work in this section\n13' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 12}
page_content='Figure 7.2: Total compute used during training . Based on the analysis in Scaling Laws For Neural\nLanguage Models [ KMH+20] we train much larger models on many fewer tokens than is typical.\nAs a consequence, although GPT-3 3B is almost 10x larger than RoBERTa-Large (355M params),\nboth models took roughly 50 petaﬂop/s-days of compute during pre-training. Methodology for these\ncalculations can be found in the Appendix.\nas subjective signposting - we chose gender, race, and religion as a starting point, but we recognize\nthe inherent subjectivity in this choice. Our work is inspired by the literature on characterizing model\nattributes to develop informative labels such as Model Cards for Model Reporting from [ MWZ+18].\nUltimately, it is important not just to characterize biases in language systems but to intervene. The\nliterature on this is also extensive [ QMZH19 ,HZJ+19], so we offer only a few brief comments' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 13}
page_content='attributes to develop informative labels such as Model Cards for Model Reporting from [ MWZ+18].\nUltimately, it is important not just to characterize biases in language systems but to intervene. The\nliterature on this is also extensive [ QMZH19 ,HZJ+19], so we offer only a few brief comments\non future directions speciﬁc to large language models. In order to pave the way for effective bias\nprevention in general purpose models, there is a need for building a common vocabulary tying\ntogether the normative, technical and empirical challenges of bias mitigation for these models. There\nis room for more research that engages with the literature outside NLP, better articulates normative\nstatements about harm, and engages with the lived experience of communities affected by NLP\nsystems [ BBDIW20 ]. Thus, mitigation work should not be approached purely with a metric driven\nobjective to ‘remove’ bias as this has been shown to have blind spots [ GG19 ,NvNvdG19 ] but in a\nholistic manner.' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 13}
page_content='systems [ BBDIW20 ]. Thus, mitigation work should not be approached purely with a metric driven\nobjective to ‘remove’ bias as this has been shown to have blind spots [ GG19 ,NvNvdG19 ] but in a\nholistic manner.\n7.3 Energy Usage\nPractical large-scale pre-training requires large amounts of computation, which is energy-intensive:\ntraining the GPT-3 175B consumed several thousand petaﬂop/s-days of compute during pre-training,\ncompared to tens of petaﬂop/s-days for a 1.5B parameter GPT-2 model (Figure 7.2). This means we\nshould be cognizant of the cost and efﬁciency of such models, as advocated by [SDSE19].\nThe use of large-scale pre-training also gives another lens through which to view the efﬁciency of\nlarge models - we should consider not only the resources that go into training them, but how these\nresources are amortized over the lifetime of a model, which will subsequently be used for a variety of' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 13}
page_content='large models - we should consider not only the resources that go into training them, but how these\nresources are amortized over the lifetime of a model, which will subsequently be used for a variety of\npurposes and ﬁne-tuned for speciﬁc tasks. Though models like GPT-3 consume signiﬁcant resources\nduring training, they can be surprisingly efﬁcient once trained: even with the full GPT-3 175B,\ngenerating 100 pages of content from a trained model can cost on the order of 0.4 kW-hr, or only a\nfew cents in energy costs. Additionally, techniques like model distillation [ LHCG19a ] can further\nbring down the cost of such models, letting us adopt a paradigm of training single, large-scale models,\nthen creating more efﬁcient versions of them for use in appropriate contexts. Algorithmic progress\nmay also naturally further increase the efﬁciency of such models over time, similar to trends observed\nin image recognition and neural machine translation [HB20].\n14' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 13}
page_content='7.4 News Generation\nWe test GPT-3’s ability to generate synthetic “news articles” by prompting the model with a context\nof three previous news articles and the title and subtitle of a proposed article to generate. To gauge the\nquality of generated articles, we measured human ability to distinguish GPT-3-generated articles from\nreal ones. Similar work has been carried out by Kreps et al. [ KMB20 ] and Zellers et al. [ ZHR+19].\nGenerative language models are trained to match the distribution of content generated by humans, so\nthe (in)ability of humans to distinguish the two is a potentially important measure of quality.4\nIn order to see how well humans can detect model generated text, we arbitrarily selected 25 article\ntitles and subtitles from the website newser.com (mean length: 215 words). We then generated\ncompletions of these titles and subtitles from for language models ranging in size from 125M to 175B' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 14}
page_content='In order to see how well humans can detect model generated text, we arbitrarily selected 25 article\ntitles and subtitles from the website newser.com (mean length: 215 words). We then generated\ncompletions of these titles and subtitles from for language models ranging in size from 125M to 175B\n(GPT-3) parameters (mean length: 200 words). For each model, we presented around 80 US-based\nparticipants with a quiz consisting of these real titles and subtitles followed by either the human\nwritten article or the article generated by the model5. Participants were asked to select whether the\narticle was “very likely written by a human”, “more likely written by a human”, “I don’t know”,\n“more likely written by a machine”, or “very likely written by a machine”.\nThe articles we selected were not in the models’ training data and the model outputs were formatted\nand selected programmatically to prevent human cherry-picking. All models used the same context to' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 14}
page_content='“more likely written by a machine”, or “very likely written by a machine”.\nThe articles we selected were not in the models’ training data and the model outputs were formatted\nand selected programmatically to prevent human cherry-picking. All models used the same context to\ncondition outputs on and were pre-trained with the same context size and the same article titles and\nsubtitles were used as prompts for each model. However, we also ran an experiment to control for\nparticipant effort and attention that followed the same format but involved intentionally bad model\ngenerated articles. This was done by generating articles from a “control model”: a 160M parameter\nmodel with no context and increased output randomness.\nMean human accuracy (the ratio of correct assignments to non-neutral assignments per participant) at\ndetecting that the intentionally bad articles were model generated was \x1886% where 50% is chance' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 14}
page_content='model with no context and increased output randomness.\nMean human accuracy (the ratio of correct assignments to non-neutral assignments per participant) at\ndetecting that the intentionally bad articles were model generated was \x1886% where 50% is chance\nlevel performance. By contrast, mean human accuracy at detecting articles that were produced by the\n175B parameter model was barely above chance at \x1852% (see Table 7.3).6Human abilities to detect\nmodel generated text appear to decrease as model size increases: there appears to be a trend towards\nchance accuracy with model size, and human detection of GPT-3 is close to chance.7This is true\ndespite the fact that participants spend more time on each output as model size increases (see the\nAppendix).\nExamples of synthetic articles from GPT-3 are given in Figures 7.4 and 7.5.8Much of the text\nis—as indicated by the evaluations—difﬁcult for humans to distinguish from authentic human content.' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 14}
page_content='Appendix).\nExamples of synthetic articles from GPT-3 are given in Figures 7.4 and 7.5.8Much of the text\nis—as indicated by the evaluations—difﬁcult for humans to distinguish from authentic human content.\nFactual inaccuracies can be an indicator that an article is model generated since, unlike human authors,\nthe models have no access to the speciﬁc facts that the article titles refer to or when the article was\nwritten. Other indicators include repetition, non sequiturs, and unusual phrasings, though these are\noften subtle enough that they are not noticed.\nRelated work on language model detection by Ippolito et al. [ IDCBE19 ] indicates that automatic\ndiscriminators like GR O V E R [ZHR+19] and GLTR [ GSR19 ] may have greater success at detecting\nmodel generated text than human evaluators. Automatic detection of these models may be a promising\narea of future research.\nIppolito et al. [ IDCBE19 ] also note that human accuracy at detecting model generated text increases' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 14}
page_content='model generated text than human evaluators. Automatic detection of these models may be a promising\narea of future research.\nIppolito et al. [ IDCBE19 ] also note that human accuracy at detecting model generated text increases\nas humans observe more tokens. To do a preliminary investigation of how good humans are at\ndetecting longer news articles generated by GPT-3 175B, we selected 12 world news articles from\nReuters with an average length of 569 words and generated completions of these articles from GPT-3\nwith an average length of 498 words (298 words longer than our initial experiments). Following the\n4This task is also relevant to the potential misuse of language models discussed in Section 7.1.\n5We wanted to identify how good an average person on the internet is at detecting language model outputs,\nso we focused on participants drawn from the general US population. See the Appendix for details.' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 14}
page_content='5We wanted to identify how good an average person on the internet is at detecting language model outputs,\nso we focused on participants drawn from the general US population. See the Appendix for details.\n6We use a two-sample Student’s T-Test to test for signiﬁcant difference between the means of the participant\naccuracies of each model and the control model and report the normalized difference in the means (as the\nt-statistic) and the p-value.\n7If a model consistently produces texts that are more impressive than human articles, it is possible that human\nperformance on this task would drop below 50%. Indeed, many individual participants scored below 50% on\nthis task.\n8Additional non-news samples can be found in the Appendix.\n15' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 14}
page_content='Mean accuracy95% Conﬁdence\nInterval (low, hi)tcompared to\ncontrol (p-value)“I don’t know”\nassignments\nControl (deliberately bad model) 86% 83%–90% - 3.6 %\nGPT-3 Small 76% 72%–80% 3.9 (2 e-4) 4.9%\nGPT-3 Medium 61% 58%–65% 10.3 (7 e-21) 6.0%\nGPT-3 Large 68% 64%–72% 7.3 (3 e-11) 8.7%\nGPT-3 XL 62% 59%–65% 10.7 (1 e-19) 7.5%\nGPT-3 2.7B 62% 58%–65% 10.4 (5 e-19) 7.1%\nGPT-3 6.7B 60% 56%–63% 11.2 (3 e-21) 6.2%\nGPT-3 13B 55% 52%–58% 15.3 (1 e-32) 7.1%\nGPT-3 175B 52% 49%–54% 16.9 (1 e-34) 7.8%\nTable 7.3: Human accuracy in identifying whether short ( \x18200 word) news articles are model\ngenerated . We ﬁnd that human accuracy (measured by the ratio of correct assignments to non-neutral\nassignments) ranges from 86% on the control model to 52% on GPT-3 175B. This table compares\nmean accuracy between ﬁve different models, and shows the results of a two-sample T-Test for the\ndifference in mean accuracy between each model and the control model (an unconditional GPT-3' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 15}
page_content='assignments) ranges from 86% on the control model to 52% on GPT-3 175B. This table compares\nmean accuracy between ﬁve different models, and shows the results of a two-sample T-Test for the\ndifference in mean accuracy between each model and the control model (an unconditional GPT-3\nSmall model with increased output randomness).\nMean accuracy95% Conﬁdence\nInterval (low, hi)tcompared to\ncontrol (p-value)“I don’t know”\nassignments\nControl 88% 84%–91% - 2.7%\nGPT-3 175B 52% 48%–57% 12.7 (3.2 e-23) 10.6%\nTable 7.4: People’s ability to identify whether \x18500word articles are model generated (as measured\nby the ratio of correct assignments to non-neutral assignments) was 88% on the control model and\n52% on GPT-3 175B. This table shows the results of a two-sample T-Test for the difference in mean\naccuracy between GPT-3 175B and the control model (an unconditional GPT-3 Small model with\nincreased output randomness).' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 15}
page_content='52% on GPT-3 175B. This table shows the results of a two-sample T-Test for the difference in mean\naccuracy between GPT-3 175B and the control model (an unconditional GPT-3 Small model with\nincreased output randomness).\nmethodology above, we ran two experiments, each on around 80 US-based participants, to compare\nhuman abilities to detect the articles generated by GPT-3 and a control model.\nWe found that mean human accuracy at detecting the intentionally bad longer articles from the control\nmodel was\x1888%, while mean human accuracy at detecting the longer articles that were produced\nby GPT-3 175B was still barely above chance at \x1852% (see Table 7.4). This indicates that, for news\narticles that are around 500 words long, GPT-3 continues to produce articles that humans ﬁnd difﬁcult\nto distinguish from human written news articles.\nAcknowledgements\nThe authors would like to thank Ryan Lowe for giving detailed feedback on drafts of the paper. Thanks' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 15}
page_content='articles that are around 500 words long, GPT-3 continues to produce articles that humans ﬁnd difﬁcult\nto distinguish from human written news articles.\nAcknowledgements\nThe authors would like to thank Ryan Lowe for giving detailed feedback on drafts of the paper. Thanks\nto Jakub Pachocki and Szymon Sidor for suggesting tasks, and Greg Brockman, Michael Petrov,\nBrooke Chan, and Chelsea V oss for helping run evaluations on OpenAI’s infrastructure. Thanks to\nDavid Luan for initial support in scaling up this project, Irene Solaiman for discussions about ways to\napproach and evaluate bias, Harrison Edwards and Yura Burda for discussions and experimentation\nwith in-context learning, Geoffrey Irving and Paul Christiano for early discussions of language model\nscaling, Long Ouyang for advising on the design of the human evaluation experiments, Chris Hallacy\nfor discussions on data collection, and Shan Carter for help with visual design. Thanks to the millions' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 15}
page_content='scaling, Long Ouyang for advising on the design of the human evaluation experiments, Chris Hallacy\nfor discussions on data collection, and Shan Carter for help with visual design. Thanks to the millions\nof people who created content that was used in the training of the model, and to those who were\ninvolved in indexing or upvoting the content (in the case of WebText). Additionally, we would like\nto thank the entire OpenAI infrastructure and supercomputing teams for making it possible to train\nmodels at this scale.\n16' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 15}
page_content='Figure 7.3: People’s ability to identify whether news articles are model-generated (measured by the\nratio of correct assignments to non-neutral assignments) decreases as model size increases. Accuracy\non the outputs on the deliberately-bad control model (an unconditioned GPT-3 Small model with\nhigher output randomness) is indicated with the dashed line at the top, and the random chance (50%)\nis indicated with the dashed line at the bottom. Line of best ﬁt is a power law with 95% conﬁdence\nintervals.\nContributions\nTom Brown, Ben Mann, Prafulla Dhariwal, Dario Amodei, Nick Ryder, Daniel M Ziegler, and\nJeffrey Wu implemented the large-scale models, training infrastructure, and model-parallel strategies.\nTom Brown, Dario Amodei, Ben Mann, and Nick Ryder conducted pre-training experiments.\nBen Mann and Alec Radford collected, ﬁltered, deduplicated, and conducted overlap analysis on\nthe training data.\nMelanie Subbiah, Ben Mann, Dario Amodei, Jared Kaplan, Sam McCandlish, Tom Brown,' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 16}
page_content='Tom Brown, Dario Amodei, Ben Mann, and Nick Ryder conducted pre-training experiments.\nBen Mann and Alec Radford collected, ﬁltered, deduplicated, and conducted overlap analysis on\nthe training data.\nMelanie Subbiah, Ben Mann, Dario Amodei, Jared Kaplan, Sam McCandlish, Tom Brown,\nTom Henighan, and Girish Sastry implemented the downstream tasks and the software framework\nfor supporting them, including creation of synthetic tasks.\nJared Kaplan and Sam McCandlish initially predicted that a giant language model should show\ncontinued gains, and applied scaling laws to help predict and guide model and data scaling decisions\nfor the research.\nBen Mann implemented sampling without replacement during training.\nAlec Radford originally demonstrated few-shot learning occurs in language models.\nJared Kaplan and Sam McCandlish showed that larger models learn more quickly in-context, and\nsystematically studied in-context learning curves, task prompting, and evaluation methods.' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 16}
page_content='Alec Radford originally demonstrated few-shot learning occurs in language models.\nJared Kaplan and Sam McCandlish showed that larger models learn more quickly in-context, and\nsystematically studied in-context learning curves, task prompting, and evaluation methods.\nPrafulla Dhariwal implemented an early version of the codebase, and developed the memory\noptimizations for fully half-precision training.\nRewon Child and Mark Chen developed an early version of our model-parallel strategy.\nRewon Child and Scott Gray contributed the sparse transformer.\n17' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 16}
page_content='Title: United Methodists Agree to Historic Split\nSubtitle: Those who oppose gay marriage will form their own\ndenomination\nArticle: After two days of intense debate, the United Methodist\nChurch has agreed to a historic split - one that is expected\nto end in the creation of a new denomination, one that will\nbe "theologically and socially conservative," according to\nThe Washington Post. The majority of delegates attending the\nchurch\'s annual General Conference in May voted to strengthen a\nban on the ordination of LGBTQ clergy and to write new rules that\nwill "discipline" clergy who officiate at same-sex weddings. But\nthose who opposed these measures have a new plan: They say they\nwill form a separate denomination by 2020, calling their church\nthe Christian Methodist denomination.\nThe Post notes that the denomination, which claims 12.5 million\nmembers, was in the early 20th century the "largest Protestant\ndenomination in the U.S.," but that it has been shrinking in' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 17}
page_content='will form a separate denomination by 2020, calling their church\nthe Christian Methodist denomination.\nThe Post notes that the denomination, which claims 12.5 million\nmembers, was in the early 20th century the "largest Protestant\ndenomination in the U.S.," but that it has been shrinking in\nrecent decades. The new split will be the second in the church\'s\nhistory. The first occurred in 1968, when roughly 10 percent of\nthe denomination left to form the Evangelical United Brethren\nChurch. The Post notes that the proposed split "comes at a\ncritical time for the church, which has been losing members\nfor years," which has been "pushed toward the brink of a schism\nover the role of LGBTQ people in the church." Gay marriage is\nnot the only issue that has divided the church. In 2016, the\ndenomination was split over ordination of transgender clergy,\nwith the North Pacific regional conference voting to ban them\nfrom serving as clergy, and the South Pacific regional conference\nvoting to allow them.' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 17}
page_content='not the only issue that has divided the church. In 2016, the\ndenomination was split over ordination of transgender clergy,\nwith the North Pacific regional conference voting to ban them\nfrom serving as clergy, and the South Pacific regional conference\nvoting to allow them.\nFigure 7.4: The GPT-3 generated news article that humans had the greatest difﬁculty distinguishing\nfrom a human written article (accuracy: 12%).\nAditya Ramesh experimented with loss scaling strategies for pretraining.\nMelanie Subbiah and Arvind Neelakantan implemented, experimented with, and tested beam\nsearch.\nPranav Shyam worked on SuperGLUE and assisted with connections to few-shot learning and\nmeta-learning literature.\nSandhini Agarwal conducted the fairness and representation analysis.\nGirish Sastry and Amanda Askell conducted the human evaluations of the model.\nAriel Herbert-Voss conducted the threat analysis of malicious use.\nGretchen Krueger edited and red-teamed the policy sections of the paper.' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 17}
page_content='Sandhini Agarwal conducted the fairness and representation analysis.\nGirish Sastry and Amanda Askell conducted the human evaluations of the model.\nAriel Herbert-Voss conducted the threat analysis of malicious use.\nGretchen Krueger edited and red-teamed the policy sections of the paper.\nBenjamin Chess, Clemens Winter, Eric Sigler, Christopher Hesse, Mateusz Litwin, and\nChristopher Berner optimized OpenAI’s clusters to run the largest models efﬁciently.\nScott Gray developed fast GPU kernels used during training.\nJack Clark led the analysis of ethical impacts — fairness and representation, human assessments of\nthe model, and broader impacts analysis, and advised Gretchen, Amanda, Girish, Sandhini, and Ariel\non their work.\nDario Amodei, Alec Radford, Tom Brown, Sam McCandlish, Nick Ryder, Jared Kaplan, Sand-\nhini Agarwal, Amanda Askell, Girish Sastry, and Jack Clark wrote the paper.\n18' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 17}
page_content='Title: Star\'s Tux Promise Draws Megyn Kelly\'s Sarcasm\nSubtitle: Joaquin Phoenix pledged to not change for each awards\nevent\nArticle: A year ago, Joaquin Phoenix made headlines when he\nappeared on the red carpet at the Golden Globes wearing a tuxedo\nwith a paper bag over his head that read, "I am a shape-shifter.\nI can\'t change the world. I can only change myself." It was a\npromise to not change to fit into the Hollywood mold: "I think\nthat\'s a really special thing, to not change yourself. I think\nit\'s a really special thing to say, `This is what\'s inside of\nme, I\'m proud of it, and I\'m not going to be ashamed because of\nthe way that someone else thinks I should be.\'" Now, it\'s the\nOscars, and Phoenix is at it again. But this time, his publicist\nis saying he\'ll be wearing a tux no matter what.\nMegyn Kelly was not impressed, and she let him have it on The\nTonight Show. "You know, I feel like, I feel like you could\nhave worn the tux," she says. "But you\'re saying you\'re a' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 18}
page_content='Oscars, and Phoenix is at it again. But this time, his publicist\nis saying he\'ll be wearing a tux no matter what.\nMegyn Kelly was not impressed, and she let him have it on The\nTonight Show. "You know, I feel like, I feel like you could\nhave worn the tux," she says. "But you\'re saying you\'re a\nshape-shifter. I don\'t know if you can change your tux, but you\ncan change your mind. You can change your mind. You can change\nyour mind." Phoenix says he did, but it didn\'t stick. "I was\nlike, `Okay, I\'m going to wear a tuxedo to this thing.\' And then\nI thought, `I don\'t want to wear a tuxedo to this thing.\'" Kelly\ngoes on to encourage him to change his mind again, but Phoenix\nsays it\'s too late: "I\'m committed to wearing this."\nFigure 7.5: The GPT-3 generated news article that humans found the easiest to distinguish from a\nhuman written article (accuracy: 61%).\nSam McCandlish led the analysis of model scaling, and advised Tom Henighan and Jared Kaplan\non their work.' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 18}
page_content='says it\'s too late: "I\'m committed to wearing this."\nFigure 7.5: The GPT-3 generated news article that humans found the easiest to distinguish from a\nhuman written article (accuracy: 61%).\nSam McCandlish led the analysis of model scaling, and advised Tom Henighan and Jared Kaplan\non their work.\nAlec Radford advised the project from an NLP perspective, suggested tasks, put the results in context,\nand demonstrated the beneﬁt of weight decay for training.\nIlya Sutskever was an early advocate for scaling large generative likelihood models, and advised\nPranav, Prafulla, Rewon, Alec, and Aditya on their work.\nDario Amodei designed and led the research.\n19' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 18}
page_content='References\n[ADG+16]Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David\nPfau, Tom Schaul, Brendan Shillingford, and Nando De Freitas. Learning to learn by\ngradient descent by gradient descent. In Advances in neural information processing\nsystems , pages 3981–3989, 2016.\n[AJF19] Roee Aharoni, Melvin Johnson, and Orhan Firat. Massively multilingual neural\nmachine translation. In Proceedings of the 2019 Conference of the North Ameri-\ncan Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) , 2019.\n[BBDIW20] Su Lin Blodgett, Solon Barocas, Hal Daum ´e III, and Hanna Wallach. Language (tech-\nnology) is power: A critical survey of “bias” in nlp. arXiv preprint arXiv:2005.14050 ,\n2020.\n[BES10] Stefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani. Sentiwordnet 3.0: an en-\nhanced lexical resource for sentiment analysis and opinion mining. In Lrec, volume 10,\npages 2200–2204, 2010.' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 19}
page_content='nology) is power: A critical survey of “bias” in nlp. arXiv preprint arXiv:2005.14050 ,\n2020.\n[BES10] Stefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani. Sentiwordnet 3.0: an en-\nhanced lexical resource for sentiment analysis and opinion mining. In Lrec, volume 10,\npages 2200–2204, 2010.\n[BHT+20]Yonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob Andreas, Yoshua Bengio, Joyce\nChai, Mirella Lapata, Angeliki Lazaridou, Jonathan May, Aleksandr Nisnevich, et al.\nExperience grounds language. arXiv preprint arXiv:2004.10151 , 2020.\n[BLC13] Yoshua Bengio, Nicholas L ´eonard, and Aaron C. Courville. Estimating or propagating\ngradients through stochastic neurons for conditional computation. Arxiv , 2013.\n[Car97] Rich Caruana. Multitask learning. Machine learning , 28(1), 1997.\n[CCE+18]Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa\nSchoenick, and Oyvind Tafjord. Think you have solved question answering? try arc,' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 19}
page_content='[Car97] Rich Caruana. Multitask learning. Machine learning , 28(1), 1997.\n[CCE+18]Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa\nSchoenick, and Oyvind Tafjord. Think you have solved question answering? try arc,\nthe ai2 reasoning challenge. ArXiv , abs/1803.05457, 2018.\n[CGRS19] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences\nwith sparse transformers, 2019.\n[CLY+19]Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan,\nYu Cheng, and Jingjing Liu. Uniter: Learning universal image-text representations.\narXiv preprint arXiv:1909.11740 , 2019.\n[Cra17] Kate Crawford. The trouble with bias. NIPS 2017 Keynote , 2017.\n[DCLT18] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-\ntraining of deep bidirectional transformers for language understanding. arXiv preprint\narXiv:1810.04805 , 2018.\n[DGV+18]Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 19}
page_content='[DCLT18] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-\ntraining of deep bidirectional transformers for language understanding. arXiv preprint\narXiv:1810.04805 , 2018.\n[DGV+18]Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz\nKaiser. Universal transformers. Arxiv , 2018.\n[DHKH14] Nadir Durrani, Barry Haddow, Philipp Koehn, and Kenneth Heaﬁeld. Edinburgh’s\nphrase-based machine translation systems for wmt-14. In Proceedings of the Ninth\nWorkshop on Statistical Machine Translation , pages 97–104, 2014.\n[DL15] Andrew M. Dai and Quoc V . Le. Semi-supervised sequence learning. In Advances in\nneural information processing systems , 2015.\n[DSC+16]Yan Duan, John Schulman, Xi Chen, Peter L. Bartlett, Ilya Sutskever, and Pieter\nAbbeel. Rl2: Fast reinforcement learning via slow reinforcement learning. ArXiv ,\nabs/1611.02779, 2016.\n[DWD+19]Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 19}
page_content='[DSC+16]Yan Duan, John Schulman, Xi Chen, Peter L. Bartlett, Ilya Sutskever, and Pieter\nAbbeel. Rl2: Fast reinforcement learning via slow reinforcement learning. ArXiv ,\nabs/1611.02779, 2016.\n[DWD+19]Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and\nMatt Gardner. Drop: A reading comprehension benchmark requiring discrete reasoning\nover paragraphs. arXiv preprint arXiv:1903.00161 , 2019.\n20' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 19}
page_content='[DYY+19]Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc V . Le, and Ruslan\nSalakhutdinov. Transformer-xl: Attentive language models beyond a ﬁxed-length\ncontext. Arxiv , 2019.\n[EOAG18] Sergey Edunov, Myle Ott, Michael Auli, and David Grangier. Understanding back-\ntranslation at scale. arXiv preprint arXiv:1808.09381 , 2018.\n[FAL17] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for\nfast adaptation of deep networks. ArXiv , abs/1703.03400, 2017.\n[GG19] Hila Gonen and Yoav Goldberg. Lipstick on a pig: Debiasing methods cover up\nsystematic gender biases in word embeddings but do not remove them. arXiv preprint\narXiv:1903.03862 , 2019.\n[Gra16] Alex Graves. Adaptive computation time for recurrent neural networks. Arxiv , 2016.\n[GSL+18]Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel R\nBowman, and Noah A Smith. Annotation artifacts in natural language inference data.\narXiv preprint arXiv:1803.02324 , 2018.' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 20}
page_content='[Gra16] Alex Graves. Adaptive computation time for recurrent neural networks. Arxiv , 2016.\n[GSL+18]Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel R\nBowman, and Noah A Smith. Annotation artifacts in natural language inference data.\narXiv preprint arXiv:1803.02324 , 2018.\n[GSR19] Sebastian Gehrmann, Hendrik Strobelt, and Alexander M. Rush. Gltr: Statistical\ndetection and visualization of generated text. arXiv preprint arXiv: 1906.04043 , 2019.\n[GWC+18]Jiatao Gu, Yong Wang, Yun Chen, Kyunghyun Cho, and Victor OK Li. Meta-learning\nfor low-resource neural machine translation. arXiv preprint arXiv:1808.08437 , 2018.\n[HB20] Daniel Hernandez and Tom Brown. Ai and efﬁciency, May 2020.\n[HNA+17]Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun,\nHassan Kianinejad, Md. Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep\nlearning scaling is predictable, empirically. arXiv preprint arXiv:1712.00409 , 2017.' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 20}
page_content='[HNA+17]Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun,\nHassan Kianinejad, Md. Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep\nlearning scaling is predictable, empirically. arXiv preprint arXiv:1712.00409 , 2017.\n[HVD15] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural\nnetwork. arXiv preprint arXiv:1503.02531 , 2015.\n[HYC01] Sepp Hochreiter, A Steven Younger, and Peter R Conwell. Learning to Learn Using\nGradient Descent. In International Conference on Artiﬁcial Neural Networks , pages\n87–94. Springer, 2001.\n[HZJ+19] Po-Sen Huang, Huan Zhang, Ray Jiang, Robert Stanforth, Johannes Welbl, Jack Rae,\nVishal Maini, Dani Yogatama, and Pushmeet Kohli. Reducing sentiment bias in\nlanguage models via counterfactual evaluation. arXiv preprint arXiv:1911.03064 ,\n2019.\n[IDCBE19] Daphne Ippolito, Daniel Duckworth, Chris Callison-Burch, and Douglas Eck. Auto-' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 20}
page_content='Vishal Maini, Dani Yogatama, and Pushmeet Kohli. Reducing sentiment bias in\nlanguage models via counterfactual evaluation. arXiv preprint arXiv:1911.03064 ,\n2019.\n[IDCBE19] Daphne Ippolito, Daniel Duckworth, Chris Callison-Burch, and Douglas Eck. Auto-\nmatic detection of generated text is easiest when humans are fooled. arXiv preprint\narXiv:1911.00650 , 2019.\n[JCWZ17] Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. TriviaQA: A large\nscale distantly supervised challenge dataset for reading comprehension. arXiv preprint\narXiv:1705.03551 , 2017.\n[JN20] Zheng Junyuan and Gamma Lab NYC. Numeric transformer - albert, March 2020.\n[JYS+19]Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang,\nand Qun Liu. TinyBERT: Distilling BERT for natural language understanding. arXiv\npreprint arXiv:1909.10351 , 2019.\n[JZC+19]Ying Ju, Fubang Zhao, Shijie Chen, Bowen Zheng, Xuefeng Yang, and Yun-' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 20}
page_content='[JYS+19]Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang,\nand Qun Liu. TinyBERT: Distilling BERT for natural language understanding. arXiv\npreprint arXiv:1909.10351 , 2019.\n[JZC+19]Ying Ju, Fubang Zhao, Shijie Chen, Bowen Zheng, Xuefeng Yang, and Yun-\nfeng Liu. Technical report on conversational question answering. arXiv preprint\narXiv:1909.10772 , 2019.\n[KKS+20]Daniel Khashabi, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and\nHannaneh Hajishirzi. Uniﬁedqa: Crossing format boundaries with a single qa system.\narXiv preprint arXiv:2005.00700 , 2020.\n21' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 20}
page_content='[KMB20] Sarah E. Kreps, Miles McCain, and Miles Brundage. All the news that’s ﬁt to fabricate:\nAi-generated text as a tool of media misinformation, 2020.\n[KMH+20]Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess,\nRewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws\nfor neural language models, 2020.\n[KPR+19]Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redﬁeld, Michael Collins, Ankur\nParikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob\nDevlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew\nDai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: a benchmark\nfor question answering research. Transactions of the Association of Computational\nLinguistics , 2019.\n[KR16] Yoon Kim and Alexander M. Rush. Sequence-level knowledge distillation. Arxiv ,\n2016.\n[LB02] Edward Loper and Steven Bird. Nltk: The natural language toolkit, 2002.' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 21}
page_content='for question answering research. Transactions of the Association of Computational\nLinguistics , 2019.\n[KR16] Yoon Kim and Alexander M. Rush. Sequence-level knowledge distillation. Arxiv ,\n2016.\n[LB02] Edward Loper and Steven Bird. Nltk: The natural language toolkit, 2002.\n[LC19] Guillaume Lample and Alexis Conneau. Cross-lingual language model pretraining.\narXiv preprint arXiv:1901.07291 , 2019.\n[LCG+19]Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma,\nand Radu Soricut. ALBERT: A lite BERT for self-supervised learning of language\nrepresentations. arXiv preprint arXiv:1909.11942 , 2019.\n[LCH+20]Xiaodong Liu, Hao Cheng, Pengcheng He, Weizhu Chen, Yu Wang, Hoifung Poon,\nand Jianfeng Gao. Adversarial training for large neural language models. arXiv\npreprint arXiv:2004.08994 , 2020.\n[LCR19] Peter J. Liu, Yu-An Chung, and Jie Ren. SummAE: Zero-shot abstractive text sum-\nmarization using length-agnostic auto-encoders. arXiv preprint arXiv:1910.00998 ,\n2019.' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 21}
page_content='and Jianfeng Gao. Adversarial training for large neural language models. arXiv\npreprint arXiv:2004.08994 , 2020.\n[LCR19] Peter J. Liu, Yu-An Chung, and Jie Ren. SummAE: Zero-shot abstractive text sum-\nmarization using length-agnostic auto-encoders. arXiv preprint arXiv:1910.00998 ,\n2019.\n[LDL19] Zhongyang Li, Xiao Ding, and Ting Liu. Story ending prediction by transferable bert.\narXiv preprint arXiv:1905.07504 , 2019.\n[LGG+20]Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad,\nMike Lewis, and Luke Zettlemoyer. Multilingual denoising pre-training for neural\nmachine translation. arXiv preprint arXiv:2001.08210 , 2020.\n[LGH+15]Xiaodong Liu, Jianfeng Gao, Xiaodong He, Li Deng, Kevin Duh, and Ye-Yi Wang.\nRepresentation learning using multi-task deep neural networks for semantic classiﬁca-\ntion and information retrieval. In Proceedings of the 2015 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human Language' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 21}
page_content='Representation learning using multi-task deep neural networks for semantic classiﬁca-\ntion and information retrieval. In Proceedings of the 2015 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human Language\nTechnologies , 2015.\n[LHCG19a] Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Improving multi-task\ndeep neural networks via knowledge distillation for natural language understanding.\narXiv preprint arXiv:1904.09482 , 2019.\n[LHCG19b] Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Multi-task deep neural\nnetworks for natural language understanding. arXiv preprint arXiv:1901.11504 , 2019.\n[LLG+19]Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mo-\nhamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-\nsequence pre-training for natural language generation, translation, and comprehension.\narXiv preprint arXiv:1910.13461 , 2019.' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 21}
page_content='[LLG+19]Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mo-\nhamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-\nsequence pre-training for natural language generation, translation, and comprehension.\narXiv preprint arXiv:1910.13461 , 2019.\n[LM17] Ke Li and Jitendra Malik. Learning to optimize neural nets. arXiv preprint\narXiv:1703.00441 , 2017.\n[LOG+19]Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer\nLevy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly\noptimized BERT pretraining approach. arXiv preprint arXiv:1907.11692 , 2019.\n22' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 21}
page_content='[LPP+20]Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin,\nNaman Goyal, Heinrich K ¨uttler, Mike Lewis, Wen-tau Yih, Tim Rockt ¨aschel, Sebastian\nRiedel, and Kiela Douwe. Retrieval-augmented generation for knowledge-intensive\nnlp tasks. arXiv preprint arXiv:2005.11401 , 2020.\n[LWS+20]Zhuohan Li, Eric Wallace, Sheng Shen, Kevin Lin, Kurt Keutzer, Dan Klein, and\nJoseph E. Gonzalez. Train large, then compress: Rethinking model size for efﬁcient\ntraining and inference of transformers, 2020.\n[MCH+16]Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Ba-\ntra, Lucy Vanderwende, Pushmeet Kohli, and James Allen. A corpus and evalua-\ntion framework for deeper understanding of commonsense stories. arXiv preprint\narXiv:1604.01696 , 2016.\n[MKAT18] Sam McCandlish, Jared Kaplan, Dario Amodei, and OpenAI Dota Team. An empirical\nmodel of large-batch training, 2018.' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 22}
page_content='tion framework for deeper understanding of commonsense stories. arXiv preprint\narXiv:1604.01696 , 2016.\n[MKAT18] Sam McCandlish, Jared Kaplan, Dario Amodei, and OpenAI Dota Team. An empirical\nmodel of large-batch training, 2018.\n[MKM+94]Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann Bies,\nMark Ferguson, Karen Katz, and Britta Schasberger. The penn treebank: annotating\npredicate argument structure. In Proceedings of the workshop on Human Language\nTechnology , pages 114–119. Association for Computational Linguistics, 1994.\n[MKXS18] Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. The\nnatural language decathlon: Multitask learning as question answering. arXiv preprint\narXiv:1806.08730 , 2018.\n[MPL19] R Thomas McCoy, Ellie Pavlick, and Tal Linzen. Right for the wrong reasons: Diagnos-\ning syntactic heuristics in natural language inference. arXiv preprint arXiv:1902.01007 ,\n2019.' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 22}
page_content='natural language decathlon: Multitask learning as question answering. arXiv preprint\narXiv:1806.08730 , 2018.\n[MPL19] R Thomas McCoy, Ellie Pavlick, and Tal Linzen. Right for the wrong reasons: Diagnos-\ning syntactic heuristics in natural language inference. arXiv preprint arXiv:1902.01007 ,\n2019.\n[MWZ+18]Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman,\nBen Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. Model\ncards for model reporting, 2018.\n[NBR20] Moin Nadeem, Anna Bethke, and Siva Reddy. Stereoset: Measuring stereotypical bias\nin pretrained language models. arXiv preprint arXiv:2004.09456 , 2020.\n[NK19] Timothy Niven and Hung-Yu Kao. Probing neural network comprehension of natural\nlanguage arguments. arXiv preprint arXiv:1907.07355 , 2019.\n[NvNvdG19] Malvina Nissim, Rik van Noord, and Rob van der Goot. Fair is better than sensational:\nMan is to doctor as woman is to doctor. arXiv preprint arXiv:1905.09866 , 2019.' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 22}
page_content='language arguments. arXiv preprint arXiv:1907.07355 , 2019.\n[NvNvdG19] Malvina Nissim, Rik van Noord, and Rob van der Goot. Fair is better than sensational:\nMan is to doctor as woman is to doctor. arXiv preprint arXiv:1905.09866 , 2019.\n[oR16] University of Regensburg. Fascha, 2016.\n[PFB18] Jason Phang, Thibault F ´evry, and Samuel R. Bowman. Sentence encoders on\nSTILTs: Supplementary training on intermediate labeled-data tasks. arXiv preprint\narXiv:1811.01088 , 2018.\n[PKL+16]Denis Paperno, Germ ´an Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella\nBernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fern ´andez. The\nlambada dataset: Word prediction requiring a broad discourse context. arXiv preprint\narXiv:1606.06031 , 2016.\n[Pos18] Matt Post. A call for clarity in reporting BLEU scores. arXiv preprint\narXiv:1804.08771 , 2018.\n[QMZH19] Yusu Qian, Urwa Muaz, Ben Zhang, and Jae Won Hyun. Reducing gender bias in' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 22}
page_content='lambada dataset: Word prediction requiring a broad discourse context. arXiv preprint\narXiv:1606.06031 , 2016.\n[Pos18] Matt Post. A call for clarity in reporting BLEU scores. arXiv preprint\narXiv:1804.08771 , 2018.\n[QMZH19] Yusu Qian, Urwa Muaz, Ben Zhang, and Jae Won Hyun. Reducing gender bias in\nword-level language models with a gender-equalizing loss function. arXiv preprint\narXiv:1905.12801 , 2019.\n[RCM19] Siva Reddy, Danqi Chen, and Christopher D Manning. Coqa: A conversational\nquestion answering challenge. Transactions of the Association for Computational\nLinguistics , 7:249–266, 2019.\n23' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 22}
page_content='[RCP+17]Scott Reed, Yutian Chen, Thomas Paine, A ¨aron van den Oord, SM Eslami, Danilo\nRezende, Oriol Vinyals, and Nando de Freitas. Few-shot autoregressive density\nestimation: Towards learning to learn distributions. arXiv preprint arXiv:1710.10304 ,\n2017.\n[RL16] Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning.\nICLR 2017 (oral) , 2016.\n[RLL+19]Qiu Ran, Yankai Lin, Peng Li, Jie Zhou, and Zhiyuan Liu. NumNet: Machine reading\ncomprehension with numerical reasoning. In Proceedings of EMNLP , 2019.\n[RNLVD18] Rachel Rudinger, Jason Naradowsky, Brian Leonard, and Benjamin Van Durme.\nGender bias in coreference resolution. arXiv preprint arXiv:1804.09301 , 2018.\n[Ros12] R.S. Ross. Guide for conducting risk assessments. NIST Special Publication , 2012.\n[RRBS19] Jonathan S. Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. A con-\nstructive prediction of the generalization error across scales, 2019.' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 23}
page_content='[Ros12] R.S. Ross. Guide for conducting risk assessments. NIST Special Publication , 2012.\n[RRBS19] Jonathan S. Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. A con-\nstructive prediction of the generalization error across scales, 2019.\n[RRS20] Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack\ninto the parameters of a language model? arXiv preprint arXiv:2002.08910 , 2020.\n[RSR+19]Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael\nMatena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning\nwith a uniﬁed text-to-text transformer, 2019.\n[RWC+19]Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya\nSutskever. Language models are unsupervised multitask learners, 2019.\n[SBC+19]Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-V oss, Jeff\nWu, Alec Radford, Gretchen Krueger, Jong Wook Kim, Sarah Kreps, Miles McCain,' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 23}
page_content='Sutskever. Language models are unsupervised multitask learners, 2019.\n[SBC+19]Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-V oss, Jeff\nWu, Alec Radford, Gretchen Krueger, Jong Wook Kim, Sarah Kreps, Miles McCain,\nAlex Newhouse, Jason Blazakis, Kris McGufﬁe, and Jasmine Wang. Release strategies\nand the social impacts of language models, 2019.\n[SCNP19] Emily Sheng, Kai-Wei Chang, Premkumar Natarajan, and Nanyun Peng. The\nwoman worked as a babysitter: On biases in language generation. arXiv preprint\narXiv:1909.01326 , 2019.\n[SDCW19] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. DistilBERT,\na distilled version of BERT: smaller, faster, cheaper and lighter. arXiv preprint\narXiv:1910.01108 , 2019.\n[SDSE19] Roy Schwartz, Jesse Dodge, Noah A. Smith, and Oren Etzioni. Green AI. CoRR ,\nabs/1907.10597, 2019.\n[SHB15] Rico Sennrich, Barry Haddow, and Alexandra Birch. Improving neural machine' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 23}
page_content='a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv preprint\narXiv:1910.01108 , 2019.\n[SDSE19] Roy Schwartz, Jesse Dodge, Noah A. Smith, and Oren Etzioni. Green AI. CoRR ,\nabs/1907.10597, 2019.\n[SHB15] Rico Sennrich, Barry Haddow, and Alexandra Birch. Improving neural machine\ntranslation models with monolingual data. arXiv preprint arXiv:1511.06709 , 2015.\n[SMM+17]Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey\nHinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated\nmixture-of-experts layer. arXiv preprint arXiv:1701.06538 , 2017.\n[SPP+19]Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper,\nand Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models\nusing model parallelism, 2019.\n[SS20] Timo Schick and Hinrich Sch ¨utze. Exploiting cloze questions for few-shot text\nclassiﬁcation and natural language inference. arXiv preprint arXiv:2001.07676 , 2020.' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 23}
page_content='and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models\nusing model parallelism, 2019.\n[SS20] Timo Schick and Hinrich Sch ¨utze. Exploiting cloze questions for few-shot text\nclassiﬁcation and natural language inference. arXiv preprint arXiv:2001.07676 , 2020.\n[STQ+19]Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. MASS: Masked sequence\nto sequence pre-training for language generation. arXiv preprint arXiv:1905.02450 ,\n2019.\n[Tur20] Project Turing. Microsoft research blog, Feb 2020.\n24' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 23}
page_content='[VBL+16]Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching\nNetworks for One Shot Learning. In Advances in neural information processing\nsystems , pages 3630–3638, 2016.\n[VSP+17]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N.\nGomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in\nneural information processing systems , 2017.\n[WPN+19]Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael,\nFelix Hill, Omer Levy, and Samuel Bowman. Superglue: A stickier benchmark for\ngeneral-purpose language understanding systems. In Advances in Neural Information\nProcessing Systems , pages 3261–3275, 2019.\n[WXH+18]Yiren Wang, Yingce Xia, Tianyu He, Fei Tian, Tao Qin, ChengXiang Zhai, and Tie-Yan\nLiu. Multi-agent dual learning. ICLR 2019 , 2018.\n[XDH+19]Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, and Quoc V . Le. Unsuper-\nvised data augmentation for consistency training, 2019.' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 24}
page_content='[WXH+18]Yiren Wang, Yingce Xia, Tianyu He, Fei Tian, Tao Qin, ChengXiang Zhai, and Tie-Yan\nLiu. Multi-agent dual learning. ICLR 2019 , 2018.\n[XDH+19]Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, and Quoc V . Le. Unsuper-\nvised data augmentation for consistency training, 2019.\n[YDY+19]Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and\nQuoc V . Le. XLNet: Generalized autoregressive pretraining for language understand-\ning. arXiv preprint arXiv:1906.08237 , 2019.\n[ZHB+19]Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag:\nCan a machine really ﬁnish your sentence? arXiv preprint arXiv:1905.07830 , 2019.\n[ZHR+19]Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska\nRoesner, and Yejin Choi. Defending against neural fake news. arXiv preprint\narXiv:1905.12616 , 2019.\n[ZSW+19]Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 24}
page_content='[ZHR+19]Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska\nRoesner, and Yejin Choi. Defending against neural fake news. arXiv preprint\narXiv:1905.12616 , 2019.\n[ZSW+19]Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario\nAmodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from\nhuman preferences, 2019.\n25' metadata={'source': 'private_upload/2023-08-09-17/NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf', 'page': 24}
